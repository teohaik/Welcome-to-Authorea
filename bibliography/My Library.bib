
@incollection{olson_fuzzy_2008,
	title = {Fuzzy {Sets} in {Data} {Mining}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_5},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {69--86},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IK95RCHP\\978-3-540-76917-0_5.html:text/html}
}

@book{martin_agile_2003,
	address = {Upper Saddle River, N.J.},
	title = {Agile software development: principles, patterns, and practices},
	isbn = {0135974445  9780135974445  9780132760584  0132760584},
	shorttitle = {Agile software development},
	abstract = {This comprehensive, pragmatic tutorial on Agile Development and eXtreme programming, written by one of the founding fathers of Agile Development: Teaches software developers and project managers how to get projects done on time, and on budget using the power of Agile Development; Uses real-world case studies to show how to of plan, test, refactor, and pair program using eXtreme programming; Contains a wealth of reusable C++ and Java code; Focuses on solving customer oriented systems problems using UML and Design Patterns.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Martin, Robert C},
	year = {2003}
}

@article{jenkins_software_2007,
	title = {Software {Architecture} {Graphs} {As} {Complex} {Networks}: {A} {Novel} {Partitioning} {Scheme} to {Measure} {Stability} and {Evolution}},
	volume = {177},
	issn = {0020-0255},
	shorttitle = {Software {Architecture} {Graphs} {As} {Complex} {Networks}},
	url = {http://dx.doi.org/10.1016/j.ins.2007.01.021},
	doi = {10.1016/j.ins.2007.01.021},
	abstract = {The stability and evolution of the structure of consecutive versions of a series of software architecture graphs are analysed using the theory of complex networks. Brief comparisons are drawn between the scale-free behaviour and second order phase transitions. On this basis a software design metric I"c"c is proposed. This software metric is used to quantify the evolution of the stability vs. maintainability of the software through various releases. It is demonstrated that the classes in the software graph are acquiring more out-going calls than incoming calls as the software ages. Three examples of software applications where maintainability and continuous refactoring are an inherent part of their development process are presented, in addition to a Sun Java2 framework where growth and backward compatibility are the more important factors for the development. Further to this a projected future evolution of the software structure and maintainability is calculated. Suggestions for future applications to software engineering and the natural sciences are briefly presented.},
	number = {12},
	urldate = {2014-02-24},
	journal = {Inf. Sci.},
	author = {Jenkins, S. and Kirk, S. R.},
	month = jun,
	year = {2007},
	keywords = {complexity, Directed network, Lehman's laws, Power law, scale-free, Software measurement, Software metric},
	pages = {2587--2601}
}

@article{chung_duplication_2003,
	title = {Duplication models for biological networks},
	volume = {10},
	issn = {1066-5277},
	doi = {10.1089/106652703322539024},
	abstract = {Are biological networks different from other large complex networks? Both large biological and nonbiological networks exhibit power-law graphs (number of nodes with degree k, N(k) approximately k(-beta)), yet the exponents, beta, fall into different ranges. This may be because duplication of the information in the genome is a dominant evolutionary force in shaping biological networks (like gene regulatory networks and protein-protein interaction networks) and is fundamentally different from the mechanisms thought to dominate the growth of most nonbiological networks (such as the Internet). The preferential choice models used for nonbiological networks like web graphs can only produce power-law graphs with exponents greater than 2. We use combinatorial probabilistic methods to examine the evolution of graphs by node duplication processes and derive exact analytical relationships between the exponent of the power law and the parameters of the model. Both full duplication of nodes (with all their connections) as well as partial duplication (with only some connections) are analyzed. We demonstrate that partial duplication can produce power-law graphs with exponents less than 2, consistent with current data on biological networks. The power-law exponent for large graphs depends only on the growth process, not on the starting graph.},
	language = {eng},
	number = {5},
	journal = {J. Comput. Biol.},
	author = {Chung, Fan and Lu, Linyuan and Dewey, T Gregory and Galas, David J},
	year = {2003},
	pmid = {14633392},
	keywords = {Internet, Models, Biological, Neural Networks (Computer), Probability, Proteins, Reproducibility of Results},
	pages = {677--687}
}

@article{smith_influence_2015,
	title = {The influence of search components and problem characteristics in early life cycle class modelling},
	volume = {103},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121214002659},
	doi = {10.1016/j.jss.2014.11.034},
	abstract = {This paper examines the factors affecting the quality of solution found by meta-heuristic search when optimising object-oriented software class models. From the algorithmic perspective, we examine the effect of encoding, choice of components such as the global search heuristic, and various means of incorporating problem- and instance-specific information. We also consider the effect of problem characteristics on the (estimated) cost of the global optimum, and the quality and distribution of local optima. The choice of global search component appears important, and adding problem and instance-specific information is generally beneficial to an evolutionary algorithm but detrimental to ant colony optimisation. The effect of problem characteristics is more complex. Neither scale nor complexity have a significant effect on the global optimum as estimated by the best solution ever found. However, using local search to locate 100,000 local optima for each problem confirms the results from meta-heuristic search: there are patterns in the distribution of local optima that increase with scale (problem size) and complexity (number of classes) and will cause problems for many classes of meta-heuristic search.},
	urldate = {2015-05-18},
	journal = {Journal of Systems and Software},
	author = {Smith, Jim and Simons, Chris},
	month = may,
	year = {2015},
	keywords = {Class modelling, Meta-heuristics, search-based software engineering},
	pages = {440--451},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NBPUF4DC\\Smith and Simons - 2015 - The influence of search components and problem cha.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\M42S84ID\\S0164121214002659.html:text/html}
}

@inproceedings{zhang_predicting_2013,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '13},
	title = {Predicting {Bug}-fixing {Time}: {An} {Empirical} {Study} of {Commercial} {Software} {Projects}},
	isbn = {978-1-4673-3076-3},
	shorttitle = {Predicting {Bug}-fixing {Time}},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486931},
	abstract = {For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective.},
	urldate = {2014-08-19},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Zhang, Hongyu and Gong, Liang and Versteeg, Steve},
	year = {2013},
	pages = {1042--1051},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\U9D27DXA\\Zhang et al. - 2013 - Predicting Bug-fixing Time An Empirical Study of .pdf:application/pdf}
}

@article{kessentini_cooperative_2014,
	title = {A {Cooperative} {Parallel} {Search}-{Based} {Software} {Engineering} {Approach} for {Code}-{Smells} {Detection}},
	volume = {40},
	issn = {0098-5589},
	doi = {10.1109/TSE.2014.2331057},
	abstract = {We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kessentini, W. and Kessentini, M. and Sahraoui, H. and Bechikh, S. and Ouni, A.},
	month = sep,
	year = {2014},
	keywords = {code-smells, code-smells detection, Computational modeling, cooperative parallel search-based software engineering approach, Detectors, distributed evolutionary algorithms, distributed optimization problem, evolutionary computation, Measurement, open source systems, Optimization, optimization process, parallel evolutionary algorithms, P-EA approach, public domain software, random search, search-based software engineering, search problems, single population-based approaches, Sociology, Software engineering, Software quality, Statistical analysis, statistics},
	pages = {841--861},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IGNBJI86\\freeabs_all.html:text/html}
}

@article{_ieee_2012,
	title = {{IEEE} {Standard} for {System} and {Software} {Verification} and {Validation} - {Redline}},
	abstract = {Verification and validation (V\&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V\&V life cycle process requirements are specified for different integrity levels. The scope of V\&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), nondevelopmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes documentation. V\&V processes include the analysis, evaluation, review, inspection, assessment, and testing of products.},
	journal = {IEEE Std 1012-2012 (Revision of IEEE Std 1012-2004) - Redline},
	month = may,
	year = {2012},
	keywords = {Computer security, Enviornmental factors, environmental verification and validation (V\&V) factors, Formal verification, Hardware, hardware V\&V, IEEE 1012, IEEE standards, independent V\&V (IV\&V), integrity level, Product life cycle management, risk/hazard/security analyses, Software engineering, software life cycle, Software resuability, Software testing, software V\&V, system life cycle, system V\&V, V\&V, V\&V measures, V\&V of reuse software},
	pages = {1--223},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2BGKUQBW\\articleDetails.html:text/html}
}

@inproceedings{wolf_predicting_2009,
	address = {Washington, DC, USA},
	series = {{ICSE} '09},
	title = {Predicting {Build} {Failures} {Using} {Social} {Network} {Analysis} on {Developer} {Communication}},
	isbn = {978-1-4244-3453-4},
	url = {http://dx.doi.org/10.1109/ICSE.2009.5070503},
	doi = {10.1109/ICSE.2009.5070503},
	urldate = {2014-07-22},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Wolf, Timo and Schroter, Adrian and Damian, Daniela and Nguyen, Thanh},
	year = {2009},
	pages = {1--11},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IHNKWB7D\\Wolf et al. - 2009 - Predicting Build Failures Using Social Network Ana.pdf:application/pdf}
}

@inproceedings{leskovec_microscopic_2008,
	address = {New York, NY, USA},
	series = {{KDD} '08},
	title = {Microscopic {Evolution} of {Social} {Networks}},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org/10.1145/1401890.1401948},
	doi = {10.1145/1401890.1401948},
	abstract = {We present a detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals. For the first time at such a large scale, we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks. Using a methodology based on the maximum-likelihood principle, we investigate a wide variety of network formation strategies, and show that edge locality plays a critical role in evolution of networks. Our findings supplement earlier network models based on the inherently non-local preferential attachment. Based on our observations, we develop a complete model of network evolution, where nodes arrive at a prespecified rate and select their lifetimes. Each node then independently initiates edges according to a "gap" process, selecting a destination for each edge according to a simple triangle-closing model free of any parameters. We show analytically that the combination of the gap distribution with the node lifetime leads to a power law out-degree distribution that accurately reflects the true network in all four cases. Finally, we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Leskovec, Jure and Backstrom, Lars and Kumar, Ravi and Tomkins, Andrew},
	year = {2008},
	keywords = {graph generators, maximum likelihood, network evolution, social networks, transitivity, triadic closure},
	pages = {462--470}
}

@misc{jetty__2014,
	url = {http://www.eclipse.org/jetty/},
	journal = {Jetty - Servlet Engine and HTTP Server},
	author = {Jetty},
	month = nov,
	year = {2014}
}

@misc{_________forecasting_2014,
	title = {Forecasting trends in {Software} {Evolution}: {Supplemental} {Material}},
	url = {http://se.uom.gr/index.php/software-evolution-model/},
	author = {\_\_\_\_\_\_\_\_},
	month = oct,
	year = {2014}
}

@inproceedings{holmes_deep_2008,
	title = {Deep intellisense: a tool for rehydrating evaporated information},
	isbn = {9781605580241},
	shorttitle = {Deep intellisense},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=75112},
	doi = {10.1145/1370750.1370755},
	language = {en},
	urldate = {2014-06-23},
	publisher = {ACM Press},
	author = {Holmes, Reid and Begel, Andrew},
	year = {2008},
	pages = {23},
	file = {Deep Intellisense\: A Tool for Rehydrating Evaporated Information - Microsoft Research:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8EQTESEG\\default.html:text/html}
}

@article{iii_fast_2012,
	title = {Fast {Generation} of {Large} {Scale} {Social} {Networks} with {Clustering}},
	volume = {abs/1202.4805},
	url = {http://arxiv.org/abs/1202.4805},
	journal = {CoRR},
	author = {III, Joseph J. Pfeiffer and Fond, Timothy La and Moreno, Sebastián and Neville, Jennifer},
	year = {2012}
}

@misc{_jfreechart_2014,
	title = {{JFreeChart}},
	url = {http://www.jfree.org/jfreechart},
	urldate = {2014-03-11},
	year = {2014}
}

@book{riel_object-oriented_1996,
	address = {Reading, Mass.},
	title = {Object-oriented design heuristics},
	isbn = {020163385X  9780201633856  9780321774965  0321774965},
	language = {English},
	publisher = {Addison-Wesley Pub. Co.},
	author = {Riel, Arthur J},
	year = {1996}
}

@misc{_github_????,
	title = {{GitHub} - {Features}},
	url = {https://github.com},
	abstract = {GitHub is the best place to build software together. Over 4 million people use GitHub to share code.},
	urldate = {2014-06-30},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KM3GU597\\features.html:text/html}
}

@inproceedings{ligu_buco_2013,
	address = {Thessaloniki, Greece},
	title = {{BuCo} {Reporter}: {Mining} {Software} and {Bug} {Repositories}},
	author = {Ligu, Elvis and Chaikalis, Theodore and Chatzigeorgiou, Alexander},
	month = sep,
	year = {2013}
}

@incollection{yazdi_analysis_2014,
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Analysis and {Prediction} of {Design} {Model} {Evolution} {Using} {Time} {Series}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-07868-7, 978-3-319-07869-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-07869-4_1},
	abstract = {Tools which support Model-Driven Engineering have to be evaluated and tested. In the domain of model differencing and model versioning, sequences of software models (model histories), in which a model is obtained from its immediate predecessor by some modification, are of special interest. Unfortunately, in this application domain adequate real test models are scarcely available and must be artificially created. To this end, model generators were proposed in recent years. Generally, such model generators should be configured in a way that the generated sequences of models are as realistic as possible, i.e. they should mimic the changes that happen in real software models. Hence, it is a necessary prerequisite to analyze and to stochastically model the evolution (changes) of real software systems at the abstraction level of models. In this paper, we present a new approach to statistically analyze the evolution of models. Our approach uses time series as a statistical method to capture the dynamics of the evolution. We applied this approach to several typical projects and we successfully modeled their evolutions. The time series models could predict the future changes of the next revisions of the systems with good accuracies. The obtained time series models are used to create more realistic model histories for model versioning and model differencing tools.},
	language = {en},
	number = {178},
	urldate = {2014-08-21},
	booktitle = {Advanced {Information} {Systems} {Engineering} {Workshops}},
	publisher = {Springer International Publishing},
	author = {Yazdi, Hamed Shariat and Mirbolouki, Mahnaz and Pietsch, Pit and Kehrer, Timo and Kelter, Udo},
	editor = {Iliadis, Lazaros and Papazoglou, Michael and Pohl, Klaus},
	month = jan,
	year = {2014},
	keywords = {Business Information Systems, Computer Appl. in Administrative Data Processing, Information Systems Applications (incl. Internet), Software engineering, Systems and Data Security},
	pages = {1--15},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GNQC7H9S\\978-3-319-07869-4_1.html:text/html}
}

@inproceedings{vasa_inevitable_2007,
	title = {The {Inevitable} {Stability} of {Software} {Change}},
	doi = {10.1109/ICSM.2007.4362613},
	abstract = {Real software systems change and become more complex over time. But which parts change and which parts remain stable? Common wisdom, for example, states that in a well-designed object-oriented system, the more popular a class is, the less likely it is to change from one version to the next, since changes to this class are likely to impact its clients. We have studied consecutive releases of several public domain, object-oriented software systems and analyzed a number of measures indicative of size, popularity, and complexity of classes and interfaces. As it turns out, the distributions of these measures are remarkably stable as an application evolves. The distribution of class size and complexity retains its shape over time. Relatively little code is modified over time. Classes that tend to be modified, however, are also the more popular ones, that is, those with greater Fan-In. In general, the more "complex" a class or interface becomes, the more likely it is to change from one version to the next.},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance}, 2007. {ICSM} 2007},
	author = {Vasa, R. and Schneider, J.-G. and Nierstrasz, O.},
	month = oct,
	year = {2007},
	keywords = {Application software, Australia, Communications technology, Computer science, object-oriented programming, object-oriented software system, public domain software, public domain software system, Shape, Size measurement, software change, Software evolution, software maintenance, Software measurement, software prototyping, Software systems, software systems change, Stability, Time measurement},
	pages = {4--13},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UTVVGKJJ\\login.html:text/html}
}

@incollection{manktelow_history_2010,
	title = {History of {Taxonomy}},
	copyright = {Uppsala University},
	language = {English},
	publisher = {Lecture from Dept. of Systematic Biology},
	author = {Manktelow, Mariette},
	year = {2010},
	note = {Lecture from Dept. of Systematic Biology}
}

@inproceedings{chaikalis_seanets:_2012,
	title = {{SEANets}: {Software} evolution analysis with networks},
	shorttitle = {{SEANets}},
	doi = {10.1109/ICSM.2012.6405341},
	abstract = {Evolving software systems can be systematically studied by treating them as networks and employing ideas and techniques from the field of Social Network Analysis. SEANets is an Eclipse plugin that allows the analysis of multiple software versions and the extraction and visualization of network properties required to investigate evolutionary trends of the underlying system.},
	booktitle = {2012 28th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {Chaikalis, T. and Melas, G. and Chatzigeorgiou, A.},
	month = sep,
	year = {2012},
	keywords = {Databases, data visualisation, Eclipse plugin, Java, Market research, Measurement, network analysis, network property extraction, network property visualization, network theory (graphs), object-oriented design, object-oriented methods, SEANets framework, social network analysis, Social network services, Software evolution, software evolution analysis, software maintenance, Software systems, software version analysis},
	pages = {634--637},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MXFGKKQK\\login.html:text/html}
}

@inproceedings{dambros_flexible_2008,
	title = {A {Flexible} {Framework} to {Support} {Collaborative} {Software} {Evolution} {Analysis}},
	doi = {10.1109/CSMR.2008.4493295},
	abstract = {To understand the evolution of software, researchers have developed a plethora of tools to parse, model, and analyze the history of systems. Despite their usefulness, a common downside of such tools is that their use comes with many strings attached, such as installation, data formats, usability, etc. The result is that many tools are only used by their creators, which is detrimental to cross-fertilization of research ideas and collaborative analysis. In this paper we present the Churrasco framework, which supports software evolution modeling, visualization and analysis through a web interface. The user provides only the URL of the Subversion repository to be analyzed and, if available, of the corresponding bug tracking system. Churrasco processes the given data and automatically creates and stores an evolutionary model in a centralized database. This database, called Meta-base is connected to Churrasco through object-relational persistence. The persistency mechanism is meta-described in terms of the EMOF meta-meta- model and automatically generated based on any given evolutionary meta-model. In case the meta-model changes, the persistency mechanism is automatically updated. After providing a detailed description of Churrasco, we provide evidence, by means of an example scenario, that it allows for collaborative software evolution analysis, based on visualizations available on our analysis web portal.},
	booktitle = {12th {European} {Conference} on {Software} {Maintenance} and {Reengineering}, 2008. {CSMR} 2008},
	author = {D'Ambros, M. and Lanza, M.},
	month = apr,
	year = {2008},
	keywords = {bug tracking system, Cause effect analysis, centralized database, Churrasco framework, Collaborative software, collaborative software evolution analysis, Data visualization, evolutionary meta-model, History, Informatics, Information analysis, Internet, Meta-base, object-oriented databases, object-relational persistence, relational databases, reverse engineering, Software systems, Software tools, software visualization, URL, Usability, Visual databases, Web interface},
	pages = {3--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CQSPPIIM\\login.html:text/html}
}

@inproceedings{godfrey_past_2008,
	title = {The past, present, and future of software evolution},
	doi = {10.1109/FOSM.2008.4659256},
	abstract = {Change is an essential characteristic of software development, as software systems must respond to evolving requirements, platforms, and other environmental pressures. In this paper, we discuss the concept of software evolution from several perspectives. We examine how it relates to and differs from software maintenance. We discuss insights about software evolution arising from Lehmanpsilas laws of software evolution and the staged lifecycle model of Bennett and Rajlich. We compare software evolution to other kinds of evolution, from science and social sciences, and we examine the forces that shape change. Finally, we discuss the changing nature of software in general as it relates to evolution, and we propose open challenges and future directions for software evolution research.},
	booktitle = {Frontiers of {Software} {Maintenance}, 2008. {FoSM} 2008.},
	author = {Godfrey, M.W. and German, D.M.},
	month = sep,
	year = {2008},
	keywords = {Computer science, Environmental economics, Evolution (biology), lifecycle model, Preventive maintenance, Runtime environment, software development, software development management, Software engineering, Software evolution, software maintenance, Software standards, Software systems, taxonomy},
	pages = {129--138},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\Z9UD95EX\\login.html:text/html}
}

@misc{_stixoi.info:_????,
	title = {stixoi.info: Στίχοι, ποιήματα, μεταφράσεις},
	shorttitle = {stixoi.info},
	url = {http://www.stixoi.info/},
	urldate = {2014-03-29},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\I33BC7M8\\www.stixoi.info.html:text/html}
}

@misc{_trends_2015,
	title = {Trends in {Software} {Libraries}},
	url = {http://se.uom.gr/index.php/trends-in-software-libraries/},
	abstract = {This is the accompanying page for the paper  “Assessing the Evolution of Quality in Software Libraries” which has been submitted to BCI 2015.},
	urldate = {2015-03-30},
	journal = {Trends in Software Libraries},
	month = mar,
	year = {2015}
}

@article{myrtveit_reliability_2005,
	title = {Reliability and validity in comparative studies of software prediction models},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.58},
	abstract = {Empirical studies on software prediction models do not converge with respect to the question "which prediction model is best?" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Myrtveit, I and Stensrud, E. and Shepperd, M.},
	month = may,
	year = {2005},
	keywords = {accuracy indicator, accuracy indicators., analogy estimation, Analytical models, arbitrary function approximators, Artificial neural networks, convergence, Cost estimation, Cost function, cross validation, cross-validation, data sample, empirical method, empirical methods, estimation by analogy, function approximation, Index Terms- Software metrics, learning (artificial intelligence), Machine learning, machine learning model, Mathematical model, Maximum likelihood estimation, Predictive models, Programming, program verification, regression analysis, regression model, reliability, simulation, software cost estimation, software metrics, software prediction model, software reliability, software validity, validity},
	pages = {380--391},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\F82ZS5IC\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\96SX8XTV\\Myrtveit et al. - 2005 - Reliability and validity in comparative studies of.pdf:application/pdf}
}

@inproceedings{bevan_facilitating_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {Facilitating {Software} {Evolution} {Research} with {Kenyon}},
	isbn = {1-59593-014-0},
	url = {http://doi.acm.org/10.1145/1081706.1081736},
	doi = {10.1145/1081706.1081736},
	abstract = {Software evolution research inherently has several resource-intensive logistical constraints. Archived project artifacts, such as those found in source code repositories and bug tracking systems, are the principal source of input data. Analysis-specific facts, such as commit metadata or the location of design patterns within the code, must be extracted for each change or configuration of interest. The results of this resource-intensive "fact extraction" phase must be stored efficiently, for later use by more experimental types of research tasks, such as algorithm or model refinement. In order to perform any type of software evolution research, each of these logistical issues must be addressed and an implementation to manage it created. In this paper, we introduce Kenyon, a system designed to facilitate software evolution research by providing a common set of solutions to these common logistical problems. We have used Kenyon for processing source code data from 12 systems of varying sizes and domains, archived in 3 different types of software configuration management systems. We present our experiences using Kenyon with these systems, and also describe Kenyon's usage by students in a graduate seminar class.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Bevan, Jennifer and Whitehead,Jr., E. James and Kim, Sunghun and Godfrey, Michael},
	year = {2005},
	keywords = {software configuration management, Software evolution, software stratigraphy},
	pages = {177--186}
}

@article{sokal_classification:_1974,
	title = {Classification: purposes, principles, progress, prospects},
	volume = {185},
	issn = {0036-8075},
	shorttitle = {Classification},
	doi = {10.1126/science.185.4157.1115},
	abstract = {There is an intimate interrelation between principles and procedures in classification, and modern work in this field has been profoundly affected by the development of electronic computers. Besides the delineation of natural systems and the achievement of economy of memory and ease of manipulation, the primary purpose of classification is the description of the structure and relationship of groups of similar objects. Successful classifications generate scientific hypotheses, although much classificatory work has applied, practical goals. The acceptance of polythetic taxa is a major conceptual advance and has directly led to classifications based on many, equally weighted characteristics. The specification of data for classification by computer will enhance objectivity but not eliminate cultural and subjective biases. Techniques of classification include cluster analysis and ordination, and numerous ways of representing classifications have been elaborated recently. By the application of graph theory to some classificatory problems it has been possible to reconstruct evolutionary branching sequences. Computer classification has been successfully applied across a broad range of disciplines.},
	language = {eng},
	number = {4157},
	journal = {Science},
	author = {Sokal, R R},
	month = sep,
	year = {1974},
	pmid = {17835456},
	pages = {1115--1123}
}

@inproceedings{antoniol_modeling_2001,
	title = {Modeling clones evolution through time series},
	doi = {10.1109/ICSM.2001.972740},
	abstract = {The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive model. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2. 0. 10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4 \%. A higher prediction error was observed only in correspondence of major system redesign},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance}, 2001. {Proceedings}},
	author = {Antoniol, G. and Casazza, G. and Di Penta, M. and Merlo, E.},
	year = {2001},
	keywords = {clones evolution modelling, Cloning, code fragments, DH-HEMTs, Economic forecasting, Maintenance engineering, medium-size software system, metric-based approach, Monitoring, mSQL, prediction error, Predictive models, Software evolution, software maintenance, software metrics, Software performance, Software quality, software system maintenance, Software systems, time series, Time series analysis},
	pages = {273--280},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7R42SW2C\\articleDetails.html:text/html}
}

@article{simons_interactive_2014,
	title = {Interactive ant colony optimization ({iACO}) for early lifecycle software design},
	volume = {8},
	issn = {1935-3812, 1935-3820},
	url = {http://link.springer.com/article/10.1007/s11721-014-0094-2},
	doi = {10.1007/s11721-014-0094-2},
	abstract = {Finding good designs in the early stages of the software development lifecycle is a demanding multi-objective problem that is crucial to success. Previously, both interactive and non-interactive techniques based on evolutionary algorithms (EAs) have been successfully applied to assist the designer. However, recently ant colony optimization was shown to outperform EAs at optimising quantitative measures of software designs with a limited computational budget. In this paper, we propose a novel interactive ACO (iACO) approach, in which the search is steered jointly by an adaptive model that combines subjective and objective measures. Results show that iACO is speedy, responsive and effective in enabling interactive, dynamic multi-objective search. Indeed, study participants rate the iACO search experience as compelling. Moreover, inspection of the learned model facilitates understanding of factors affecting users’ judgements, such as the interplay between a design’s elegance and the interdependencies between its components.},
	language = {en},
	number = {2},
	urldate = {2015-05-18},
	journal = {Swarm Intell},
	author = {Simons, Christopher L. and Smith, Jim and White, Paul},
	month = jun,
	year = {2014},
	keywords = {Ant colony optimization, Appl.Mathematics/Computational Methods of Engineering, Artificial Intelligence (incl. Robotics), Communications Engineering, Networks, Computer Communication Networks, Computer Systems Organization and Communication Networks, Interactive search, Software design},
	pages = {139--157},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\Q9PX8BHA\\Simons et al. - 2014 - Interactive ant colony optimization (iACO) for ear.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JQHWCQX4\\s11721-014-0094-2.html:text/html}
}

@article{chaikalis_investigating_????,
	title = {Investigating the effect of evolution and refactorings on feature scattering},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/article/10.1007/s11219-013-9204-4},
	doi = {10.1007/s11219-013-9204-4},
	abstract = {The implementation of a functional requirement is often distributed across several modules posing difficulties to software maintenance. In this paper, we attempt to quantify the extent of feature scattering and study its evolution with the passage of software versions. To this end, we trace the classes and methods involved in the implementation of a feature, apply formal approaches for studying variations across versions, measure whether feature implementation is uniformly distributed and visualize the reuse among features. Moreover, we investigate the impact of refactoring application on feature scattering in order to assess the circumstances under which a refactoring might improve the distribution of methods implementing a feature. The proposed techniques are exemplified for various features on several versions of four open-source projects.},
	language = {en},
	urldate = {2014-03-12},
	journal = {Software Qual J},
	author = {Chaikalis, Theodore and Chatzigeorgiou, Alexander and Examiliotou, Georgina},
	keywords = {Data Structures, Cryptology and Information Theory, Feature identification, Feature scattering, Operating Systems, Programming Languages, Compilers, Interpreters, Program understanding, Refactorings, Requirements traceability, Software Engineering/Programming and Operating Systems, Software evolution},
	pages = {1--27},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\N5GBE65D\\Chaikalis et al. - Investigating the effect of evolution and refactor.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GE8459U8\\s11219-013-9204-4.html:text/html}
}

@book{hennessy_computer_2012,
	address = {Amsterdam; Waltham (MA)},
	title = {Computer architecture: a quantitative approach},
	isbn = {9780123838728 012383872X},
	shorttitle = {Computer architecture},
	language = {English},
	publisher = {Elsevier ; Morgan Kaufmann},
	author = {Hennessy, John L and Patterson, David A and Asanović, Krste},
	year = {2012}
}

@inproceedings{wettel_visual_2009,
	title = {Visual exploration of large-scale evolving software},
	doi = {10.1109/ICSE-COMPANION.2009.5071029},
	abstract = {The comprehensive understanding of today's software systems is a daunting activity, because of the sheer size and complexity that such systems exhibit. Moreover, software systems evolve, which dramatically increases the amount of data one needs to analyze in order to gain insights into such systems. Indeed, software complexity is recognized as one of the major challenges to the development and maintenance of industrial-size software projects. Our vision is a 3D visualization approach which helps software engineers build knowledge about their systems. We settled on an intuitive metaphor, which depicts software systems as cities. To validate the ideas emerging from our research, we implemented a tool called CodeCity. We devised a set of visualization techniques to support tasks related to program comprehension, design quality assessment, and evolution analysis, and applied them on large open-source systems written in Java, C++, or Smalltalk. Our next research goals are enriching our metaphor with meaningful representations for relations and encoding higher-level information.},
	booktitle = {31st {International} {Conference} on {Software} {Engineering} - {Companion} {Volume}, 2009. {ICSE}-{Companion} 2009},
	author = {Wettel, R.},
	month = may,
	year = {2009},
	keywords = {3D visualization, C++ language, Cities and towns, CodeCity, Computer industry, Data visualization, design quality assessment, industrial-size software project, Java, Knowledge engineering, Large-scale systems, Open source software, open-source system, program comprehension, program visualisation, Project management, public domain software, Quality assessment, reverse engineering, Smalltalk, software complexity, software development, Software evolution, software maintenance, software metrics, Software quality, software system, Software systems},
	pages = {391--394},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5TPR3TP9\\abstractKeywords.html:text/html}
}

@inproceedings{travassos_environment_2008,
	title = {An {Environment} to {Support} {Large} {Scale} {Experimentation} in {Software} {Engineering}},
	doi = {10.1109/ICECCS.2008.30},
	abstract = {Experimental studies have been used as a mechanism to acquire knowledge through a scientific approach based on measurement of phenomena in different areas. However it is hard to run such studies when they require models (simulation), produce amount of information, and explore science in scale. In this case, a computerized infrastructure is necessary and constitutes a complex system to be built. In this paper we discuss an experimentation environment that has being built to support large scale experimentation and scientific knowledge management in software engineering.},
	booktitle = {13th {IEEE} {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems}, 2008. {ICECCS} 2008},
	author = {Travassos, G.H. and dos Santos, P.S.M. and Neto, P.G.M. and Biolchini, J.},
	month = mar,
	year = {2008},
	keywords = {Area measurement, case tools, Distributed computing, Engineering management, e-science, experimental software engineering, experimentation environments, knowledge acquisition, Knowledge engineering, knowledge management, Large-scale systems, scientific knowledge management, Software engineering, Software measurement, Software quality, Software systems},
	pages = {193--202},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2USGIXZT\\login.html:text/html}
}

@inproceedings{de_souza_sometimes_2004,
	address = {New York, NY, USA},
	series = {{CSCW} '04},
	title = {Sometimes {You} {Need} to {See} {Through} {Walls}: {A} {Field} {Study} of {Application} {Programming} {Interfaces}},
	isbn = {1-58113-810-5},
	shorttitle = {Sometimes {You} {Need} to {See} {Through} {Walls}},
	url = {http://doi.acm.org/10.1145/1031607.1031620},
	doi = {10.1145/1031607.1031620},
	abstract = {Information hiding is one of the most important and influential principles in software engineering. It prescribes that software modules hide implementation details from other modules in order to decrease the dependency between them. This separation also decreases the dependency among software developers implementing modules, thus simplifying some aspects of collaboration. A common instantiation of this principle is in the form of application programming interfaces (APIs). We performed a field study of the use of APIs and observed that they served many roles. We observed that APIs were successful indeed in supporting collaboration by serving as contracts among stakeholders as well as by reifying organizational boundaries. However, the separation that they accomplished also hindered other forms of collaboration, particularly among members of different teams. Therefore, we think argue that API's do not only have beneficial purposes. Based on our results, we discuss implications for collaborative software development tools.},
	urldate = {2014-03-26},
	booktitle = {Proceedings of the 2004 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work}},
	publisher = {ACM},
	author = {de Souza, Cleidson R. B. and Redmiles, David and Cheng, Li-Te and Millen, David and Patterson, John},
	year = {2004},
	keywords = {application programming interfaces, collaborative software development, interfaces, qualitative studies},
	pages = {63--71},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\VWBHBDNC\\de Souza et al. - 2004 - Sometimes You Need to See Through Walls A Field S.pdf:application/pdf}
}

@inproceedings{harman_pareto_2007,
	address = {New York, NY, USA},
	series = {{GECCO} '07},
	title = {Pareto {Optimal} {Search} {Based} {Refactoring} at the {Design} {Level}},
	isbn = {978-1-59593-697-4},
	url = {http://doi.acm.org/10.1145/1276958.1277176},
	doi = {10.1145/1276958.1277176},
	abstract = {Refactoring aims to improve the quality of a software systems' structure, which tends to degrade as the system evolves. While manually determining useful refactorings can be challenging, search based techniques can automatically discover useful refactorings. Current search based refactoring approaches require metrics to be combined in a complex fashion, and producea single sequence of refactorings. In this paper we show how Pareto optimality can improve search based refactoring, making the combination of metrics easier, and aiding the presentation of multiple sequences of optimal refactorings to users.},
	urldate = {2015-05-18},
	booktitle = {Proceedings of the 9th {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
	publisher = {ACM},
	author = {Harman, Mark and Tratt, Laurence},
	year = {2007},
	keywords = {Pareto optimality, refactoring, search based, Software engineering},
	pages = {1106--1113},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2QTM8VJS\\Harman and Tratt - 2007 - Pareto Optimal Search Based Refactoring at the Des.pdf:application/pdf}
}

@article{parnas_criteria_1972,
	title = {On the {Criteria} to {Be} {Used} in {Decomposing} {Systems} into {Modules}},
	volume = {15},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/361598.361623},
	doi = {10.1145/361598.361623},
	abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
	number = {12},
	urldate = {2014-03-26},
	journal = {Commun. ACM},
	author = {Parnas, D. L.},
	month = dec,
	year = {1972},
	keywords = {KWIC index, modularity, modules, Software, Software design, Software engineering},
	pages = {1053--1058},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\XJ3A8Z5C\\Parnas - 1972 - On the Criteria to Be Used in Decomposing Systems .pdf:application/pdf}
}

@inproceedings{raemaekers_measuring_2012,
	title = {Measuring software library stability through historical version analysis},
	doi = {10.1109/ICSM.2012.6405296},
	abstract = {Backward compatibility is a major concern for any library developer. In this paper, we evaluate how stable a set of frequently used third-party libraries is in terms of method removals, implementation change, the ratio of change in old methods to change in new ones and the percentage of new methods in each snapshot. We provide a motivating example of a commercial company which demonstrates several issues associated with the usage of third-party libraries. To obtain dependencies from software systems we developed a framework which extracts dependencies from Maven build files and which analyzes system and library code. We propose four metrics which provide different insights in the implementation and interface stability of a library. The usage frequency of library methods is utilized as a weight in the final metric and is obtained from a dataset of more than 2300 snapshots of 140 industrial Java systems. We finally describe three scenarios and an example of the application of our metrics.},
	booktitle = {2012 28th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {Raemaekers, S. and van Deursen, A. and Visser, J.},
	month = sep,
	year = {2012},
	keywords = {API Stability, API usage, application program interfaces, backward compatibility, commercial company, Conferences, historical version analysis, industrial Java systems, interface stability, Java, Libraries, library code, library method usage frequency, Maven build files, Measurement, Security, Software, software libraries, software library stability measurement, software metrics, software reusability, software reuse, Stability analysis, system code, third-party libraries},
	pages = {378--387},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\W8PK5U4N\\freeabs_all.html:text/html}
}

@misc{_statistics_????,
	title = {Statistics {For} {Business} {Decisions} by {J}. {K}. {Das}: {Academic} {Publishers}, {Kolkata} 9789380599557 {Soft} cover, {Second} - {Books} in my {Basket}},
	shorttitle = {Statistics {For} {Business} {Decisions} by {J}. {K}. {Das}},
	url = {http://www.abebooks.com/Statistics-Business-Decisions-J-K-Academic/3546282582/bd},
	abstract = {AbeBooks.com: Statistics For Business Decisions: 352pp.},
	urldate = {2014-08-27},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7KUCVPEV\\bd.html:text/html}
}

@inproceedings{marinescu_detection_2004,
	title = {Detection strategies: metrics-based rules for detecting design flaws},
	shorttitle = {Detection strategies},
	doi = {10.1109/ICSM.2004.1357820},
	abstract = {In order to support the maintenance of an object-oriented software system, the quality of its design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation metrics are oftentimes too fine grained to quantify comprehensively an investigated design aspect (e.g., distribution of system's intelligence among classes). To help developers and maintainers detect and localize design problems in a system, we propose a novel mechanism - called detection strategy - for formulating metrics-based rules that capture deviations from good design principles and heuristics. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g., God Class), rather than having to infer the real design problem from a large set of abnormal metric values. We have defined such detection strategies for capturing around ten important flaws of object-oriented design found in the literature and validated the approach experimentally on multiple large-scale case-studies.},
	booktitle = {20th {IEEE} {International} {Conference} on {Software} {Maintenance}, 2004. {Proceedings}},
	author = {Marinescu, R.},
	month = sep,
	year = {2004},
	keywords = {abnormal metric value, Computer science, Design engineering, design flaw detection, design heuristics, detection strategy, Diseases, Intelligent systems, isolation metrics, Large-scale systems, Maintenance engineering, metrics-based rule, object-oriented design, Object oriented modeling, object-oriented programming, object-oriented software system, program debugging, quality assurance, Software design, software maintenance, software metrics, Software systems},
	pages = {350--359},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\XM3VQAHQ\\freeabs_all.html:text/html}
}

@incollection{amal_use_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Use} of {Machine} {Learning} and {Search}-{Based} {Software} {Engineering} for {Ill}-{Defined} {Fitness} {Function}: {A} {Case} {Study} on {Software} {Refactoring}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-09939-2, 978-3-319-09940-8},
	shorttitle = {On the {Use} of {Machine} {Learning} and {Search}-{Based} {Software} {Engineering} for {Ill}-{Defined} {Fitness} {Function}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-09940-8_3},
	abstract = {The most challenging step when adapting a search-based technique for a software engineering problem is the definition of the fitness function. For several software engineering problems, a fitness function is ill-defined, subjective, or difficult to quantify. For example, the evaluation of a software design is subjective. This paper introduces the use of a neural network-based fitness function for the problem of software refactoring. The software engineers evaluate manually the suggested refactoring solutions by a Genetic Algorithm (GA) for few iterations then an Artificial Neural Network (ANN) uses these training examples to evaluate the refactoring solutions for the remaining iterations. We evaluate the efficiency of our approach using six different open-source systems through an empirical study and compare the performance of our technique with several existing refactoring studies.},
	language = {en},
	number = {8636},
	urldate = {2015-05-18},
	booktitle = {Search-{Based} {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Amal, Boukhdhir and Kessentini, Marouane and Bechikh, Slim and Dea, Josselin and Said, Lamjed Ben},
	editor = {Goues, Claire Le and Yoo, Shin},
	month = aug,
	year = {2014},
	keywords = {Algorithm Analysis and Problem Complexity, Computation by Abstract Devices, Pattern Recognition, Programming Languages, Compilers, Interpreters, Programming Techniques, Software engineering},
	pages = {31--45},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IGQE5B7I\\978-3-319-09940-8_3.html:text/html}
}

@article{glass_research_2002,
	title = {Research in software engineering: an analysis of the literature},
	volume = {44},
	issn = {0950-5849},
	shorttitle = {Research in software engineering},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584902000496},
	doi = {10.1016/S0950-5849(02)00049-6},
	abstract = {In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions:1.
What topics do SE researchers address?
2.
What research approaches do SE researchers use?
3.
What research methods do SE researchers use?
4.
On what reference disciplines does SE research depend?
5.
At what levels of analysis do SE researchers conduct research?


To answer those questions, we examined 369 papers in six leading research journals in the SE field, answering those research questions for each paper.

From that examination, we conclude that SE research is diverse regarding topic, narrow regarding research approach and method, inwardly-focused regarding reference discipline, and technically focused (as opposed to behaviorally focused) regarding level of analysis.

We pass no judgment on the SE field as a result of these findings. Instead, we present them as groundwork for future SE research efforts.},
	number = {8},
	urldate = {2014-01-20},
	journal = {Information and Software Technology},
	author = {Glass, R. L. and Vessey, I. and Ramesh, V.},
	month = jun,
	year = {2002},
	keywords = {Level of analysis: profession, Reference discipline: not applicable, Research approach: evaluative-other, Research method: literature analysis, Topic: computing research},
	pages = {491--506},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\N9786QPB\\S0950584902000496.html:text/html}
}

@incollection{olson_support_2008,
	title = {Support {Vector} {Machines}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_7},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {111--123},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\76UZ2V99\\978-3-540-76917-0_7.html:text/html}
}

@article{kagdi_survey_2007,
	title = {A {Survey} and {Taxonomy} of {Approaches} for {Mining} {Software} {Repositories} in the {Context} of {Software} {Evolution}},
	volume = {19},
	issn = {1532-060X},
	url = {http://dx.doi.org/10.1002/smr.344},
	doi = {10.1002/smr.344},
	abstract = {A comprehensive literature survey on approaches for mining software repositories (MSR) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of MSR investigations) and effective (i.e., facilitates similarities and comparisons of MSR investigations). Lastly, a number of open research issues in MSR that require further investigation are identified.},
	number = {2},
	urldate = {2014-03-11},
	journal = {J. Softw. Maint. Evol.},
	author = {Kagdi, Huzefa and Collard, Michael L. and Maletic, Jonathan I.},
	month = mar,
	year = {2007},
	keywords = {mining software repositories, multi-version analysis, Software evolution},
	pages = {77--131}
}

@book{_experimentation_????,
	title = {Experimentation in {Software} {Engineering}},
	url = {http://www.springer.com/computer/swe/book/978-3-642-29043-5},
	abstract = {Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and ...},
	urldate = {2014-09-05},
	keywords = {Experimentation in Software Engineering, Methodology of the Social Sciences, Software engineering},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BZRDX85B\\978-3-642-29043-5.html:text/html}
}

@misc{_nasa_????,
	title = {{NASA} {Technical} {Reports} {Server} ({NTRS})},
	url = {http://ntrs.nasa.gov/search.jsp}
}

@misc{_facebook/facebook-android-sdk_????,
	title = {facebook/facebook-android-sdk},
	url = {https://github.com/facebook/facebook-android-sdk},
	abstract = {facebook-android-sdk - Used to integrate Android apps with Facebook Platform.},
	urldate = {2015-02-25},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\M8KFSIHI\\facebook-android-sdk.html:text/html}
}

@book{hockney_science_1996,
	title = {The {Science} of {Computer} {Benchmarking}},
	isbn = {9780898713633},
	abstract = {This book provides an introduction to computer benchmarking. Hockney includes material concerned with the definition of performance parameters and metrics and defines a set of suitable metrics with which to measure performance and units with which to express them. He also presents new ideas resulting from the application of dimensional analysis to the field of computer benchmarking.},
	language = {en},
	publisher = {SIAM},
	author = {Hockney, Roger W.},
	year = {1996},
	keywords = {Computers / Software Development \& Engineering / General}
}

@incollection{bavota_putting_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Putting the {Developer} in-the-{Loop}: {An} {Interactive} {GA} for {Software} {Re}-modularization},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-33118-3, 978-3-642-33119-0},
	shorttitle = {Putting the {Developer} in-the-{Loop}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33119-0_7},
	abstract = {This paper proposes the use of Interactive Genetic Algorithms (IGAs) to integrate developer’s knowledge in a re-modularization task. Specifically, the proposed algorithm uses a fitness composed of automatically-evaluated factors—accounting for the modularization quality achieved by the solution—and a human-evaluated factor, penalizing cases where the way re-modularization places components into modules is considered meaningless by the developer. The proposed approach has been evaluated to re-modularize two software systems, SMOS and GESA. The obtained results indicate that IGA is able to produce solutions that, from a developer’s perspective, are more meaningful than those generated using the full-automated GA. While keeping feedback into account, the approach does not sacrifice the modularization quality, and may work requiring a very limited set of feedback only, thus allowing its application also for large systems without requiring a substantial human effort.},
	language = {en},
	number = {7515},
	urldate = {2015-05-18},
	booktitle = {Search {Based} {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bavota, Gabriele and Carnevale, Filomena and Lucia, Andrea De and Penta, Massimiliano Di and Oliveto, Rocco},
	editor = {Fraser, Gordon and Souza, Jerffeson Teixeira de},
	year = {2012},
	keywords = {Algorithm Analysis and Problem Complexity, Computation by Abstract Devices, Operating Systems, Pattern Recognition, Programming Techniques, Software engineering},
	pages = {75--89},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\4VXQFZU3\\Bavota et al. - 2012 - Putting the Developer in-the-Loop An Interactive .pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PU7NTZVT\\978-3-642-33119-0_7.html:text/html}
}

@book{_object-oriented_1997,
	address = {Upper Saddle River, NJ},
	title = {Object-oriented software construction {Buch} {Buch}},
	isbn = {0136291554 9780136291558},
	language = {English},
	publisher = {Prentice Hall},
	year = {1997}
}

@book{liskov_data_????,
	title = {Data {Abstraction} and {Hierarchy}},
	abstract = {Data abstraction is a valuable method for organizing programs to make them easier to modify and maintain. Inheritance allows one implementation of a data abstraction to be related to another hierarchically. This paper investigates the usefulness of hierarchy in program development, and concludes that although data abstraction is the more important idea, hierarchy does extend its usefulness in some situations.},
	author = {Liskov, Barbara},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MZTHWPP3\\Liskov - Data Abstraction and Hierarchy.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\255TTW9F\\summary.html:text/html}
}

@misc{_apache_2014,
	title = {Apache {Software} {Foundation}},
	url = {http://www.apache.org/},
	urldate = {2014-03-11},
	year = {2014},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CBTI5VNR\\www.apache.org.html:text/html}
}

@article{potanin_scale-free_2005,
	title = {Scale-free {Geometry} in {OO} {Programs}},
	volume = {48},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/1060710.1060716},
	doi = {10.1145/1060710.1060716},
	abstract = {Though conventional OO design suggests programs should be built from many small objects, like Lego bricks, they are instead built from objects that are scale-free, like fractals, and unlike Lego bricks.},
	number = {5},
	urldate = {2014-01-30},
	journal = {Commun. ACM},
	author = {Potanin, Alex and Noble, James and Frean, Marcus and Biddle, Robert},
	month = may,
	year = {2005},
	pages = {99--103}
}

@article{simons_interactive_2010,
	title = {Interactive, {Evolutionary} {Search} in {Upstream} {Object}-{Oriented} {Class} {Design}},
	volume = {36},
	issn = {0098-5589},
	doi = {10.1109/TSE.2010.34},
	abstract = {Although much evidence exists to suggest that early life cycle software engineering design is a difficult task for software engineers to perform, current computational tool support for software engineers is limited. To address this limitation, interactive search-based approaches using evolutionary computation and software agents are investigated in experimental upstream design episodes for two example design domains. Results show that interactive evolutionary search, supported by software agents, appears highly promising. As an open system, search is steered jointly by designer preferences and software agents. Directly traceable to the design problem domain, a mass of useful and interesting class designs is arrived at which may be visualized by the designer with quantitative measures of structural integrity, such as design coupling and class cohesion. The class designs are found to be of equivalent or better coupling and cohesion when compared to a manual class design for the example design domains, and by exploiting concurrent execution, the runtime performance of the software agents is highly favorable.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Simons, C.L. and Parmee, I.C. and Gwynllyw, R.},
	month = nov,
	year = {2010},
	keywords = {concurrent execution, Design engineering, design problem domain, evolutionary computation, interactive evolutionary search, interactive search., interactive systems, life cycle software engineering design, object-oriented methods, open system, open systems, Runtime, runtime performance, search problems, software agent, software agents, Software design, Software engineering, Software performance, Software tools, structural integrity, upstream object oriented class design, Visualization},
	pages = {798--816},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\H3BH5QW8\\freeabs_all.html:text/html}
}

@inproceedings{yu_predicting_2002,
	title = {Predicting fault-proneness using {OO} metrics. {An} industrial case study},
	doi = {10.1109/CSMR.2002.995794},
	abstract = {Software quality is an important external software attribute that is difficult to measure objectively. In this case study, we empirically validate a set of object-oriented metrics in terms of their usefulness in predicting fault-proneness, an important software quality indicator We use a set of ten software product metrics that relate to the following software attributes: the size of the software, coupling, cohesion, inheritance, and reuse. Eight hypotheses on the correlations of the metrics with fault-proneness are given. These hypotheses are empirically tested in a case study, in which the client side of a large network service management system is studied. The subject system is written in Java and it consists of 123 classes. The validation is carried out using two data analysis techniques: regression analysis and discriminant analysis},
	booktitle = {Sixth {European} {Conference} on {Software} {Maintenance} and {Reengineering}, 2002. {Proceedings}},
	author = {Yu, Ping and Systa, T. and Muller, H.},
	year = {2002},
	keywords = {client-server systems, Computer aided software engineering, Costs, data analysis, Design engineering, discriminant analysis, inheritance, network service management system, object-oriented programming, object-oriented software, regression analysis, Software design, software maintenance, Software measurement, software metrics, Software quality, software reusability, software reuse, Software testing, Statistical analysis},
	pages = {99--107},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\Z7WQDMHE\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\22I3ZKDT\\Yu et al. - 2002 - Predicting fault-proneness using OO metrics. An in.pdf:application/pdf}
}

@inproceedings{kechagia_improvement_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Improvement of {Applications}' {Stability} {Through} {Robust} {APIs}},
	isbn = {978-1-4503-3013-8},
	url = {http://doi.acm.org/10.1145/2642937.2653473},
	doi = {10.1145/2642937.2653473},
	abstract = {Modern programs require useful and robust APIs to guarantee applications' responsiveness. Given that large APIs can be used by novice developers and that not all experts are infallible, a well-designed API should inform developers about all the possible exceptions that an application can throw when it calls specific API methods. This research aims to identify automatically which exceptions should be included in an API reference. For this, there will be an evaluation of the Android API with an emphasis on its Java error-handling mechanism. First goal will be the automatic identification of as many as possible critical exceptions that each API method of the system can throw.Second goal will be the recommendation of undocumented exceptions that can lead client applications to execution failures (crashes). Consequently, the contribution of this research would be the decrease of possible application crashes that are associated with undocumented exceptions.},
	urldate = {2015-03-30},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Kechagia, Maria},
	year = {2014},
	keywords = {application programming interfaces, Documentation, exceptions, stack traces},
	pages = {907--910},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\77DMGJMT\\Kechagia - 2014 - Improvement of Applications' Stability Through Rob.pdf:application/pdf}
}

@article{turnu_fractal_2013,
	series = {Statistics with {Imperfect} {Data}},
	title = {The fractal dimension of software networks as a global quality metric},
	volume = {245},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025513003885},
	doi = {10.1016/j.ins.2013.05.014},
	abstract = {We analyzed the source code of various releases of two large Object Oriented Open Source Java software systems, Eclipse and Netbeans, investigating the complexity of the whole release and of its subprojects. We show that when the classes in the source code and the dependencies between them are considered, such systems can be viewed as complex software networks, and emerging structures, characteristic of fractals, appear at different length scales – on the entire systems and on subprojects of any size.

We were able to find in all examined cases a scaling region where it is possible to compute a self-similar coefficient, the fractal dimension, using “the box counting method”. Such a coefficient is a single metric related to the system’s complexity.

More importantly, we were able to show that this measure looks fairly related to software quality, acting as a global quality software metric. In particular, we computed the defects of each software system, and we found a clear correlation among the number of defects in the system, or in a subproject, and its fractal dimension. This correlation exists across all the subprojects and also along the time evolution of the software systems, as new releases are delivered.},
	urldate = {2014-07-10},
	journal = {Information Sciences},
	author = {Turnu, I. and Concas, G. and Marchesi, M. and Tonelli, R.},
	month = oct,
	year = {2013},
	keywords = {Fractal dimension, Object-oriented languages, Software engineering, software metrics, Software quality},
	pages = {290--303},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TQFXCUV6\\Turnu et al. - 2013 - The fractal dimension of software networks as a gl.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NB8P3W8U\\S0020025513003885.html:text/html}
}

@article{glass_research_2002-1,
	title = {Research in software engineering: an analysis of the literature},
	volume = {44},
	issn = {0950-5849},
	shorttitle = {Research in software engineering},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584902000496},
	doi = {10.1016/S0950-5849(02)00049-6},
	abstract = {In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions:1.
What topics do SE researchers address?
2.
What research approaches do SE researchers use?
3.
What research methods do SE researchers use?
4.
On what reference disciplines does SE research depend?
5.
At what levels of analysis do SE researchers conduct research?


To answer those questions, we examined 369 papers in six leading research journals in the SE field, answering those research questions for each paper.

From that examination, we conclude that SE research is diverse regarding topic, narrow regarding research approach and method, inwardly-focused regarding reference discipline, and technically focused (as opposed to behaviorally focused) regarding level of analysis.

We pass no judgment on the SE field as a result of these findings. Instead, we present them as groundwork for future SE research efforts.},
	number = {8},
	urldate = {2014-12-17},
	journal = {Information and Software Technology},
	author = {Glass, R. L. and Vessey, I. and Ramesh, V.},
	month = jun,
	year = {2002},
	keywords = {Level of analysis: profession, Reference discipline: not applicable, Research approach: evaluative-other, Research method: literature analysis, Topic: computing research},
	pages = {491--506},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HFI3CCEA\\Glass et al. - 2002 - Research in software engineering an analysis of t.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\T7IN5REW\\S0950584902000496.html:text/html}
}

@article{hall_systematic_2012,
	title = {A {Systematic} {Literature} {Review} on {Fault} {Prediction} {Performance} in {Software} {Engineering}},
	volume = {38},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.103},
	abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Hall, T. and Beecham, S. and Bowes, D. and Gray, D. and Counsell, S.},
	month = nov,
	year = {2012},
	keywords = {Analytical models, Bayes methods, Context modeling, contextual information, cost reduction, Data models, Fault diagnosis, fault prediction models, fault prediction performance, fault prediction study, feature selection, independent variables, logistic regression, methodological information, naive Bayes, Predictive models, predictive performance, regression analysis, reliable methodology, simple modeling techniques, Software engineering, software fault prediction, software fault tolerance, Software quality, Software testing, Systematic literature review, Systematics},
	pages = {1276--1304},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8TBWQSD4\\login.html:text/html}
}

@inproceedings{faloutsos_power-law_1999,
	address = {New York, NY, USA},
	series = {{SIGCOMM} '99},
	title = {On {Power}-law {Relationships} of the {Internet} {Topology}},
	isbn = {1-58113-135-6},
	url = {http://doi.acm.org/10.1145/316188.316229},
	doi = {10.1145/316188.316229},
	abstract = {Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45\% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96\% or higher.Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the {Conference} on {Applications}, {Technologies}, {Architectures}, and {Protocols} for {Computer} {Communication}},
	publisher = {ACM},
	author = {Faloutsos, Michalis and Faloutsos, Petros and Faloutsos, Christos},
	year = {1999},
	pages = {251--262}
}

@article{srinivasan_machine_1995,
	title = {Machine learning approaches to estimating software development effort},
	volume = {21},
	issn = {0098-5589},
	doi = {10.1109/32.345828},
	abstract = {Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Srinivasan, K. and Fisher, D.},
	month = feb,
	year = {1995},
	keywords = {contract bids, contracts, Costs, development resources, historical data, human resource management, Integrated circuit modeling, learning (artificial intelligence), Machine learning, Machine learning algorithms, model-construction strategy, personnel, Programming, Regression tree analysis, software development effort estimation, software development management, Software engineering, Software testing, time pressures},
	pages = {126--137},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\47VEFIE3\\freeabs_all.html:text/html}
}

@inproceedings{shang_experience_2010,
	address = {New York, NY, USA},
	series = {{ASE} '10},
	title = {An {Experience} {Report} on {Scaling} {Tools} for {Mining} {Software} {Repositories} {Using} {MapReduce}},
	isbn = {978-1-4503-0116-9},
	url = {http://doi.acm.org/10.1145/1858996.1859050},
	doi = {10.1145/1858996.1859050},
	abstract = {The need for automated software engineering tools and techniques continues to grow as the size and complexity of studied systems and analysis techniques increase. Software engineering researchers often scale their analysis techniques using specialized one-off solutions, expensive infrastructures, or heuristic techniques (e.g., search-based approaches). However, such efforts are not reusable and are often costly to maintain. The need for scalable analysis is very prominent in the Mining Software Repositories (MSR) field, which specializes in the automated recovery and analysis of large data stored in software repositories. In this paper, we explore the scaling of automated software engineering analysis techniques by reusing scalable analysis platforms from the web field. We use three representative case studies from the MSR field to analyze the potential of the MapReduce platform to scale MSR tools with minimal effort. We document our experience such that other researchers could benefit from them. We find that many of the web field's guidelines for using the MapReduce platform need to be modified to better fit the characteristics of software engineering problems.},
	urldate = {2014-03-11},
	booktitle = {Proceedings of the {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Shang, Weiyi and Adams, Bram and Hassan, Ahmed E.},
	year = {2010},
	keywords = {cloud computing, mapreduce, mining software repositories},
	pages = {275--284},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ENUPHCKT\\Shang et al. - 2010 - An Experience Report on Scaling Tools for Mining S.pdf:application/pdf}
}

@article{kontogiannis_pattern_1996,
	title = {Pattern matching for clone and concept detection},
	volume = {3},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/article/10.1007/BF00126960},
	doi = {10.1007/BF00126960},
	abstract = {A legacy system is an operational, large-scale software system that is maintained beyond its first generation of programmers. It typically represents a massive economic investment and is critical to the mission of the organization it serves. As such systems age, they become increasingly complex and brittle, and hence harder to maintain. They also become even more critical to the survival of their organization because the business rules encoded within the system are seldom documented elsewhere. Our research is concerned with developing a suite of tools to aid the maintainers of legacy systems in recovering the knowledge embodied within the system. The activities, known collectively as “program understanding”, are essential preludes for several key processes, including maintenance and design recovery for reengineering. In this paper we present three pattern-matching techniques: source code metrics, a dynamic programming algorithm for finding the best alignment between two code fragments, and a statistical matching algorithm between abstract code descriptions represented in an abstract language and actual source code. The methods are applied to detect instances of code cloning in several moderately-sized production systems including tcsh, bash, and CLIPS. The programmer's skill and experience are essential elements of our approach. Selection of particular tools and analysis methods depends on the needs of the particular task to be accomplished. Integration of the tools provides opportunities for synergy, allowing the programmer to select the most appropriate tool for a given task.},
	language = {en},
	number = {1-2},
	urldate = {2014-08-21},
	journal = {Automated Software Engineering},
	author = {Kontogiannis, K. A. and Demori, R. and Merlo, E. and Galler, M. and Bernstein, M.},
	month = jun,
	year = {1996},
	keywords = {Artificial Intelligence (incl. Robotics), dynamic programming, pattern matching, Program understanding, reverse engineering, Software Engineering/Programming and Operating Systems, software metrics},
	pages = {77--108},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WH6J62A6\\Kontogiannis et al. - 1996 - Pattern matching for clone and concept detection.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5UQTDUJH\\10.html:text/html}
}

@article{ying_predicting_2004,
	title = {Predicting source code changes by mining change history},
	volume = {30},
	issn = {0098-5589},
	doi = {10.1109/TSE.2004.52},
	abstract = {Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ying, AT.T. and Murphy, G.C. and Ng, R. and Chu-Carroll, M.C.},
	month = sep,
	year = {2004},
	keywords = {65, association rules, change history, classification, clustering, code base, Computer languages, Computer science, Computer Society, configuration management, data mining, data mining., data mining technique, Eclipse open source project, Frequency, History, Index Terms- Enhancement, Maintainability, modification task, Mozilla open source project, Pattern analysis, pattern classification, pattern clustering, Programming profession, program verification, software developers, software maintainability, software maintenance, Software systems, Software tools, source code changes prediction},
	pages = {574--586},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NGH269GU\\login.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\AEIP92FH\\Ying et al. - 2004 - Predicting source code changes by mining change hi.pdf:application/pdf}
}

@inproceedings{nagwani_data_2010,
	title = {A {Data} {Mining} {Model} to {Predict} {Software} {Bug} {Complexity} {Using} {Bug} {Estimation} and {Clustering}},
	doi = {10.1109/ITC.2010.56},
	abstract = {Software defect(bug) repositories are great source of knowledge. Data mining can be applied on these repositories to explore useful interesting patterns. Complexity of a bug helps the development team to plan future software build and releases. In this paper a prediction model is proposed to predict the bug's complexity. The proposed technique is a three step method. In the first step, fix duration for all the bugs stored in bug repository is calculated and complexity clusters are created based on the calculated bug fix duration. In second step, bug for which complexity is required its estimated fix time is calculated using bug estimation techniques. And in the third step based on the estimated fix time of bug it is mapped to a complexity cluster, which defines the complexity of the bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.},
	booktitle = {2010 {International} {Conference} on {Recent} {Trends} in {Information}, {Telecommunication} and {Computing} ({ITC})},
	author = {Nagwani, N.K. and Bhansali, A},
	month = mar,
	year = {2010},
	keywords = {bug clustering, Bug complexity, bug estimation, complexity cluster, Complexity Prediction, Computer bugs, data mining, Open source software, open source technology, pattern clustering, prediction model, Predictive models, program debugging, Programming, Project management, software bug complexity, Software bug repositories, software defect repositories, software development management, Software testing, System testing, Telecommunication computing},
	pages = {13--17},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\3UPWC36S\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WU64P94P\\Nagwani and Bhansali - 2010 - A Data Mining Model to Predict Software Bug Comple.pdf:application/pdf}
}

@article{kitchenham_using_2011,
	series = {Special {Section}: {Best} papers from the {APSEC} {Best} papers from the {APSEC}},
	title = {Using mapping studies as the basis for further research – {A} participant-observer case study},
	volume = {53},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584910002272},
	doi = {10.1016/j.infsof.2010.12.011},
	abstract = {Context
We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.
Objective
This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic.
Method
We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies.
Results
Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers.
Conclusion
Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research.},
	number = {6},
	urldate = {2014-12-17},
	journal = {Information and Software Technology},
	author = {Kitchenham, Barbara A. and Budgen, David and Pearl Brereton, O.},
	month = jun,
	year = {2011},
	keywords = {Case study, Mapping studies, Software engineering, Systematic literature review},
	pages = {638--651},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IQ72CRZM\\Kitchenham et al. - 2011 - Using mapping studies as the basis for further res.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WVSZXSXQ\\S0950584910002272.html:text/html}
}

@article{pfeiffer_iii_fast_2012,
	title = {Fast {Generation} of {Large} {Scale} {Social} {Networks} with {Clustering}},
	url = {http://arxiv.org/abs/1202.4805},
	abstract = {A key challenge within the social network literature is the problem of network generation - that is, how can we create synthetic networks that match characteristics traditionally found in most real world networks? Important characteristics that are present in social networks include a power law degree distribution, small diameter and large amounts of clustering; however, most current network generators, such as the Chung Lu and Kronecker models, largely ignore the clustering present in a graph and choose to focus on preserving other network statistics, such as the power law distribution. Models such as the exponential random graph model have a transitivity parameter, but are computationally difficult to learn, making scaling to large real world networks intractable. In this work, we propose an extension to the Chung Lu ran- dom graph model, the Transitive Chung Lu (TCL) model, which incorporates the notion of a random transitive edge. That is, with some probability it will choose to connect to a node exactly two hops away, having been introduced to a 'friend of a friend'. In all other cases it will follow the standard Chung Lu model, selecting a 'random surfer' from anywhere in the graph according to the given invariant distribution. We prove TCL's expected degree distribution is equal to the degree distribution of the original graph, while being able to capture the clustering present in the network. The single parameter required by our model can be learned in seconds on graphs with millions of edges, while networks can be generated in time that is linear in the number of edges. We demonstrate the performance TCL on four real- world social networks, including an email dataset with hundreds of thousands of nodes and millions of edges, showing TCL generates graphs that match the degree distribution, clustering coefficients and hop plots of the original networks.},
	urldate = {2014-02-21},
	journal = {arXiv:1202.4805 [physics]},
	author = {Pfeiffer III, Joseph J. and La Fond, Timothy and Moreno, Sebastian and Neville, Jennifer},
	month = feb,
	year = {2012},
	keywords = {Computer Science - Social and Information Networks, G.2.2, G.3, Physics - Physics and Society},
	annote = {Comment: 11 pages},
	file = {1202.4805 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JXBHJEP9\\Pfeiffer III et al. - 2012 - Fast Generation of Large Scale Social Networks wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\23A3UHGB\\1202.html:text/html}
}

@article{mockus_predicting_2000,
	title = {Predicting risk of software changes},
	volume = {5},
	issn = {1089-7089},
	doi = {10.1002/bltj.2229},
	abstract = {Reducing the number of software failures is one of the most challenging problems of software production. We assume that software development proceeds as a series of changes and model the probability that a change to software will cause a failure. We use predictors based on the properties of a change itself. Such predictors include size in lines of code added, deleted, and unmodified; diffusion of the change and its component subchanges, as reflected in the number of files, modules, and subsystems touched, or changed; several measures of developer experience; and the type of change and its subchanges (fault fixes or new code). The model is built on historic information and is used to predict the risk of new changes. In this paper we apply the model to 5ESS® software updates and find that change diffusion and developer experience are essential to predicting failures. The predictive model is implemented as a Web-based tool to allow timely prediction of change quality. The ability to predict the quality of change enables us to make appropriate decisions regarding inspection, testing, and delivery. Historic information on software changes is recorded in many commercial software projects, suggesting that our results can be easily and widely applied in practice.},
	number = {2},
	journal = {Bell Labs Technical Journal},
	author = {Mockus, Audris and Weiss, David M.},
	month = apr,
	year = {2000},
	pages = {169--180},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\D5FSI498\\abs_all.html:text/html}
}

@inproceedings{chaturvedi_tools_2013,
	title = {Tools in {Mining} {Software} {Repositories}},
	doi = {10.1109/ICCSA.2013.22},
	abstract = {Mining software repositories (MSR) is an important area of research. An international workshop on MSR has been established under the umbrella of international conference on software engineering (ICSE) in year 2004. The quality papers received and presented in the workshop has led to initiate full-fledged conference which purely focuses on issues related to mining software engineering data since 2007. This paper is the result of reviewing all the papers published in the proceedings of the conferences on Mining Software Repositories (MSR) and in other related conference/journals. We have analyzed the papers that contained experimental analysis of software projects related to data mining in software engineering. We have identified the data sets, techniques and tools used/ developed/ proposed in these papers. More than half of the papers are involved in the task accomplished by building or using the data mining tools to mine the software engineering data. It is apparent from the results obtained by analyzing these papers that MSR authors process the raw data which in general publicly available. We categorizes different tools used in MSR on the basis of newly developed, traditional data mining tools, prototype developed and scripts. We have shown the type of mining task that has been performed by using these tools along with the datasets used in these studies.},
	booktitle = {2013 13th {International} {Conference} on {Computational} {Science} and {Its} {Applications} ({ICCSA})},
	author = {Chaturvedi, K.K. and Sing, V.B. and Singh, P.},
	month = jun,
	year = {2013},
	keywords = {Conferences, data mining, data mining tools, ICSE, international conference on software engineering, Java, Maintenance engineering, mining software repositories, MSR, Prototypes, Software, Software engineering, software projects, software repositories},
	pages = {89--98},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JG9GPNUA\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2763B6UJ\\Chaturvedi et al. - 2013 - Tools in Mining Software Repositories.pdf:application/pdf}
}

@inproceedings{abrahamsson_effort_2007,
	title = {Effort {Prediction} in {Iterative} {Software} {Development} {Processes} – {Incremental} {Versus} {Global} {Prediction} {Models}},
	doi = {10.1109/ESEM.2007.16},
	abstract = {Estimation of development effort without imposing overhead on the project and the development team is of paramount importance for any software company. This study proposes a new effort estimation methodology aimed at agile and iterative development environments not suitable for description by traditional prediction methods. We propose a detailed development methodology, discuss a number of architectures of such models (including a wealth of augmented regression models and neural networks) and include a thorough case study of Extreme Programming (XP) in two semi-industrial projects. The results of this research evidence that in the XP environment under study the proposed incremental model outperforms traditional estimation techniques most notably in early phases of development. Moreover, when dealing with new projects, the incremental model can be developed from scratch without resorting itself to historic data.},
	booktitle = {First {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}, 2007. {ESEM} 2007},
	author = {Abrahamsson, P. and Moser, R. and Pedrycz, W. and Sillitti, A and Succi, G.},
	month = sep,
	year = {2007},
	keywords = {agile development environments, Buildings, Costs, effort estimation methodology, effort prediction, Electric variables measurement, Extreme programming, historic data, iterative development environments, Iterative methods, iterative software development processes, Object oriented modeling, Prediction methods, Predictive models, Programming, semiindustrial projects, software company, Software engineering, Software measurement},
	pages = {344--353},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TKEKSQK9\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RWXE3X9V\\Abrahamsson et al. - 2007 - Effort Prediction in Iterative Software Developmen.pdf:application/pdf}
}

@article{zanetti_network_2012,
	title = {A {Network} {Perspective} on {Software} {Modularity}},
	url = {http://arxiv.org/abs/1201.3771},
	abstract = {Modularity is a desirable characteristic for software systems. In this article we propose to use a quantitative method from complex network sciences to estimate the coherence between the modularity of the dependency network of large open source Java projects and their decomposition in terms of Java packages. The results presented in this article indicate that our methodology offers a promising and reasonable quantitative approach with potential impact on software engineering processes.},
	urldate = {2014-07-18},
	journal = {arXiv:1201.3771 [nlin, physics:physics]},
	author = {Zanetti, Marcelo Serrano and Schweitzer, Frank},
	month = jan,
	year = {2012},
	note = {arXiv: 1201.3771},
	keywords = {Computer Science - Social and Information Networks, Computer Science - Software Engineering, D.2.2, D.2.8, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Physics and Society},
	file = {arXiv\:1201.3771 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\S3H99K3G\\Zanetti and Schweitzer - 2012 - A Network Perspective on Software Modularity.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\X7WUC5UH\\1201.html:text/html}
}

@inproceedings{gousios_alitheia_2009,
	address = {Washington, DC, USA},
	series = {{ICSE} '09},
	title = {Alitheia {Core}: {An} {Extensible} {Software} {Quality} {Monitoring} {Platform}},
	isbn = {978-1-4244-3453-4},
	shorttitle = {Alitheia {Core}},
	url = {http://dx.doi.org/10.1109/ICSE.2009.5070560},
	doi = {10.1109/ICSE.2009.5070560},
	abstract = {Research in the fields of software quality and maintainability requires the analysis of large quantities of data, which often originate from open source software projects. Pre-processing data, calculating metrics, and synthesizing composite results from a large corpus of project artefacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. The system has been used to process several projects successfully, forming the basis of an emerging ecosystem of quality analysis tools.},
	urldate = {2014-03-11},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Gousios, Georgios and Spinellis, Diomidis},
	year = {2009},
	pages = {579--582},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DX2ZD7TK\\Gousios and Spinellis - 2009 - Alitheia Core An Extensible Software Quality Moni.pdf:application/pdf}
}

@misc{_nasa_????-1,
	title = {{NASA} {Software} {Engineering} {Laboratory} {Database}},
	url = {https://sw.csiac.org/databases/sled/sel.php}
}

@inproceedings{bhattacharya_graph-based_2012,
	title = {Graph-based analysis and prediction for software evolution},
	doi = {10.1109/ICSE.2012.6227173},
	abstract = {We exploit recent advances in analysis of graph topology to better understand software evolution, and to construct predictors that facilitate software development and maintenance. Managing an evolving, collaborative software system is a complex and expensive process, which still cannot ensure software reliability. Emerging techniques in graph mining have revolutionized the modeling of many complex systems and processes. We show how we can use a graph-based characterization of a software system to capture its evolution and facilitate development, by helping us estimate bug severity, prioritize refactoring efforts, and predict defect-prone releases. Our work consists of three main thrusts. First, we construct graphs that capture software structure at two different levels: (a) the product, i.e., source code and module level, and (b) the process, i.e., developer collaboration level. We identify a set of graph metrics that capture interesting properties of these graphs. Second, we study the evolution of eleven open source programs, including Firefox, Eclipse, MySQL, over the lifespan of the programs, typically a decade or more. Third, we show how our graph metrics can be used to construct predictors for bug severity, high-maintenance software parts, and failure-prone releases. Our work strongly suggests that using graph topology analysis concepts can open many actionable avenues in software engineering research and practice.},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Bhattacharya, P. and Iliofotou, M. and Neamtiu, I. and Faloutsos, Michalis},
	month = jun,
	year = {2012},
	keywords = {bug severity estimation, Collaboration, defect prediction, defect-prone release prediction, developer collaboration level, empirical studies, evolving collaborative software system, Fires, graph-based analysis, graph-based prediction, graph metrics, graph mining, Graph science, graph theory, graph topology, groupware, Maintenance engineering, Measurement, module level, open source program evolution, productivity metrics, program debugging, public domain software, refactoring effort prioritization, Software, software development, Software engineering, Software evolution, software maintenance, Software quality, software reliability, software structure, software system graph-based characterization, source code, Topology},
	pages = {419--429},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DUQUQ6BQ\\login.html:text/html}
}

@article{harman_search-based_2012,
	title = {Search-based {Software} {Engineering}: {Trends}, {Techniques} and {Applications}},
	volume = {45},
	issn = {0360-0300},
	shorttitle = {Search-based {Software} {Engineering}},
	url = {http://doi.acm.org/10.1145/2379776.2379787},
	doi = {10.1145/2379776.2379787},
	abstract = {In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives. This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.},
	number = {1},
	urldate = {2015-05-18},
	journal = {ACM Comput. Surv.},
	author = {Harman, Mark and Mansouri, S. Afshin and Zhang, Yuanyuan},
	month = dec,
	year = {2012},
	keywords = {search-based techniques, Software engineering, survey},
	pages = {11:1--11:61},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8TBVNFB4\\Harman et al. - 2012 - Search-based Software Engineering Trends, Techniq.pdf:application/pdf}
}

@book{martin_agile_2003-1,
	address = {Upper Saddle River, N.J.},
	title = {Agile software development: principles, patterns, and practices},
	isbn = {0135974445  9780135974445  9780132760584  0132760584},
	shorttitle = {Agile software development},
	abstract = {This comprehensive, pragmatic tutorial on Agile Development and eXtreme programming, written by one of the founding fathers of Agile Development: Teaches software developers and project managers how to get projects done on time, and on budget using the power of Agile Development; Uses real-world case studies to show how to of plan, test, refactor, and pair program using eXtreme programming; Contains a wealth of reusable C++ and Java code; Focuses on solving customer oriented systems problems using UML and Design Patterns.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Martin, Robert C},
	year = {2003}
}

@inproceedings{verner_guidelines_2009,
	title = {Guidelines for {Industrially}-{Based} {Multiple} {Case} {Studies} in {Software} {Engineering}.},
	booktitle = {International {Conference} on {Research} {Challenges} in {Information} {Science}},
	author = {Verner, June M. and Sampson, Jennifer and Tosic, Vladimir and Bakar, Nur Azzah Abu and Kitchenham, Barbara},
	year = {2009},
	pages = {313--324}
}

@article{vessey_unified_2005,
	title = {A unified classification system for research in the computing disciplines},
	volume = {47},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584904001260},
	doi = {10.1016/j.infsof.2004.08.006},
	abstract = {The field of computing is made up of several disciplines of which Computer Science, Software Engineering, and Information Systems are arguably three of the primary ones. Despite the fact that each discipline has a specific focus, there is also considerable overlap. Knowledge sharing, however, is becoming increasingly difficult as the body of knowledge in each discipline increases and specialization results. For effective knowledge sharing, it is therefore important to have a unified classification system by means of which the bodies of knowledge that constitute the field may be compared and contrasted. This paper presents a multi-faceted system based on five research-focused characteristics: topic, approach, method, unit of analysis, and reference discipline. The classification system was designed based on the requirements for effective classification systems, and was then used to investigate these five characteristics of research in the computing field.},
	number = {4},
	urldate = {2014-01-20},
	journal = {Information and Software Technology},
	author = {Vessey, Iris and Ramesh, V. and Glass, Robert L.},
	month = mar,
	year = {2005},
	keywords = {Classification system, Computing, Reference discipline, Research approach, Research method, Unit of analysis},
	pages = {245--255},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\6V6GEUAW\\S0950584904001260.html:text/html}
}

@inproceedings{bettenburg_think_2012,
	title = {Think locally, act globally: {Improving} defect and effort prediction models},
	shorttitle = {Think locally, act globally},
	doi = {10.1109/MSR.2012.6224300},
	abstract = {Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.},
	booktitle = {2012 9th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Bettenburg, Nicolas and Nagappan, M. and Hassan, AE.},
	month = jun,
	year = {2012},
	keywords = {Adaptation models, Biological system modeling, data handling, Data models, effort prediction models, fine-grained subsets, learning (artificial intelligence), machine learning models, Measurement, models, prediction model defect, Predictive models, regression analysis, Software, Software engineering, software metrics, splitting datasets, statistical regression models, techniques},
	pages = {60--69},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KEZ7MD3D\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PACNQISN\\Bettenburg et al. - 2012 - Think locally, act globally Improving defect and .pdf:application/pdf}
}

@article{pfleeger_experimental_1994,
	title = {Experimental design{\textasciitilde} and analysis in software engineering},
	issn = {0163-5948},
	journal = {ACM Sigsoft Software Engineering Notes},
	author = {Pfleeger, S. L.},
	year = {1994},
	file = {Experimental design~ and analysis in software engineering (parts 1 - ResearchGate:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\T3WR63PS\\242401332_Experimental_design_and_analysis_in_software_engineering_(parts_1.html:text/html}
}

@misc{_svnkit_2014,
	title = {{SVNKit} :: {Subversion} for {Java}},
	url = {http://svnkit.com/},
	urldate = {2014-03-11},
	year = {2014}
}

@inproceedings{kumar_stochastic_2000,
	title = {Stochastic models for the {Web} graph},
	doi = {10.1109/SFCS.2000.892065},
	abstract = {The Web may be viewed as a directed graph each of whose vertices is a static HTML Web page, and each of whose edges corresponds to a hyperlink from one Web page to another. We propose and analyze random graph models inspired by a series of empirical observations on the Web. Our graph models differ from the traditional Gn,p models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the Web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the Web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the Web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-Renyi models},
	booktitle = {41st {Annual} {Symposium} on {Foundations} of {Computer} {Science}, 2000. {Proceedings}},
	author = {Kumar, R. and Raghavan, P. and Rajagopalan, S. and Sivakumar, D. and Tomkins, A. and Upfal, E.},
	year = {2000},
	keywords = {Computer science, Couplings, directed graph, directed graphs, Erdos-Renyi models, HTML, hyperlink, information resources, Predictive models, random graph model, random processes, sparse random graphs, static HTML Web page, statistics, stochastic models, stochastic processes, vertices, Web graph, Web pages},
	pages = {57--65},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2H5JQUTG\\abstractAuthors.html:text/html}
}

@inproceedings{sun_structural_2009,
	title = {On {Structural} {Properties} of {Large}-{Scale} {Software} {Systems}: {From} the {Perspective} of {Complex} {Networks}},
	volume = {7},
	shorttitle = {On {Structural} {Properties} of {Large}-{Scale} {Software} {Systems}},
	doi = {10.1109/FSKD.2009.635},
	abstract = {From the viewpoint of network, large-scale computer software system scan be regarded as complex networks composed of interacting units at different levels of granularity (such as functions, classes, packages, source files, etc.). In this paper, the collaboration relationships between header files in the source node of Linux kernels, which are representative examples of large-scale open-source software systems, are analyzed by constructing weighted network-header file collaboration network (HFCN). Through using appropriate non-weighted and weighted quantities, the complex structural properties, the weight distribution and the impact between them of these networks are characterized and analyzed. These results can provide a better description of the organizational principles at the basis of the architecture of source codes in large computer software systems.},
	booktitle = {Sixth {International} {Conference} on {Fuzzy} {Systems} and {Knowledge} {Discovery}, 2009. {FSKD} '09},
	author = {Sun, Shiwen and Xia, Chengyi and Chen, Zhenhai and Sun, Junqing and Wang, Li},
	month = aug,
	year = {2009},
	keywords = {Collaborative software, complex network, Complex networks, complex structural properties, Computer networks, groupware, header file collaboration network, Kernel, large-scale computer software system, Large-scale systems, Linux, Linux kernel, Open source software, open-source software system, operating system kernels, Packaging, public domain software, software packages, Software systems, weight distribution},
	pages = {309--313},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PTWP9F4A\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MHKQ9XPA\\Sun et al. - 2009 - On Structural Properties of Large-Scale Software S.pdf:application/pdf}
}

@article{pendharkar_probabilistic_2005,
	title = {A probabilistic model for predicting software development effort},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.75},
	abstract = {Recently, Bayesian probabilistic models have been used for predicting software development effort. One of the reasons for the interest in the use of Bayesian probabilistic models, when compared to traditional point forecast estimation models, is that Bayesian models provide tools for risk estimation and allow decision-makers to combine historical data with subjective expert estimates. In this paper, we use a Bayesian network model and illustrate how a belief updating procedure can be used to incorporate decision-making risks. We develop a causal model from the literature and, using a data set of 33 real-world software projects, we illustrate how decision-making risks can be incorporated in the Bayesian networks. We compare the predictive performance of the Bayesian model with popular nonparametric neural-network and regression tree forecasting models and show that the Bayesian model is a competitive model for forecasting software development effort.},
	number = {7},
	journal = {IEEE Transactions on Software Engineering},
	author = {Pendharkar, P.C. and Subramanian, G.H. and Rodger, J.A},
	month = jul,
	year = {2005},
	keywords = {Bayesian belief network, Bayesian methods, Bayesian network model, Bayesian probabilistic model, Bayes methods, belief networks, belief updating procedure, Costs, decision making, decision-making risk, Index Terms- Bayesian belief networks, Linear regression, neural nets, Neural networks, nonparametric neural-network, point forecast estimation model, Predictive models, Probability, Probability distribution, probability theory, probability theory., Programming, real-world software project, regression analysis, Regression tree analysis, regression tree forecasting model, risk estimation, risk management, software cost estimation, software development effort forecasting, software development effort prediction, software development management, software effort estimation},
	pages = {615--624},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ZQJTD7UU\\abs_all.html:text/html}
}

@article{zheng_analyzing_2008,
	title = {Analyzing open-source software systems as complex networks},
	volume = {387},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437108005852},
	doi = {10.1016/j.physa.2008.06.050},
	abstract = {Software systems represent one of the most complex man-made artifacts. Understanding the structure of software systems can provide useful insights into software engineering efforts and can potentially help the development of complex system models applicable to other domains. In this paper, we analyze one of the most popular open-source Linux meta packages/distributions called the Gentoo Linux. In our analysis, we model software packages as nodes and dependencies among them as edges. Our empirical results show that the resulting Gentoo network cannot be easily explained by existing complex network models. This in turn motivates our research in developing two new network growth models in which a new node is connected to an old node with the probability that depends not only on the degree but also on the “age” of the old node. Through computational and empirical studies, we demonstrate that our models have better explanatory power than the existing ones. In an effort to further explore the properties of these new models, we also present some related analytical results.},
	number = {24},
	urldate = {2014-01-30},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Zheng, Xiaolong and Zeng, Daniel and Li, Huiqian and Wang, Feiyue},
	month = oct,
	year = {2008},
	keywords = {Complex networks, Degree distribution, Open-source software systems},
	pages = {6190--6200},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NMWJ4N6T\\S0378437108005852.html:text/html}
}

@misc{atunes__2013,
	url = {http://www.atunes.org},
	journal = {aTunes audio player and organizer},
	author = {aTunes},
	month = oct,
	year = {2013}
}

@article{steininger_building_2013,
	title = {Building {Taxonomies} in {IS} and {Management} – {A} {Systematic} {Approach} {Based} on {Content} {Analysis}},
	url = {http://aisel.aisnet.org/wi2013/90},
	journal = {Wirtschaftsinformatik Proceedings 2013},
	author = {Steininger, Dennis and Trenz, Manuel and Veit, Daniel},
	month = jan,
	year = {2013},
	file = {"Building Taxonomies in IS and Management – A Systematic Approach Based" by Dennis M. Steininger, Manuel Trenz et al.:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PI9ESUT7\\90.html:text/html}
}

@misc{_diffutils_2014,
	title = {Diffutils - {GNU} {Project}},
	url = {http://www.gnu.org/software/diffutils},
	urldate = {2014-03-11},
	year = {2014}
}

@article{louridas_power_2008,
	title = {Power {Laws} in {Software}},
	volume = {18},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/1391984.1391986},
	doi = {10.1145/1391984.1391986},
	abstract = {A single statistical framework, comprising power law distributions and scale-free networks, seems to fit a wide variety of phenomena. There is evidence that power laws appear in software at the class and function level. We show that distributions with long, fat tails in software are much more pervasive than previously established, appearing at various levels of abstraction, in diverse systems and languages. The implications of this phenomenon cover various aspects of software engineering research and practice.},
	number = {1},
	urldate = {2014-01-30},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Louridas, Panagiotis and Spinellis, Diomidis and Vlachos, Vasileios},
	month = oct,
	year = {2008},
	keywords = {power laws, Scale-free networks},
	pages = {2:1--2:26}
}

@article{chaikalis_investigating_????-1,
	title = {Investigating the effect of evolution and refactorings on feature scattering},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/article/10.1007/s11219-013-9204-4},
	doi = {10.1007/s11219-013-9204-4},
	abstract = {The implementation of a functional requirement is often distributed across several modules posing difficulties to software maintenance. In this paper, we attempt to quantify the extent of feature scattering and study its evolution with the passage of software versions. To this end, we trace the classes and methods involved in the implementation of a feature, apply formal approaches for studying variations across versions, measure whether feature implementation is uniformly distributed and visualize the reuse among features. Moreover, we investigate the impact of refactoring application on feature scattering in order to assess the circumstances under which a refactoring might improve the distribution of methods implementing a feature. The proposed techniques are exemplified for various features on several versions of four open-source projects.},
	language = {en},
	urldate = {2014-02-27},
	journal = {Software Qual J},
	author = {Chaikalis, Theodore and Chatzigeorgiou, Alexander and Examiliotou, Georgina},
	keywords = {Data Structures, Cryptology and Information Theory, Feature identification, Feature scattering, Operating Systems, Programming Languages, Compilers, Interpreters, Program understanding, Refactorings, Requirements traceability, Software Engineering/Programming and Operating Systems, Software evolution},
	pages = {1--27},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WNAF3APF\\10.html:text/html}
}

@article{glass_analysis_2004,
	title = {An {Analysis} of {Research} in {Computing} {Disciplines}},
	volume = {47},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/990680.990686},
	doi = {10.1145/990680.990686},
	abstract = {Comparing the topics and methods of the three major subdivisions of the computing realm.},
	number = {6},
	urldate = {2015-01-05},
	journal = {Commun. ACM},
	author = {Glass, Robert L. and Ramesh, V. and Vessey, Iris},
	month = jun,
	year = {2004},
	pages = {89--94}
}

@inproceedings{kagdi_improving_2007,
	address = {New York, NY, USA},
	series = {{ASE} '07},
	title = {Improving {Change} {Prediction} with {Fine}-grained {Source} {Code} {Mining}},
	isbn = {978-1-59593-882-4},
	url = {http://doi.acm.org/10.1145/1321631.1321742},
	doi = {10.1145/1321631.1321742},
	abstract = {The thesis proposes a software-change prediction approach that is based on mining fine-grained evolutionary couplings from source code repositories. Here, fine-grain refers to identifying couplings between source code entities such as methods, control structures, or even comments. This differs from current source code mining techniques that typically only identify couplings between files or fairly high-level entities. Furthermore, the model combines the mined evolutionary couplings with the estimated changes identified by traditional impact analysis techniques (e.g., static analysis of call and program-dependency graphs). The research hypothesis is that software-change prediction using the proposed synergistic approach results in an overall improved expressiveness (i.e., granularity and context given to a developer) and effectiveness (i.e., accuracy of the prediction)},
	urldate = {2014-08-27},
	booktitle = {Proceedings of the {Twenty}-second {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Kagdi, Huzefa},
	year = {2007},
	keywords = {mining software repositories, software change prediction},
	pages = {559--562},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\E6XEFU3B\\Kagdi - 2007 - Improving Change Prediction with Fine-grained Sour.pdf:application/pdf}
}

@inproceedings{brindescu_how_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {How {Do} {Centralized} and {Distributed} {Version} {Control} {Systems} {Impact} {Software} {Changes}?},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568322},
	doi = {10.1145/2568225.2568322},
	abstract = {Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are benefitting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions.   In this paper we present the first in-depth, large scale empirical study that looks at the influence of DVCS on the practice of splitting, grouping, and committing changes. We recruited 820 participants for a survey that sheds light into the practice of using DVCS. We also analyzed 409M lines of code changed by 358300 commits, made by 5890 developers, in 132 repositories containing a total of 73M LOC. Using this data, we uncovered some interesting facts. For example, (i) commits made in distributed repositories were 32\% smaller than the centralized ones, (ii) developers split commits more often in DVCS, and (iii) DVCS commits are more likely to have references to issue tracking labels.},
	urldate = {2014-06-30},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny},
	year = {2014},
	keywords = {Centralized Version Control, Distributed Version Control, software change, Version Control},
	pages = {322--333},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RUZX7XDP\\Brindescu et al. - 2014 - How Do Centralized and Distributed Version Control.pdf:application/pdf}
}

@article{khuri_response_2010,
	title = {Response surface methodology},
	volume = {2},
	copyright = {Copyright © 2010 John Wiley \& Sons, Inc.},
	issn = {1939-0068},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wics.73/abstract},
	doi = {10.1002/wics.73},
	abstract = {The purpose of this article is to provide a survey of the various stages in the development of response surface methodology (RSM). The coverage of these stages is organized in three parts that describe the evolution of RSM since its introduction in the early 1950s. Part I covers the period, 1951–1975, during which the so-called classical RSM was developed. This includes a review of basic experimental designs for fitting linear response surface models, in addition to a description of methods for the determination of optimum operating conditions. Part II, which covers the period, 1976–1999, discusses more recent modeling techniques in RSM, in addition to a coverage of Taguchi's robust parameter design and its response surface alternative approach. Part III provides a coverage of further extensions and research directions in modern RSM. This includes discussions concerning response surface models with random effects, generalized linear models, and graphical techniques for comparing response surface designs. Copyright © 2010 John Wiley \& Sons, Inc. For further resources related to this article, please visit the WIREs website.},
	language = {en},
	number = {2},
	urldate = {2015-05-18},
	journal = {WIREs Comp Stat},
	author = {Khuri, André I. and Mukhopadhyay, Siuli},
	month = mar,
	year = {2010},
	keywords = {generalized linear models, graphical procedures, multiresponse experiments, random effects, robust parameter designs},
	pages = {128--149},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\S9N584ZB\\abstract.html:text/html}
}

@inproceedings{hall_establishing_2014,
	title = {Establishing the {Source} {Code} {Disruption} {Caused} by {Automated} {Remodularisation} {Tools}},
	doi = {10.1109/ICSME.2014.75},
	abstract = {Current software remodularisation tools only operate on abstractions of a software system. In this paper, we investigate the actual impact of automated remodularisation on source code using a tool that automatically applies remodularisations as refactorings. This shows us that a typical remodularisation (as computed by the Bunch tool) will require changes to thousands of lines of code, spread throughout the system (typically no code files remain untouched). In a typical multi-developer project this presents a serious integration challenge, and could contribute to the low uptake of such tools in an industrial context. We relate these findings with our ongoing research into techniques that produce iterative commit friendly" code changes to address this problem.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Hall, M. and Khojaye, M.A. and Walkinshaw, N. and McMinn, P.},
	month = sep,
	year = {2014},
	keywords = {automated remodularisation tools, Bunch, Conferences, iterative commit friendly code changes, Java, Measurement, multideveloper project, refactoring, remodularization, Software engineering, software maintenance, software refactorings, software remodularisation tools, software system, Software systems, Software tools, source code disruption, source code (software)},
	pages = {466--470},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UMWWTZGV\\freeabs_all.html:text/html}
}

@inproceedings{bloch_how_2006,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {How to {Design} a {Good} {API} and {Why} {It} {Matters}},
	isbn = {1-59593-491-X},
	url = {http://doi.acm.org/10.1145/1176617.1176622},
	doi = {10.1145/1176617.1176622},
	abstract = {In lieu of a traditional , I've tried to distill the essence of the talk into a collection of maxims:All programmers are API designers. Good programs are modular, and intermodular boundaries define APIs. Good modules get reused.APIs can be among your greatest assets or liabilities. Good APIs create long-term customers; bad ones create long-term support nightmares.Public APIs, like diamonds, are forever. You have one chance to get it right so give it your best.APIs should be easy to use and hard to misuse. It should be easy to do simple things; possible to do complex things; and impossible, or at least difficult, to do wrong things.APIs should be self-documenting: It should rarely require documentation to read code written to a good API. In fact, it should rarely require documentation to write it.When designing an API, first gather requirements - with a healthy degree of skepticism. People often provide solutions; it's your job to ferret out the underlying problems and find the best solutions.Structure requirements as use-cases: they are the yardstick against which you'll measure your API.Early drafts of APIs should be short, typically one page with class and method signatures and one-line descriptions. This makes it easy to restructure the API when you don't get it right the first time.Code the use-cases against your API before you implement it, even before you specify it properly. This will save you from implementing, or even specifying, a fundamentally broken API.Maintain the code for uses-cases as the API evolves. Not only will this protect you from rude surprises, but the resulting code will become the examples for the API, the basis for tutorials and tests.Example code should be exemplary. If an API is used widely, its examples will be the archetypes for thousands of programs. Any mistakes will come back to haunt you a thousand fold.You can't please everyone so aim to displease everyone equally. Most APIs are overconstrained.Expect API-design mistakes due to failures of imagination. You can't reasonably hope to imagine everything that everyone will do with an API, or how it will interact with every other part of a system.API design is not a solitary activity. Show your design to as many people as you can, and take their feedback seriously. Possibilities that elude your imagination may be clear to others.Avoid fixed limits on input sizes. They limit usefulness and hasten obsolescence.If it's hard to find good names, go back to the drawing board. Don't be afraid to split or merge an API, or embed it in a more general setting. If names start falling into place, you're on the right track.Names matter. Strive for intelligibility, consistency, and symmetry. Every API is a little language, and people must learn to read and write it. If you get an API right, code will read like prose.When in doubt, leave it out. If there is a fundamental theorem of API design, this is it. It applies equally to functionality, classes, methods, and parameters. Every facet of an API should be as small as possible, but no smaller. You can always add things later, but you can't take them away.Minimizing conceptual weight is more important than class- or method-count.Keep APIs free of implementations details. They confuse users and inhibit the flexibility to evolve. It isn't always obvious what's an implementation detail: Be wary of overspecification.Minimize mutability. Immutable objects are simple, thread-safe, and freely sharable.Documentation matters. No matter how good an API, it won't get used without good documentation. Document every exported API element: every class, method, field, and parameter.Consider the performance consequences of API design decisions, but don't warp an API to achieve performance gains. Luckily, good APIs typically lend themselves to fast implementations.APIs must coexist peacefully with the platform, so do what is customary. It is almost always wrong to "transliterate" an API from one platform to another.Minimize accessibility; when in doubt, make it private. This simplifies APIs and reduces coupling.Subclass only if you can say with a straight face that every instance of the subclass is an instance of the superclass. Exposed classes should never subclass just to reuse implementation code.Design and document for inheritance or else prohibit it. This documentation takes the form of self-use patterns: how methods in a class use one another. Without it, safe subclassing is impossible.Don't make the client do anything the library could do. Violating this rule leads to boilerplate code in the client, which is annoying and error-prone.Obey the principle of least astonishment. Every method should do the least surprising thing it could, given its name. If a method doesn't do what users think it will, bugs will result.Fail fast. The sooner you report a bug, the less damage it will do. Compile-time is best. If you must fail at run-time, do it as soon as possible.Provide programmatic access to all data available in string form. Otherwise, programmers will be forced to parse strings, which is painful. Worse, the string forms will turn into de facto APIs.Overload with care. If the behaviors of two methods differ, it's better to give them different names.Use the right data type for the job. For example, don't use string if there is a more appropriate type.Use consistent parameter ordering across methods. Otherwise, programmers will get it backwards.Avoid long parameter lists, especially those with multiple consecutive parameters of the same type.Avoid return values that demand exceptional processing. Clients will forget to write the special-case code, leading to bugs. For example, return zero-length arrays or collections rather than nulls.Throw exceptions only to indicate exceptional conditions. Otherwise, clients will be forced to use exceptions for normal flow control, leading to programs that are hard to read, buggy, or slow.Throw unchecked exceptions unless clients can realistically recover from the failure.API design is an art, not a science. Strive for beauty, and trust your gut. Do not adhere slavishly to the above heuristics, but violate them only infrequently and with good reason..},
	urldate = {2014-03-26},
	booktitle = {Companion to the 21st {ACM} {SIGPLAN} {Symposium} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Bloch, Joshua},
	year = {2006},
	pages = {506--507},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\52SH33JW\\Bloch - 2006 - How to Design a Good API and Why It Matters.pdf:application/pdf}
}

@inproceedings{kitchenham_evidence-based_2004,
	title = {Evidence-based software engineering},
	doi = {10.1109/ICSE.2004.1317449},
	abstract = {Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
	booktitle = {26th {International} {Conference} on {Software} {Engineering}, 2004. {ICSE} 2004. {Proceedings}},
	author = {Kitchenham, B.A. and Dyba, Tore and Jorgensen, M.},
	month = may,
	year = {2004},
	keywords = {Australia, Best practices, Computer science, Costs, EBM, EBSE, evidence-based medicine, Evidence-based software engineering, Laboratories, medical computing, Medical services, Psychiatry, Psychology, Software engineering, software engineeringlifecycle factor, software engineeringskill factor, Technological innovation},
	pages = {273--281},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JCIZCTCX\\freeabs_all.html:text/html}
}

@article{jorgensen_forecasting_2007,
	title = {Forecasting of software development work effort: {Evidence} on expert judgement and formal models},
	volume = {23},
	issn = {0169-2070},
	shorttitle = {Forecasting of software development work effort},
	url = {http://www.sciencedirect.com/science/article/pii/S016920700700074X},
	doi = {10.1016/j.ijforecast.2007.05.008},
	abstract = {The review presented in this paper examines the evidence on the use of expert judgement, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgement-based effort estimates was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgement-based effort estimates were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated effort estimates based on a combination of expert judgement and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other estimation methods.},
	number = {3},
	urldate = {2014-08-21},
	journal = {International Journal of Forecasting},
	author = {Jørgensen, Magne},
	month = jul,
	year = {2007},
	keywords = {Combining forecasts, Comparative studies, Evaluating forecasts, Forecasting practice, Judgemental forecasting},
	pages = {449--462},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MAIDGB3U\\Jørgensen - 2007 - Forecasting of software development work effort E.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KPR5CUJD\\S016920700700074X.html:text/html}
}

@book{meyer_object-oriented_1997,
	title = {Object-oriented software construction},
	isbn = {9780136291558},
	abstract = {This is, quite simply, the definitive reference on the most important development in software technology for the last 20 years: object-orientation.A whole generation was introduced to object technology through the first edition of this book. This long-awaited new edition retains the qualities of clarity, practicality and scholarship that made the first an instant best-seller, but has been thoroughly revised and expanded. Among the new topics covered in depth are: Concurrency, distribution, client/server and the Internet; object-oriented databases; design by contract; fundamental design patterns; finding classes; the use and misuse of inheritance; abstract data types; and typing issues. The book also includes completely updated discussions of reusability, modularity, software quality, object-oriented languages, memory management, and many other essential topics.All software developers and computer science students, worldwide.},
	language = {en},
	publisher = {Prentice Hall PTR},
	author = {Meyer, Bertrand},
	year = {1997},
	keywords = {Computers / General, Computer software, Computer software - Development, Computer software/ Development, Computers / Programming / Object Oriented, Computers / Software Development \& Engineering / General, Object-oriented programming (Computer science)}
}

@article{zhou_predicting_2007,
	series = {The {Impact} of {Barry} {Boehm}’s {Work} on {Software} {Engineering} {Education} and {Training}},
	title = {Predicting object-oriented software maintainability using multivariate adaptive regression splines},
	volume = {80},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121206003372},
	doi = {10.1016/j.jss.2006.10.049},
	abstract = {Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique.},
	number = {8},
	urldate = {2014-08-22},
	journal = {Journal of Systems and Software},
	author = {Zhou, Yuming and Leung, Hareton},
	month = aug,
	year = {2007},
	keywords = {Maintainability, Multiple adaptive regression splines, Object-oriented, Prediction},
	pages = {1349--1361},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\S97K9M6J\\Zhou and Leung - 2007 - Predicting object-oriented software maintainabilit.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\6F8ZT826\\S0164121206003372.html:text/html}
}

@incollection{kessentini_search-based_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Search-{Based} {Design} {Defects} {Detection} by {Example}},
	copyright = {©2011 Springer Berlin Heidelberg},
	isbn = {978-3-642-19810-6, 978-3-642-19811-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-19811-3_28},
	abstract = {We propose an automated approach to detect various types of design defects in source code. Our approach allows to automatically find detection rules, thus relieving the designer from doing so manually. Rules are defined as combinations of metrics/thresholds that better conform to known instances of design defects (defect examples). In our setting, we use and compare between different heuristic search algorithms for rule extraction: Harmony Search, Particle Swarm Optimization, and Simulated Annealing. We evaluate our approach by finding potential defects in two open-source systems. For all these systems, we found, in average, more than 75\% of known defects, a better result when compared to a state-of-the-art approach, where the detection rules are manually or semi-automatically specified.},
	language = {en},
	number = {6603},
	urldate = {2015-05-18},
	booktitle = {Fundamental {Approaches} to {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kessentini, Marouane and Sahraoui, Houari and Boukadoum, Mounir and Wimmer, Manuel},
	editor = {Giannakopoulou, Dimitra and Orejas, Fernando},
	year = {2011},
	keywords = {by example, Computer Communication Networks, design defects, Logics and Meanings of Programs, Management of Computing and Information Systems, Metrics, Programming Languages, Compilers, Interpreters, Programming Techniques, search-based software engineering, Software engineering, Software quality},
	pages = {401--415},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NXFRAESN\\Kessentini et al. - 2011 - Search-Based Design Defects Detection by Example.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PGIKNREH\\978-3-642-19811-3_28.html:text/html}
}

@misc{_wikipedia_2014,
	title = {Wikipedia, the free encyclopedia},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {http://en.wikipedia.org/w/index.php?title=Main_Page&oldid=598252063},
	language = {en},
	urldate = {2014-03-29},
	journal = {Wikipedia, the free encyclopedia},
	month = mar,
	year = {2014},
	note = {Page Version ID: 598252063},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NBIXUBAI\\index.html:text/html}
}

@misc{center_for_history_and_new_media_zotero_????,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}},
	annote = {Welcome to Zotero!View the Quick Start Guide to learn how to begin collecting, managing, citing, and sharing your research sources.Thanks for installing Zotero.}
}

@incollection{taube-schock_can_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Can {We} {Avoid} {High} {Coupling}?},
	copyright = {©2011 Springer-Verlag GmbH Berlin Heidelberg},
	isbn = {978-3-642-22654-0, 978-3-642-22655-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-22655-7_10},
	abstract = {It is considered good software design practice to organize source code into modules and to favour within-module connections (cohesion) over between-module connections (coupling), leading to the oft-repeated maxim “low coupling/high cohesion”. Prior research into network theory and its application to software systems has found evidence that many important properties in real software systems exhibit approximately scale-free structure, including coupling; researchers have claimed that such scale-free structures are ubiquitous. This implies that high coupling must be unavoidable, statistically speaking, apparently contradicting standard ideas about software structure. We present a model that leads to the simple predictions that approximately scale-free structures ought to arise both for between-module connectivity and overall connectivity, and not as the result of poor design or optimization shortcuts. These predictions are borne out by our large-scale empirical study. Hence we conclude that high coupling is not avoidable—and that this is in fact quite reasonable.},
	number = {6813},
	urldate = {2014-02-21},
	booktitle = {{ECOOP} 2011 – {Object}-{Oriented} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Taube-Schock, Craig and Walker, Robert J. and Witten, Ian H.},
	editor = {Mezini, Mira},
	month = jan,
	year = {2011},
	keywords = {Computer Communication Networks, Logics and Meanings of Programs, Management of Computing and Information Systems, Programming Languages, Compilers, Interpreters, Programming Techniques, Software engineering},
	pages = {204--228},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HK8QVRZN\\10.html:text/html}
}

@article{breslow_generalized_1970,
	title = {A generalized {Kruskal}-{Wallis} test for comparing {K} samples subject to unequal patterns of censorship},
	volume = {57},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/57/3/579},
	doi = {10.1093/biomet/57.3.579},
	abstract = {SUMMARY A generalization of the Kruskal-Wallis test, which extends Gehan's generalization of Wilcoxon's test, is proposed for testing the equality of K continuous distribution functions when observations are subject to arbitrary right censorship. The distribution of the censoring variables is allowed to differ for different populations. An alternative statistic is proposed for use when the censoring distributions may be assumed equal. These statistics have asymptotic chi-squared distributions under their respective null hypotheses, whether the censoring variables are regarded as random or as fixed numbers. Asymptotic power and efficiency calculations are made and numerical examples provided.},
	language = {en},
	number = {3},
	urldate = {2014-01-20},
	journal = {Biometrika},
	author = {Breslow, Norman},
	month = dec,
	year = {1970},
	pages = {579--594},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\G53A64PI\\579.html:text/html}
}

@inproceedings{petersen_systematic_2008,
	address = {Swinton, UK, UK},
	series = {{EASE}'08},
	title = {Systematic {Mapping} {Studies} in {Software} {Engineering}},
	url = {http://dl.acm.org/citation.cfm?id=2227115.2227123},
	abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
	urldate = {2014-01-20},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {British Computer Society},
	author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
	year = {2008},
	keywords = {evidence based software engineering, systematic mapping studies, systematic reviews},
	pages = {68--77}
}

@article{li_study_2009,
	title = {A study of project selection and feature weighting for analogy based software cost estimation},
	volume = {82},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121208001325},
	doi = {10.1016/j.jss.2008.06.001},
	abstract = {A number of software cost estimation methods have been presented in literature over the past decades. Analogy based estimation (ABE), which is essentially a case based reasoning (CBR) approach, is one of the most popular techniques. In order to improve the performance of ABE, many previous studies proposed effective approaches to optimize the weights of the project features (feature weighting) in its similarity function. However, ABE is still criticized for the low prediction accuracy, the large memory requirement, and the expensive computation cost. To alleviate these drawbacks, in this paper we propose the project selection technique for ABE (PSABE) which reduces the whole project base into a small subset that consist only of representative projects. Moreover, PSABE is combined with the feature weighting to form FWPSABE for a further improvement of ABE. The proposed methods are validated on four datasets (two real-world sets and two artificial sets) and compared with conventional ABE, feature weighted ABE (FWABE), and machine learning methods. The promising results indicate that project selection technique could significantly improve analogy based models for software cost estimation.},
	number = {2},
	urldate = {2014-08-27},
	journal = {Journal of Systems and Software},
	author = {Li, Y. F. and Xie, M. and Goh, T. N.},
	month = feb,
	year = {2009},
	keywords = {Analogy based estimation, Artificial datasets, Feature weighting, Genetic algorithm, Project selection, software cost estimation},
	pages = {241--252},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\W838G7AJ\\S0164121208001325.html:text/html}
}

@article{glass_analysis_2004-1,
	title = {An {Analysis} of {Research} in {Computing} {Disciplines}},
	volume = {47},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/990680.990686},
	doi = {10.1145/990680.990686},
	abstract = {Comparing the topics and methods of the three major subdivisions of the computing realm.},
	number = {6},
	urldate = {2014-01-20},
	journal = {Commun. ACM},
	author = {Glass, Robert L. and Ramesh, V. and Vessey, Iris},
	month = jun,
	year = {2004},
	pages = {89--94}
}

@inproceedings{mcdonnell_empirical_2013,
	title = {An {Empirical} {Study} of {API} {Stability} and {Adoption} in the {Android} {Ecosystem}},
	doi = {10.1109/ICSM.2013.18},
	abstract = {When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28\% of API references in client applications are outdated with a median lagging time of 16 months. 22\% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.},
	booktitle = {2013 29th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {McDonnell, T. and Ray, B. and Kim, Miryung},
	month = sep,
	year = {2013},
	keywords = {Android API coevolution behavior, Android ecosystem, Androids, API evolution, API Stability, API usage adaptation code, application program interfaces, github, Google, History, Humanoid robots, Mobile communication, mobile computing, operating systems (computers), Smart phones, Software, software ecosystems, software maintenance, version history data},
	pages = {70--79},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TWPR3PVG\\freeabs_all.html:text/html}
}

@book{deo_graph_1974,
	title = {Graph {Theory} with {Applications} to {Engineering} and {Computer} {Science}},
	isbn = {9788120301450},
	language = {en},
	publisher = {PHI Learning Pvt. Ltd.},
	author = {Deo, Narsingh},
	year = {1974}
}

@inproceedings{dyer_boa:_2013,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '13},
	title = {Boa: {A} {Language} and {Infrastructure} for {Analyzing} {Ultra}-large-scale {Software} {Repositories}},
	isbn = {978-1-4673-3076-3},
	shorttitle = {Boa},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486844},
	abstract = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
	urldate = {2014-03-11},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
	year = {2013},
	pages = {422--431},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CIFMX4MK\\Dyer et al. - 2013 - Boa A Language and Infrastructure for Analyzing U.pdf:application/pdf}
}

@book{das_statistics_????,
	title = {Statistics for {Business} {Decisions}},
	isbn = {9789380599557},
	language = {en},
	publisher = {Academic Publishers},
	author = {Das, J. K.}
}

@misc{_figshare_????,
	title = {figshare - credit for all your research},
	url = {http://figshare.com/}
}

@inproceedings{gomez-perez_evaluation_1999,
	address = {Banff, Alberta, Canada},
	title = {Evaluation of {Taxonomic} {Knowledge} in {Ontologies} and {Knowledge} {Bases}},
	volume = {2},
	url = {http://sern.ucalgary.ca/KSI/KAW/KAW99},
	booktitle = {Banff {Knowledge} {Acquisition} for {Knowledge}-{Based} {Systems}, {KAW}'99},
	publisher = {University of Calgary, Alberta, Canada},
	author = {Gómez-Pérez, Asunción},
	month = oct,
	year = {1999},
	pages = {6.1.1--6.1.18}
}

@inproceedings{kessentini_deviance_2010,
	address = {New York, NY, USA},
	series = {{ASE} '10},
	title = {Deviance from {Perfection} is a {Better} {Criterion} {Than} {Closeness} to {Evil} when {Identifying} {Risky} {Code}},
	isbn = {978-1-4503-0116-9},
	url = {http://doi.acm.org/10.1145/1858996.1859015},
	doi = {10.1145/1858996.1859015},
	abstract = {We propose an approach for the automatic detection of potential design defects in code. The detection is based on the notion that the more code deviates from good practices, the more likely it is bad. Taking inspiration from artificial immune systems, we generated a set of detectors that characterize different ways that a code can diverge from good practices. We then used these detectors to measure how far code in assessed systems deviates from normality. We evaluated our approach by finding potential defects in two open-source systems (Xerces-J and Gantt). We used the library JHotDraw as the code base representing good design/programming practices. In both systems, we found that 90\% of the riskiest classes were defects, a precision far superiour to state of the art rule-based approaches.},
	urldate = {2015-05-18},
	booktitle = {Proceedings of the {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Kessentini, Marouane and Vaucher, Stéphane and Sahraoui, Houari},
	year = {2010},
	keywords = {artificial immune systems, design defects, maintenance},
	pages = {113--122},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\X82JP59N\\Kessentini et al. - 2010 - Deviance from Perfection is a Better Criterion Tha.pdf:application/pdf}
}

@misc{_campwood_2014,
	title = {Campwood {Software} – {Source} {Monitor}},
	url = {http://www.campwoodsw.com/sourcemonitor.html},
	urldate = {2014-03-11},
	year = {2014}
}

@article{valverde_hierarchical_2003,
	title = {Hierarchical {Small} {Worlds} in {Software} {Architecture}},
	url = {http://arxiv.org/abs/cond-mat/0307278},
	abstract = {In this paper, we present a complex network approach to the study of software engineering. We have found universal network patterns in a large collection of object-oriented (OO) software systems written in C++ and Java. All the systems analyzed here display the small-world behavior, that is, the average distance between any pair of classes is very small even when coupling is low and cohesion is high. In addition, the structure of OO software is a very heterogeneous network characterized by a degree distribution following a power-law with similar exponents. We have investigated the origin of these universal patterns. Our study suggests that some features of OO programing languages, like encapsulation, seem to be largely responsible for the small-world behavior. On the other hand, software heterogeneity is largely independent of the purpose and objectives of the particular system under study and appears to be related to a pattern of constrained growth. A number of software engineering topics may benefit from the present approach, including empirical software measurement and program comprehension.},
	urldate = {2015-01-10},
	journal = {arXiv:cond-mat/0307278},
	author = {Valverde, Sergi and Sole, Ricard V.},
	month = jul,
	year = {2003},
	note = {arXiv: cond-mat/0307278},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: Accepted for publication in Special Issue on Software Engineering and Complex Networks Dynamics of Continuous, Discrete and Impulsive Systems Series B (2007)},
	file = {arXiv\:cond-mat/0307278 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\B6U4N9TN\\Valverde and Sole - 2003 - Hierarchical Small Worlds in Software Architecture.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\9ZMV4H48\\0307278.html:text/html}
}

@incollection{olson_performance_2008,
	title = {Performance {Evaluation} for {Predictive} {Modeling}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_9},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {137--147},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\6P2CJCQJ\\978-3-540-76917-0_9.html:text/html}
}

@book{tulach_practical_2012,
	address = {[New York]; New York},
	title = {Practical {API} design: confessions of a {Java} framework architect},
	isbn = {9781430243175  1430243171},
	shorttitle = {Practical {API} design},
	language = {English},
	publisher = {Apress ; Distributed to the book trade worldwide by Springer Science+Business Media New York},
	author = {Tulach, Jaroslav},
	year = {2012}
}

@inproceedings{leskovec_graphs_2005,
	title = {Graphs over {Time}: {Densification} {Laws}, {Shrinking} {Diameters} and {Possible} {Explanations}},
	shorttitle = {Graphs over {Time}},
	abstract = {How do real graphs evolve over time? What are "normal" growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.},
	booktitle = {In {KDD}},
	publisher = {ACM Press},
	author = {Leskovec, Jurij and Kleinberg, Jon and Faloutsos, Christos},
	year = {2005},
	pages = {177--187},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\F2JIJQIS\\Leskovec et al. - 2005 - Graphs over Time Densification Laws, Shrinking Di.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\4VI87R4G\\summary.html:text/html}
}

@article{brereton_lessons_2007,
	series = {Software {Performance} 5th {International} {Workshop} on {Software} and {Performance}},
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S016412120600197X},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.
The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
	number = {4},
	urldate = {2014-03-26},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	keywords = {Empirical software engineering, Systematic literature review},
	pages = {571--583},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\22ESK2BR\\Brereton et al. - 2007 - Lessons from applying the systematic literature re.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CAB58ZJH\\S016412120600197X.html:text/html}
}

@inproceedings{parnas_software_1994,
	address = {Los Alamitos, CA, USA},
	series = {{ICSE} '94},
	title = {Software {Aging}},
	isbn = {0-8186-5855-X},
	url = {http://dl.acm.org/citation.cfm?id=257734.257788},
	urldate = {2015-03-27},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society Press},
	author = {Parnas, David Lorge},
	year = {1994},
	pages = {279--287},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UCNQQPI9\\Parnas - 1994 - Software Aging.pdf:application/pdf}
}

@article{concas_power-laws_2007,
	title = {Power-{Laws} in a {Large} {Object}-{Oriented} {Software} {System}},
	volume = {33},
	issn = {0098-5589},
	doi = {10.1109/TSE.2007.1019},
	abstract = {We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto - or sometimes log-normal - distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data?s statistical distributions.},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Concas, G. and Marchesi, M. and Pinna, S. and Serra, N.},
	month = oct,
	year = {2007},
	keywords = {D.2.3.a Object-oriented programming, D.2.4.h Statistical methods, D.2.8.a Complexity measures, D.2.8.d Product metrics, D.2.8.e Software science, D.3.2.p Object-oriented languages, G.3.p Stochastic processes, Java, Object oriented modeling, Object oriented programming, Power generation, Power system modeling, Shape measurement, Software systems, Statistical analysis, Statistical distributions, Tail},
	pages = {687--708},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5MGMEDFG\\login.html:text/html}
}

@article{simons_elegant_2012,
	title = {Elegant {Object}-{Oriented} {Software} {Design} via {Interactive}, {Evolutionary} {Computation}},
	volume = {42},
	issn = {1094-6977},
	doi = {10.1109/TSMCC.2012.2225103},
	abstract = {Design is fundamental to software development but can be demanding to perform. Thus, to assist the software designer, evolutionary computing is being increasingly applied using machine-based, quantitative fitness functions to evolve software designs. However, in nature, elegance and symmetry play a crucial role in the reproductive fitness of various organisms. In addition, subjective evaluation has also been exploited in interactive evolutionary computation (IEC). Therefore, to investigate the role of elegance and symmetry in software design, four novel elegance measures are proposed which are based on the evenness of distribution of design elements. In controlled experiments in a dynamic IEC environment, designers are presented with visualizations of object-oriented software designs, which they rank according to a subjective assessment of elegance. For three out of the four elegance measures proposed, it is found that a significant correlation exists between elegance values and reward elicited. These three elegance measures assess the evenness of distribution of 1) attributes and methods among classes; 2) external couples between classes; and 3) the ratio of attributes to methods. It is concluded that symmetrical elegance is in some way significant in software design, and that this can be exploited in dynamic, multiobjective IEC to produce elegant software designs.},
	number = {6},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews},
	author = {Simons, C.L. and Parmee, I.C.},
	month = nov,
	year = {2012},
	keywords = {design element, Design methodology, distribution evenness assessment, Elegance, elegance measure, elegant object-oriented software design, evolutionary computation, interactive evolutionary computation, interactive evolutionary computation (IEC), machine-based function, object-oriented programming, object-oriented software design visualization, quantitative fitness function, reproductive fitness, Software design, software development, Software engineering, Software measurement, subjective elegance assessment, subjective evaluation, symmetrical elegance, symmetry},
	pages = {1797--1805},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QU2I4ID8\\freeabs_all.html:text/html}
}

@article{agrawal_software_2007,
	title = {Software {Effort}, {Quality}, and {Cycle} {Time}: {A} {Study} of {CMM} {Level} 5 {Projects}},
	volume = {33},
	issn = {0098-5589},
	shorttitle = {Software {Effort}, {Quality}, and {Cycle} {Time}},
	doi = {10.1109/TSE.2007.29},
	abstract = {The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle time. Prior research literature, while not exclusively focusing on CMM level 5 projects, has identified a host of factors as determinants of software development effort, quality, and cycle time. In this study, we focus exclusively on CMM level 5 projects from multiple organizations to study the impacts of highly mature processes on effort, quality, and cycle time. Using a linear regression model based on data collected from 37 CMM level 5 projects of four organizations, we find that high levels of process maturity, as indicated by CMM level 5 rating, reduce the effects of most factors that were previously believed to impact software development effort, quality, and cycle time. The only factor found to be significant in determining effort, cycle time, and quality was software size. On the average, the developed models predicted effort and cycle time around 12 percent and defects to about 49 percent of the actuals, across organizations. Overall, the results in this paper indicate that some of the biggest rewards from high levels of process maturity come from the reduction in variance of software development outcomes that were caused by factors other than software size},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Agrawal, Manish and Chari, Kaushal},
	month = mar,
	year = {2007},
	keywords = {Best practices, Capability Maturity Model, CMM level 5 project, Coordinate measuring machines, Cost estimation, Costs, ISO standards, linear regression model, Object oriented modeling, organization productivity, Productivity, productivity., Programming, regression analysis, Six sigma, software cost estimation, software cycle time estimation, software development effort, software development management, software development process improvement, Software quality, time estimation},
	pages = {145--156},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WKAFDQ8V\\abs_all.html:text/html}
}

@article{miller_replicating_2005,
	title = {Replicating software engineering experiments: a poisoned chalice or the {Holy} {Grail}},
	volume = {47},
	issn = {0950-5849},
	shorttitle = {Replicating software engineering experiments},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584904001259},
	doi = {10.1016/j.infsof.2004.08.005},
	abstract = {Recently, software engineering has witnessed a great increase in the amount of work with an empirical component; however, this work has often little or no established empirical framework within the topic to draw upon. Frequently, researchers use frameworks from other disciplines in an attempt to alleviate this deficiency. A common underpinning in these frameworks is that experimental replication is available as the cornerstone of knowledge discovery within the discipline. This paper investigates the issues involved in accepting this premise as a fundamental building block with empirical software engineering and recommends extending the traditional view of replication to improve the effectiveness of this essential process within our domain.},
	number = {4},
	urldate = {2015-01-05},
	journal = {Information and Software Technology},
	author = {Miller, James},
	month = mar,
	year = {2005},
	keywords = {Empirical values, Real world settings, Software engineer},
	pages = {233--244},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\XKF2B7VM\\S0950584904001259.html:text/html}
}

@misc{maestrosnew_._2007,
	title = {Γ.Νταλάρας - Όταν σφίγγουν το χέρι (Θεοδωράκης - Ρίτσος)},
	url = {http://www.youtube.com/watch?v=Ml9vXQecaPE&feature=youtube_gdata_player},
	abstract = {Dalaras sings Otan sfiggoun to heri, a song from Romiosini at Megaron Mousikis Athinon. George Kimoulis is reading a text of Giannis Ritsos at the beggining. Romiosini by Mikis Theodorakis, Giannis Ritsos.},
	urldate = {2014-02-22},
	collaborator = {{maestrosnew}},
	month = may,
	year = {2007}
}

@article{moore_utilization_2013,
	title = {Utilization of {Relationship}-{Oriented} {Social} {Media} in the {Selling} {Process}: {A} {Comparison} of {Consumer} ({B}2C) and {Industrial} ({B}2B) {Salespeople}},
	volume = {12},
	issn = {1533-2861},
	shorttitle = {Utilization of {Relationship}-{Oriented} {Social} {Media} in the {Selling} {Process}},
	url = {http://dx.doi.org/10.1080/15332861.2013.763694},
	doi = {10.1080/15332861.2013.763694},
	abstract = {The rapid pace of technological advancement and adoption of social media among consumers/organizations has been unprecedented in recent years. This study provides insights into understanding social media utilization among professional salespeople. Specifically, social media applications are separated into 15 categories, with multiple applications falling within each category. From a sample of 395 salespeople in B2B and B2C markets, utilization of relationship-oriented social media applications are presented and examined. Overall, findings show that B2B practitioners tend to use media targeted at professionals whereas their B2C counterparts tend to utilize more sites targeted to the general public for engaging in one-on-one dialogue with their customers. Moreover, B2B professionals tend to use relationship-oriented social media technologies more than B2C professionals for the purpose of prospecting, handling objections, and after sale follow-up.},
	number = {1},
	urldate = {2015-02-05},
	journal = {Journal of Internet Commerce},
	author = {Moore, Jesse N. and Hopkins, Christopher D. and Raymond, Mary Anne},
	month = feb,
	year = {2013},
	pages = {48--75},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KXC7TZT6\\Moore et al. - 2013 - Utilization of Relationship-Oriented Social Media .pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8ZJ2G6DC\\15332861.2013.html:text/html}
}

@article{lehman_understanding_1979,
	title = {On understanding laws, evolution, and conservation in the large-program life cycle},
	volume = {1},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/0164121279900220},
	doi = {10.1016/0164-1212(79)90022-0},
	abstract = {The paper presents interpretations of some recently discovered laws of evolution and conservation in the largeprogram life cycle.
Program development and maintenance processes are managed and implemented by people; thus in the long term they could be expected to be unpredictable, dependant on the judgments, whims, and actions of programming process participants (e.g., managers, programmers, and product users). Yet, observed, measured, and modeled regularities suggest laws that are closer to biological laws or those of modern physics than to those currently formulated in other areas subject to human influence (e.g., economics and sociology).
After a brief discussion of the first four laws, to highlight underlying phenomena and natural attributes of the program evolution process, the paper concentrates on a fifth law and shows how, and why, this law represents a conservation phenomenon: the Conservation of Familiarity.},
	urldate = {2014-03-14},
	journal = {Journal of Systems and Software},
	author = {Lehman, M. M.},
	year = {1979},
	pages = {213--221},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DSS4MMKX\\Lehman - 1979 - On understanding laws, evolution, and conservation.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KCTK52PQ\\0164121279900220.html:text/html}
}

@inproceedings{bird_putting_2009,
	title = {Putting {It} {All} {Together}: {Using} {Socio}-technical {Networks} to {Predict} {Failures}},
	shorttitle = {Putting {It} {All} {Together}},
	doi = {10.1109/ISSRE.2009.17},
	abstract = {Studies have shown that social factors in development organizations have a dramatic effect on software quality. Separately, program dependency information has also been used successfully to predict which software components are more fault prone. Interestingly, the influence of these two phenomena have only been studied separately. Intuition and practical experience suggests,however, that task assignment (i.e. who worked on which components and how much) and dependency structure (which components have dependencies on others)together interact to influence the quality of the resulting software. We study the influence of combined socio-technical software networks on the fault-proneness of individual software components within a system. The network properties of a software component in this combined network are able to predict if an entity is failure prone with greater accuracy than prior methods which use dependency or contribution information in isolation. We evaluate our approach in different settings by using it on Windows Vista and across six releases of the Eclipse development environment including using models built from one release to predict failure prone components in the next release. We compare this to previous work. In every case, our method performs as well or better and is able to more accurately identify those software components that have more post-release failures, with precision and recall rates as high as 85\%.},
	booktitle = {20th {International} {Symposium} on {Software} {Reliability} {Engineering}, 2009. {ISSRE} '09},
	author = {Bird, C. and Nagappan, N. and Gall, H. and Murphy, B. and Devanbu, P.},
	month = nov,
	year = {2009},
	keywords = {bug prediction, Costs, dependency structure, development organization, Eclipse development environment, empirical studies, failure prone components prediction, History, human factors, individual software components fault proneness, NIST, Predictive models, Reliability engineering, social factor, Social factors, social networks, social sciences, socio-technical network, software component network property, software development management, software fault tolerance, Software performance, Software quality, software reliability, Software systems, task assignment},
	pages = {109--119},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ASJ434DI\\login.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\6EH47R52\\Bird et al. - 2009 - Putting It All Together Using Socio-technical Net.pdf:application/pdf}
}

@inproceedings{sjoberg_future_2007,
	title = {The {Future} of {Empirical} {Methods} in {Software} {Engineering} {Research}},
	doi = {10.1109/FOSE.2007.30},
	abstract = {We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
	booktitle = {Future of {Software} {Engineering}, 2007. {FOSE} '07},
	author = {Sjoberg, Dag I.K. and Dyba, Tore and Jorgensen, M.},
	month = may,
	year = {2007},
	keywords = {Computer industry, Computer science, Computer Society, empirical methods, Informatics, Information science, Laboratories, Project management, scientific knowledge, software development management, Software engineering, Software systems},
	pages = {358--378},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\K4454UCG\\freeabs_all.html:text/html}
}

@misc{_sonarqube_????,
	type = {Open source platform to manage code quality},
	title = {{SonarQube}™ - {Open} source platform to manage code quality},
	url = {http://www.sonarqube.org/},
	urldate = {2014-06-23},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\AUWXGP2B\\www.sonarqube.org.html:text/html}
}

@article{josh_bloch_lcsd:_2006,
	title = {{LCSD}: library-centric software design.},
	shorttitle = {{LCSD}},
	doi = {10.1145/1176617.1176634},
	author = {Josh Bloch, Jaakko Järvi},
	year = {2006},
	pages = {618},
	file = {LCSD\: library-centric software design. - ResearchGate:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5Q2RJBW9\\221320828_LCSD_library-centric_software_design.html:text/html}
}

@article{arcuri_hitchhikers_2014,
	title = {A {Hitchhiker}'s guide to statistical tests for assessing randomized algorithms in software engineering},
	volume = {24},
	copyright = {Copyright © 2012 John Wiley \& Sons, Ltd.},
	issn = {1099-1689},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/stvr.1486/abstract},
	doi = {10.1002/stvr.1486},
	abstract = {Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2015-05-18},
	journal = {Softw. Test. Verif. Reliab.},
	author = {Arcuri, Andrea and Briand, Lionel},
	month = may,
	year = {2014},
	keywords = {Bonferroni adjustment, confidence interval, effect size, nonparametric test, parametric test, statistical difference, survey, Systematic review},
	pages = {219--250},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2B8C8R96\\abstract.html:text/html}
}

@inproceedings{briand_assessment_1999,
	title = {An assessment and comparison of common software cost estimation modeling techniques},
	abstract = {This paper investigates two essential questions related to data-driven, software cost modeling: (1) What modeling techniques are likely to yield more accurate results when using typical software development cost data? and (2) What are the benefits and drawbacks of using organization-specific data as compared to multi-organization databases? The former question is important in guiding software cost analysts in their choice of the right type of modeling technique, if at all possible. In order to address this issue, we assess and compare a selection of common cost modeling techniques fulfilling a number of important criteria using a large multi-organizational database in the business application domain. Namely, these are: ordinary least squares regression, stepwise ANOVA, CART, and analogy. The latter question is important in order to assess the feasibility of using multi-organization cost databases to build cost models and the benefits gained from local, company-specific data collection and modeling. As a large subset of the data in the multi-company database came from one organization, we were able to investigate this issue by comparing organization-specific models with models based on multi-organization data. Results show that the performances of the modeling techniques considered were not significantly different, with the exception of the analogy-based models which appear to be less accurate. Surprisingly, when using standard cost factors (e.g., COCOMO-like factors, Function Points), organization specific models did not yield better results than generic, multi-organization models.},
	booktitle = {Proceedings of the 1999 {International} {Conference} on {Software} {Engineering}, 1999},
	author = {Briand, L.C. and El Emam, K. and Surmann, D. and Wieczorek, I and Maxwell, K.D.},
	month = may,
	year = {1999},
	keywords = {analysis of variance, Application software, Business, CART, Cost function, Databases, data-driven, Least squares methods, multi-organizational database, ordinary least squares regression, Programming, Regression tree analysis, software cost estimation, software cost modeling, Software engineering, Statistical analysis, stepwise ANOVA, Yield estimation},
	pages = {313--323},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GRHASE2H\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\H3498V44\\Briand et al. - 1999 - An assessment and comparison of common software co.pdf:application/pdf}
}

@article{miyazaki_robust_1994,
	title = {Robust regression for developing software estimation models},
	volume = {27},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/0164121294901104},
	doi = {10.1016/0164-1212(94)90110-4},
	abstract = {To develop a good software estimation model fitted to actual data, the evaluation criteria of goodness of fit is necessary. The first major problem discussed here is that ordinary relative error used for this criterion is not suitable because it has a bound in the case of under-estimation and no bound in the case of overestimation. We propose use of a new relative error called balanced relative error as the basis for the criterion and introduce seven evaluation criteria for software estimation models. The second major problem is that the ordinary least-squares method used for calculation of parameter values of a software estimation model is neither consistent with the criteria nor robust enough, which means that the solution is easily distorted by outliers. We propose a new consistent and robust method called the least-squares of inverted balanced relative errors (LIRS) and demonstrates its superiority to the ordinary least-squares method by use of five actual data sets. Through the analysis of these five data sets with LIRS, we show the importance of consistent data collection and development standarization to develop a good software sizing model. We compare goodness of fit between the sizing model based on the number of screens, forms, and files, and the sizing model based on the number of data elements for each of them. Based on this comparison, the validity of the number of data elements as independent variables for a sizing model is examined. Moreover, the validity of increasing the number of independent variables is examined.},
	number = {1},
	urldate = {2014-08-21},
	journal = {Journal of Systems and Software},
	author = {Miyazaki, Y. and Terakado, M. and Ozaki, K. and Nozaki, H.},
	month = oct,
	year = {1994},
	pages = {3--16},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BGH4VXM7\\0164121294901104.html:text/html}
}

@inproceedings{arisholm_predicting_2006,
	address = {New York, NY, USA},
	series = {{ISESE} '06},
	title = {Predicting {Fault}-prone {Components} in a {Java} {Legacy} {System}},
	isbn = {1-59593-218-6},
	url = {http://doi.acm.org/10.1145/1159733.1159738},
	doi = {10.1145/1159733.1159738},
	abstract = {This paper reports on the construction and validation of faultproneness prediction models in the context of an object-oriented, evolving, legacy system. The goal is to help QA engineers focus their limited verification resources on parts of the system likely to contain faults. A number of measures including code quality, class structure, changes in class structure, and the history of class-level changes and faults are included as candidate predictors of class fault-proneness. A cross-validated classification analysis shows that the obtained model has less than 20\% of false positives and false negatives, respectively. However, as shown in this paper, statistics regarding the classification accuracy tend to inflate the potential usefulness of the fault-proneness prediction models. We thus propose a simple and pragmatic methodology for assessing the costeffectiveness of the predictions to focus verification effort. On the basis of the cost-effectiveness analysis we show that change and fault data from previous releases is paramount to developing a practically useful prediction model. When our model is applied to predict faults in a new release, the estimated potential savings in verification effort is about 29\%. In contrast, the estimated savings in verification effort drops to 0\% when history data is not included.},
	urldate = {2014-08-21},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering}},
	publisher = {ACM},
	author = {Arisholm, Erik and Briand, Lionel C.},
	year = {2006},
	pages = {8--17},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\X5S8NVRA\\Arisholm and Briand - 2006 - Predicting Fault-prone Components in a Java Legacy.pdf:application/pdf}
}

@article{myers_software_2003,
	title = {Software systems as complex networks: {Structure}, function, and evolvability of software collaboration graphs},
	volume = {68},
	shorttitle = {Software systems as complex networks},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.68.046116},
	doi = {10.1103/PhysRevE.68.046116},
	abstract = {Software systems emerge from mere keystrokes to form intricate functional networks connecting many collaborating modules, objects, classes, methods, and subroutines. Building on recent advances in the study of complex networks, I have examined software collaboration graphs contained within several open-source software systems, and have found them to reveal scale-free, small-world networks similar to those identified in other technological, sociological, and biological systems. I present several measures of these network topologies, and discuss their relationship to software engineering practices. I also present a simple model of software system evolution based on refactoring processes which captures some of the salient features of the observed systems. Some implications of object-oriented design for questions about network robustness, evolvability, degeneracy, and organization are discussed in the wake of these findings.},
	number = {4},
	urldate = {2014-01-30},
	journal = {Phys. Rev. E},
	author = {Myers, Christopher R.},
	month = oct,
	year = {2003},
	pages = {046116},
	file = {APS Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HKX6BCAZ\\e046116.html:text/html}
}

@misc{_promisedata_????,
	title = {Promisedata - {Data} for reusable {SE} experiments},
	url = {https://code.google.com/p/promisedata/}
}

@book{lehmann_testing_2005,
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {9780387988641},
	abstract = {The third edition of Testing Statistical Hypotheses updates and expands upon the classic graduate text, emphasizing optimality theory for hypothesis testing and confidence sets. The principal additions include a rigorous treatment of large sample optimality, together with the requisite tools. In addition, an introduction to the theory of resampling methods such as the bootstrap is developed. The sections on multiple testing and goodness of fit testing are expanded. The text is suitable for Ph.D. students in statistics and includes over 300 new problems out of a total of more than 760.  E.L. Lehmann is Professor of Statistics Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands and the University of Chicago. He is the author of Elements of Large-Sample Theory and (with George Casella) he is also the author of Theory of Point Estimation, Second Edition.  Joseph P. Romano is Professor of Statistics at Stanford University. He is a recipient of a Presidential Young Investigator Award and a Fellow of the Institute of Mathematical Statistics. He has coauthored two other books, Subsampling with Dimitris Politis and Michael Wolf, and Counterexamples in Probability and Statistics with Andrew Siegel.},
	language = {en},
	publisher = {Springer},
	author = {Lehmann, Erich L. and Romano, Joseph P.},
	month = apr,
	year = {2005},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{dambros_evaluating_2012,
	title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
	volume = {17},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Evaluating defect prediction approaches},
	url = {http://link.springer.com/article/10.1007/s10664-011-9173-9},
	doi = {10.1007/s10664-011-9173-9},
	abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
	language = {en},
	number = {4-5},
	urldate = {2014-07-22},
	journal = {Empir Software Eng},
	author = {D’Ambros, Marco and Lanza, Michele and Robbes, Romain},
	month = aug,
	year = {2012},
	keywords = {Change metrics, defect prediction, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, Source code metrics},
	pages = {531--577},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\74ZM2ENH\\D’Ambros et al. - 2012 - Evaluating defect prediction approaches a benchma.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BC63HSSV\\s10664-011-9173-9.html:text/html}
}

@incollection{olson_memory-based_2008,
	title = {Memory-{Based} {Reasoning} {Methods}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_3},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {39--52},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\Z9PAI36C\\978-3-540-76917-0_3.html:text/html}
}

@article{kitchenham_principles_2002,
	title = {Principles of {Survey} {Research} {Part} 4: {Questionnaire} {Evaluation}},
	volume = {27},
	issn = {0163-5948},
	shorttitle = {Principles of {Survey} {Research} {Part} 4},
	url = {http://doi.acm.org/10.1145/638574.638580},
	doi = {10.1145/638574.638580},
	abstract = {This article discusses how to avoid biased questions in survey instruments, how to motivate people to complete instruments and how to evaluate instruments. In the context of survey evaluation, we discuss how to assess survey reliability i.e. how reproducible a survey's data is and survey validity i.e. how well a survey instrument measures what it sets out to measure.},
	number = {3},
	urldate = {2014-12-17},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence},
	month = may,
	year = {2002},
	keywords = {researcher bias, respondent motivation, survey reliability, survey validity},
	pages = {20--23},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\R9CHKPF9\\Kitchenham and Pfleeger - 2002 - Principles of Survey Research Part 4 Questionnair.pdf:application/pdf}
}

@misc{weka__2013,
	url = {http://www.cs.waikato.ac.nz/ml/weka/},
	journal = {Weka: Data mining software in Java},
	author = {Weka},
	month = oct,
	year = {2013}
}

@article{boehm_software_2000,
	title = {Software development cost estimation approaches — {A} survey},
	volume = {10},
	issn = {1022-7091, 1573-7489},
	url = {http://link.springer.com/article/10.1023/A%3A1018991717352},
	doi = {10.1023/A:1018991717352},
	abstract = {This paper summarizes several classes of software cost estimation models and techniques: parametric models, expertise‐based techniques, learning‐oriented techniques, dynamics‐based models, regression‐based models, and composite‐Bayesian techniques for integrating expertise‐based and regression‐based models. Experience to date indicates that neural‐net and dynamics‐based techniques are less mature than the other classes of techniques, but that all classes of techniques are challenged by the rapid pace of change in software technology. The primary conclusion is that no single technique is best for all situations, and that a careful comparison of the results of several approaches is most likely to produce realistic estimates.},
	language = {en},
	number = {1-4},
	urldate = {2014-08-21},
	journal = {Annals of Software Engineering},
	author = {Boehm, Barry and Abts, Chris and Chulani, Sunita},
	month = nov,
	year = {2000},
	keywords = {Software Engineering/Programming and Operating Systems},
	pages = {177--205},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\53C2ZTQE\\A1018991717352.html:text/html}
}

@inproceedings{wolf_predicting_2009-1,
	address = {Washington, DC, USA},
	series = {{ICSE} '09},
	title = {Predicting {Build} {Failures} {Using} {Social} {Network} {Analysis} on {Developer} {Communication}},
	isbn = {978-1-4244-3453-4},
	url = {http://dx.doi.org/10.1109/ICSE.2009.5070503},
	doi = {10.1109/ICSE.2009.5070503},
	urldate = {2014-07-22},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Wolf, Timo and Schroter, Adrian and Damian, Daniela and Nguyen, Thanh},
	year = {2009},
	pages = {1--11},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MUNTDM2Z\\Wolf et al. - 2009 - Predicting Build Failures Using Social Network Ana.pdf:application/pdf}
}

@misc{_bugzilla_2014,
	title = {Bugzilla},
	url = {http://www.bugzilla.org/},
	urldate = {2014-03-11},
	year = {2014}
}

@inproceedings{zanetti_network_2012-1,
	address = {Muenchen},
	title = {A network perspective on software modularity},
	abstract = {Modularity is a desirable characteristic for software systems. In this article we propose to use a quantitative method from complex network sciences to estimate the coherence between the modularity of the dependency network of large open source Java projects and their decomposition in terms of Java packages. The results presented in this article indicate that our methodology offers a promising and reasonable quantitative approach with potential impact on software engineering processes.},
	booktitle = {{ARCS} {Workshops} ({ARCS})},
	publisher = {IEEE},
	author = {Zanetti, M.S. and Schweitzer, F.},
	month = feb,
	year = {2012},
	keywords = {Coherence, complex network sciences, dependency network modularity, Java, Java packages, large open source Java projects, Measurement, network perspective, Programming, public domain software, quantitative method, software architecture, software engineering processes, software modularity, Software systems, Visualization},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MVWMUXMV\\login.html:text/html}
}

@inproceedings{briand_replicated_2000,
	address = {New York, NY, USA},
	series = {{ICSE} '00},
	title = {A {Replicated} {Assessment} and {Comparison} of {Common} {Software} {Cost} {Modeling} {Techniques}},
	isbn = {1-58113-206-9},
	url = {http://doi.acm.org/10.1145/337180.337223},
	doi = {10.1145/337180.337223},
	abstract = {Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared.The current study replicates a comprehensive comparison of common estimation techniques within different organizational contexts, using data from the European Space Agency. Our study is motivated by the challenge to assess the feasibility of using multi-organization data to build cost models and the benefits gained from company-specific data collection. Using the European Space Agency data set, we investigated a yet unexplored application domain, including military and space projects. The results showed that traditional techniques, namely, ordinary least-squares regression and analysis of variance outperformed Analogy-based estimation and regression trees. Consistent with the results of the replicated study no significant difference was found in accuracy between estimates derived from company-specific data and estimates derived from multi-organizational data.},
	urldate = {2014-08-19},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Briand, Lionel C. and Langley, Tristen and Wieczorek, Isabella},
	year = {2000},
	keywords = {analogy, analysis of variance, classification and regression trees, Cost estimation, ordinary least-squares regression, replication},
	pages = {377--386},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\FB8ACQNS\\Briand et al. - 2000 - A Replicated Assessment and Comparison of Common S.pdf:application/pdf}
}

@book{wohlin_experimentation_2000,
	address = {Norwell, MA, USA},
	title = {Experimentation in {Software} {Engineering}: {An} {Introduction}},
	isbn = {0-7923-8682-5},
	publisher = {Kluwer Academic Publishers},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Bjöorn and Wesslén, Anders},
	year = {2000}
}

@inproceedings{jensen_use_2010,
	address = {New York, NY, USA},
	series = {{GECCO} '10},
	title = {On the {Use} of {Genetic} {Programming} for {Automated} {Refactoring} and the {Introduction} of {Design} {Patterns}},
	isbn = {978-1-4503-0072-8},
	url = {http://doi.acm.org/10.1145/1830483.1830731},
	doi = {10.1145/1830483.1830731},
	abstract = {Maintaining an object-oriented design for a piece of software is a difficult, time-consuming task. Prior approaches to automated design refactoring have focused on making small, iterative changes to a given software design. However, such approaches do not take advantage of composition of design changes, thus limiting the richness of the refactoring strategies that they can generate. In order to address this problem, this paper introduces an approach that supports composition of design changes and makes the introduction of design patterns a primary goal of the refactoring process. The proposed approach uses genetic programming and software engineering metrics to identify the most suitable set of refactorings to apply to a software design. We illustrate the efficacy of this approach by applying it to a large set of published models, as well as a real-world case study},
	urldate = {2015-05-18},
	booktitle = {Proceedings of the 12th {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
	publisher = {ACM},
	author = {Jensen, Adam C. and Cheng, Betty H.C.},
	year = {2010},
	keywords = {Design patterns, evolutionary computation, intelligent search, object-oriented design, refactoring, search-based software engineering, software metrics},
	pages = {1341--1348},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\B577CPZR\\Jensen and Cheng - 2010 - On the Use of Genetic Programming for Automated Re.pdf:application/pdf}
}

@article{shaw_what_2002,
	title = {What makes good research in software engineering?},
	volume = {4},
	issn = {1433-2779},
	url = {http://link.springer.com/article/10.1007/s10009-002-0083-4},
	doi = {10.1007/s10009-002-0083-4},
	abstract = {Physics, biology, and medicine have well-refined public explanations of their research processes. Even in simplified form, these provide guidance about what counts as “good research” both inside and outside the field. Software engineering has not yet explicitly identified and explained either our research processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they adopt, and the criteria by which they evaluate their results. I will present such a characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies should help software engineers design research plans and report the results clearly; it should also help explain the character of software engineering research to computer science at large and to other scientists.},
	language = {en},
	number = {1},
	urldate = {2014-01-26},
	journal = {STTT},
	author = {Shaw, Mary},
	month = oct,
	year = {2002},
	pages = {1--7},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7K9SGWCH\\s10009-002-0083-4.html:text/html}
}

@article{andersson_replicated_2007,
	title = {A {Replicated} {Quantitative} {Analysis} of {Fault} {Distributions} in {Complex} {Software} {Systems}},
	volume = {33},
	issn = {0098-5589},
	doi = {10.1109/TSE.2007.1005},
	abstract = {To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Andersson, C. and Runeson, P.},
	month = may,
	year = {2007},
	keywords = {complex software system, Computer languages, Conducting materials, Empirical research, Helium, Pareto optimisation, Pareto principle, Quality management, replication, Software engineering, software fault distribution, software fault distributions., software fault tolerance, Software systems, System testing, Telecommunication switching},
	pages = {273--286},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5ZRMCK9Q\\abstractAuthors.html:text/html}
}

@article{jabangwe_empirical_2014,
	title = {Empirical evidence on the link between object-oriented measures and external quality attributes: a systematic literature review},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Empirical evidence on the link between object-oriented measures and external quality attributes},
	url = {http://link.springer.com/article/10.1007/s10664-013-9291-7},
	doi = {10.1007/s10664-013-9291-7},
	abstract = {There is a plethora of studies investigating object-oriented measures and their link with external quality attributes, but usefulness of the measures may differ across empirical studies. This study aims to aggregate and identify useful object-oriented measures, specifically those obtainable from the source code of object-oriented systems that have gone through such empirical evaluation. By conducting a systematic literature review, 99 primary studies were identified and traced to four external quality attributes: reliability, maintainability, effectiveness and functionality. A vote-counting approach was used to investigate the link between object-oriented measures and the attributes, and to also assess the consistency of the relation reported across empirical studies. Most of the studies investigate links between object-oriented measures and proxies for reliability attributes, followed by proxies for maintainability. The least investigated attributes were: effectiveness and functionality. Measures from the C\&K measurement suite were the most popular across studies. Vote-counting results suggest that complexity, cohesion, size and coupling measures have a better link with reliability and maintainability than inheritance measures. However, inheritance measures should not be overlooked during quality assessment initiatives; their link with reliability and maintainability could be context dependent. There were too few studies traced to effectiveness and functionality attributes; thus a meaningful vote-counting analysis could not be conducted for these attributes. Thus, there is a need for diversification of quality attributes investigated in empirical studies. This would help with identifying useful measures during quality assessment initiatives, and not just for reliability and maintainability aspects.},
	language = {en},
	urldate = {2015-03-30},
	journal = {Empir Software Eng},
	author = {Jabangwe, Ronald and Börstler, Jürgen and Šmite, Darja and Wohlin, Claes},
	month = mar,
	year = {2014},
	keywords = {object-oriented system, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, software metrics, Software quality, Source code analysis, Source code measures, Static analysis, Systematic literature review},
	pages = {1--54},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HK7XIEAP\\Jabangwe et al. - 2014 - Empirical evidence on the link between object-orie.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\44UZ9WQ3\\10.html:text/html}
}

@misc{hfs_explorer__2013,
	url = {http://www.catacombae.org/hfsx.html},
	journal = {HFS Explorer},
	author = {HFS Explorer},
	month = oct,
	year = {2013}
}

@misc{_r:_????,
	title = {R: {The} {R} {Project} for {Statistical} {Computing}},
	url = {www.r-project.org},
	abstract = {R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.},
	urldate = {2015-03-27},
	journal = {The R Project for Statistical Computing}
}

@inproceedings{tamai_analysis_2002,
	address = {New York, NY, USA},
	series = {{IWPSE} '02},
	title = {Analysis of {Software} {Evolution} {Processes} {Using} {Statistical} {Distribution} {Models}},
	isbn = {1-58113-545-9},
	url = {http://doi.acm.org/10.1145/512035.512063},
	doi = {10.1145/512035.512063},
	abstract = {Size data of software systems are constantly collected but so far there have been no studies of applying statistical distribution models to analyze and interpret those data. In this paper, we show that the negative binomial distribution fits well to the distribution of size data such as the number of methods per class and number of lines of code per method and can be effectively used to trace software evolution processes.},
	urldate = {2014-07-18},
	booktitle = {Proceedings of the {International} {Workshop} on {Principles} of {Software} {Evolution}},
	publisher = {ACM},
	author = {Tamai, Tetsuo and Nakatani, Takako},
	year = {2002},
	pages = {120--123}
}

@incollection{olson_rough_2008,
	title = {Rough {Sets}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_6},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {87--109},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PASKDCF4\\978-3-540-76917-0_6.html:text/html}
}

@article{gray_reflections_2012,
	title = {Reflections on the {NASA} {MDP} data sets},
	volume = {6},
	issn = {17518806},
	url = {http://digital-library.theiet.org/content/journals/10.1049/iet-sen.2011.0132},
	doi = {10.1049/iet-sen.2011.0132},
	language = {en},
	number = {6},
	urldate = {2015-01-05},
	journal = {IET Software},
	author = {Gray, D. and Bowes, D. and Davey, N. and Sun, Y. and Christianson, B.},
	year = {2012},
	pages = {549},
	file = {IET Digital Library\: Reflections on the NASA MDP data sets:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NUX7NU9R\\iet-sen.2011.html:text/html}
}

@incollection{genero_using_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Metrics} to {Predict} {OO} {Information} {Systems} {Maintainability}},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-42215-0, 978-3-540-45341-3},
	url = {http://link.springer.com/chapter/10.1007/3-540-45341-5_26},
	abstract = {The quality of object oriented information systems (OOIS) depends greatly on the decisions taken at early phases of their development. As an early available artifact the quality of the class diagram is crucial to the success of system development. Class diagrams lay the foundation for all later design work. So, their quality heavily affects the product that will be ultimately implemented. Even though the appearance of the Unified Modeling Language (UML) as a standard of modelling OOIS has contributed greatly towards building quality OOIS, it is not enough. Early availability of metrics is a key factor in the successful management of OOIS development. The aim of this paper is to present a set of metrics for measuring the structural complexity of UML class diagrams and to use them for predicting their maintainability that will heavily be correlated with OOIS maintainability.},
	language = {en},
	number = {2068},
	urldate = {2014-08-22},
	booktitle = {Advanced {Information} {Systems} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Genero, Marcela and Olivas, José and Piattini, Mario and Romero, Francisco},
	editor = {Dittrich, Klaus R. and Geppert, Andreas and Norrie, Moira C.},
	month = jan,
	year = {2001},
	keywords = {Business Information Systems, class diagrams complexity, Database Management, fuzzy deformable prototypes, Information Storage and Retrieval, Information Systems Applications (incl.Internet), Management of Computing and Information Systems, object oriented information systems maintainability, object oriented metrics, prediction models, Software engineering, UML},
	pages = {388--401},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QP3IKVB8\\Genero et al. - 2001 - Using Metrics to Predict OO Information Systems Ma.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GDGQKW9K\\3-540-45341-5_26.html:text/html}
}

@inproceedings{tichy_empirical_2007,
	title = {Empirical {Methods} in {Software} {Engineering} {Research}.},
	booktitle = {{ICSE} {Companion}},
	author = {Tichy, Walter F. and Padberg, Frank},
	year = {2007},
	pages = {163--164}
}

@inproceedings{bloch_how_2006-1,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {How to {Design} a {Good} {API} and {Why} {It} {Matters}},
	isbn = {1-59593-491-X},
	url = {http://doi.acm.org/10.1145/1176617.1176622},
	doi = {10.1145/1176617.1176622},
	abstract = {In lieu of a traditional , I've tried to distill the essence of the talk into a collection of maxims:All programmers are API designers. Good programs are modular, and intermodular boundaries define APIs. Good modules get reused.APIs can be among your greatest assets or liabilities. Good APIs create long-term customers; bad ones create long-term support nightmares.Public APIs, like diamonds, are forever. You have one chance to get it right so give it your best.APIs should be easy to use and hard to misuse. It should be easy to do simple things; possible to do complex things; and impossible, or at least difficult, to do wrong things.APIs should be self-documenting: It should rarely require documentation to read code written to a good API. In fact, it should rarely require documentation to write it.When designing an API, first gather requirements - with a healthy degree of skepticism. People often provide solutions; it's your job to ferret out the underlying problems and find the best solutions.Structure requirements as use-cases: they are the yardstick against which you'll measure your API.Early drafts of APIs should be short, typically one page with class and method signatures and one-line descriptions. This makes it easy to restructure the API when you don't get it right the first time.Code the use-cases against your API before you implement it, even before you specify it properly. This will save you from implementing, or even specifying, a fundamentally broken API.Maintain the code for uses-cases as the API evolves. Not only will this protect you from rude surprises, but the resulting code will become the examples for the API, the basis for tutorials and tests.Example code should be exemplary. If an API is used widely, its examples will be the archetypes for thousands of programs. Any mistakes will come back to haunt you a thousand fold.You can't please everyone so aim to displease everyone equally. Most APIs are overconstrained.Expect API-design mistakes due to failures of imagination. You can't reasonably hope to imagine everything that everyone will do with an API, or how it will interact with every other part of a system.API design is not a solitary activity. Show your design to as many people as you can, and take their feedback seriously. Possibilities that elude your imagination may be clear to others.Avoid fixed limits on input sizes. They limit usefulness and hasten obsolescence.If it's hard to find good names, go back to the drawing board. Don't be afraid to split or merge an API, or embed it in a more general setting. If names start falling into place, you're on the right track.Names matter. Strive for intelligibility, consistency, and symmetry. Every API is a little language, and people must learn to read and write it. If you get an API right, code will read like prose.When in doubt, leave it out. If there is a fundamental theorem of API design, this is it. It applies equally to functionality, classes, methods, and parameters. Every facet of an API should be as small as possible, but no smaller. You can always add things later, but you can't take them away.Minimizing conceptual weight is more important than class- or method-count.Keep APIs free of implementations details. They confuse users and inhibit the flexibility to evolve. It isn't always obvious what's an implementation detail: Be wary of overspecification.Minimize mutability. Immutable objects are simple, thread-safe, and freely sharable.Documentation matters. No matter how good an API, it won't get used without good documentation. Document every exported API element: every class, method, field, and parameter.Consider the performance consequences of API design decisions, but don't warp an API to achieve performance gains. Luckily, good APIs typically lend themselves to fast implementations.APIs must coexist peacefully with the platform, so do what is customary. It is almost always wrong to "transliterate" an API from one platform to another.Minimize accessibility; when in doubt, make it private. This simplifies APIs and reduces coupling.Subclass only if you can say with a straight face that every instance of the subclass is an instance of the superclass. Exposed classes should never subclass just to reuse implementation code.Design and document for inheritance or else prohibit it. This documentation takes the form of self-use patterns: how methods in a class use one another. Without it, safe subclassing is impossible.Don't make the client do anything the library could do. Violating this rule leads to boilerplate code in the client, which is annoying and error-prone.Obey the principle of least astonishment. Every method should do the least surprising thing it could, given its name. If a method doesn't do what users think it will, bugs will result.Fail fast. The sooner you report a bug, the less damage it will do. Compile-time is best. If you must fail at run-time, do it as soon as possible.Provide programmatic access to all data available in string form. Otherwise, programmers will be forced to parse strings, which is painful. Worse, the string forms will turn into de facto APIs.Overload with care. If the behaviors of two methods differ, it's better to give them different names.Use the right data type for the job. For example, don't use string if there is a more appropriate type.Use consistent parameter ordering across methods. Otherwise, programmers will get it backwards.Avoid long parameter lists, especially those with multiple consecutive parameters of the same type.Avoid return values that demand exceptional processing. Clients will forget to write the special-case code, leading to bugs. For example, return zero-length arrays or collections rather than nulls.Throw exceptions only to indicate exceptional conditions. Otherwise, clients will be forced to use exceptions for normal flow control, leading to programs that are hard to read, buggy, or slow.Throw unchecked exceptions unless clients can realistically recover from the failure.API design is an art, not a science. Strive for beauty, and trust your gut. Do not adhere slavishly to the above heuristics, but violate them only infrequently and with good reason..},
	urldate = {2015-03-30},
	booktitle = {Companion to the 21st {ACM} {SIGPLAN} {Symposium} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Bloch, Joshua},
	year = {2006},
	pages = {506--507},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CGP9TJQA\\Bloch - 2006 - How to Design a Good API and Why It Matters.pdf:application/pdf}
}

@article{vinay_kumar_software_2008,
	title = {Software development cost estimation using wavelet neural networks},
	volume = {81},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121208000071},
	doi = {10.1016/j.jss.2007.12.793},
	abstract = {Software development has become an essential investment for many organizations. Software engineering practitioners have become more and more concerned about accurately predicting the cost and quality of software product under development. Accurate estimates are desired but no model has proved to be successful at effectively and consistently predicting software development cost. In this paper, we propose the use of wavelet neural network (WNN) to forecast the software development effort. We used two types of WNN with Morlet function and Gaussian function as transfer function and also proposed threshold acceptance training algorithm for wavelet neural network (TAWNN). The effectiveness of the WNN variants is compared with other techniques such as multilayer perceptron (MLP), radial basis function network (RBFN), multiple linear regression (MLR), dynamic evolving neuro-fuzzy inference system (DENFIS) and support vector machine (SVM) in terms of the error measure which is mean magnitude relative error (MMRE) obtained on Canadian financial (CF) dataset and IBM data processing services (IBMDPS) dataset. Based on the experiments conducted, it is observed that the WNN-Morlet for CF dataset and WNN-Gaussian for IBMDPS outperformed all the other techniques. Also, TAWNN outperformed all other techniques except WNN.},
	number = {11},
	urldate = {2014-08-27},
	journal = {Journal of Systems and Software},
	author = {Vinay Kumar, K. and Ravi, V. and Carr, Mahil and Raj Kiran, N.},
	month = nov,
	year = {2008},
	keywords = {software cost estimation, software development effort, Threshold accepting based wavelet neural network, Wavelet neural networks},
	pages = {1853--1867},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NTNTT68A\\S0164121208000071.html:text/html}
}

@inproceedings{hora_bugmaps:_2012,
	title = {{BugMaps}: {A} {Tool} for the {Visual} {Exploration} and {Analysis} of {Bugs}},
	shorttitle = {{BugMaps}},
	url = {http://hal.inria.fr/hal-00668397},
	abstract = {To harness the complexity of big legacy software, software engineering tools need more and more information on these systems. This information may come from analysis of the source code, study of execution traces, computing of metrics, etc. One source of information received less attention than source code: the bugs on the system. Little is known about the evolutionary behavior, lifetime, distribution, and stability of bugs. In this paper, we propose to consider bugs as first class entities and a useful source of information that can answer such topics. Such analysis is inherently complex, because bugs are intangible, invisible, and difficult to be traced. Therefore, our tool extracts information about bugs from bug tracking systems, link this information to other software artifacts, and explore interactive visualizations of bugs that we call bug maps.},
	urldate = {2014-03-11},
	author = {Hora, Andre and Anquetil, Nicolas and Ducasse, Stéphane and Bhatti, Muhammad Usman and Couto, Cesar and Valente, Marco Tulio and Martins, Julio},
	year = {2012},
	keywords = {moose lse-pub raweb2012},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BVKXBAQE\\Hora et al. - 2012 - BugMaps A Tool for the Visual Exploration and Ana.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HUDUA92R\\hal-00668397.html:text/html}
}

@article{li_towards_2005,
	title = {Towards a theory of scale-free graphs: {Definition}, properties, and implications},
	volume = {2},
	shorttitle = {Towards a theory of scale-free graphs},
	abstract = {Abstract. There is a large, popular, and growing literature on “scale-free ” networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks. 1.},
	journal = {Internet Mathematics},
	author = {Li, Lun and Alderson, David and Doyle, John C. and Willinger, Walter},
	year = {2005},
	pages = {4},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IRCTZUUN\\Li et al. - 2005 - Towards a theory of scale-free graphs Definition,.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PFD9KQKX\\summary.html:text/html}
}

@article{ma_network_2008,
	title = {Network {Motifs} in {Object}-{Oriented} {Software} {Systems}},
	url = {http://arxiv.org/abs/0808.3292},
	abstract = {Nowadays, software has become a complex piece of work that may be beyond our control. Understanding how software evolves over time plays an important role in controlling software development processes. Recently, a few researchers found the quantitative evidence of structural duplication in software systems or web applications, which is similar to the evolutionary trend found in biological systems. To investigate the principles or rules of software evolution, we introduce the relevant theories and methods of complex networks into structural evolution and change of software systems. According to the results of our experiment on network motifs, we find that the stability of a motif shows positive correlation with its abundance and a motif with high Z score tends to have stable structure. These findings imply that the evolution of software systems is based on functional cloning as well as structural duplication and tends to be structurally stable. So, the work presented in this paper will be useful for the analysis of structural changes of software systems in reverse engineering.},
	urldate = {2014-01-30},
	journal = {arXiv:0808.3292 [cs]},
	author = {Ma, Yutao and He, Keqing and Liu, Jing},
	month = aug,
	year = {2008},
	note = {Dynamics of Continuous, Discrete and Impulsive Systems (Series B: Applications \& Algorithms), 2007, 14(S6): 166-172},
	keywords = {Computer Science - Software Engineering, D.2.8, K.6.3},
	annote = {Comment: 7 pages, 4 figures, 1 table, the revised version has been published by DCDIS-B special issue on software engineering and complex networks},
	file = {0808.3292 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\88RMVKPE\\Ma et al. - 2008 - Network Motifs in Object-Oriented Software Systems.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\REBAH2KE\\0808.html:text/html}
}

@inproceedings{de_magalhaes_towards_2013,
	title = {Towards a {Taxonomy} of {Replications} in {Empirical} {Software} {Engineering} {Research}: {A} {Research} {Proposal}},
	shorttitle = {Towards a {Taxonomy} of {Replications} in {Empirical} {Software} {Engineering} {Research}},
	doi = {10.1109/RESER.2013.10},
	abstract = {Goal: We present a research proposal that aims to collect, analyze, and synthesize data towards the construction of a taxonomy of replications in empirical software engineering research. Method: We propose a cross-sectional survey with researchers that performed replications of empirical studies in software engineering. The population of participants is comprised of all researchers that published replications in software engineering and that were identified in a recently published mapping study. Expected Results: We expect to collect data from researchers that have performed different types of replications in order to support the definition of types or categories of replication using a grounded approach. Conclusion: We expect that the study proposed in this article will motivate a discussion in the empirical software engineering community about the need for a clear cut classification of types of replications among other definitions that will be investigated.},
	booktitle = {2013 3rd {International} {Workshop} on {Replication} in {Empirical} {Software} {Engineering} {Research} ({RESER})},
	author = {De Magalhaes, C.V.C. and Da Silva, F.Q.B.},
	month = oct,
	year = {2013},
	keywords = {Communities, Conferences, Context, data analysis, data collection, data synthesis, Empirical software engineering, empirical software engineering research, Instruments, Proposals, replications taxonomy, replication types, research and development, Software engineering, survey research, taxonomy, taxonomy of replications},
	pages = {50--55},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8GDNE5BE\\login.html:text/html}
}

@article{li_flexible_2007,
	title = {A flexible method for software effort estimation by analogy},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-006-7552-4},
	doi = {10.1007/s10664-006-7552-4},
	abstract = {Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods? First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number N of analogies (similar objects) taken for adaptation and the threshold T for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of N and T likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods.},
	language = {en},
	number = {1},
	urldate = {2014-08-21},
	journal = {Empir Software Eng},
	author = {Li, Jingzhou and Ruhe, Guenther and Al-Emran, Ahmed and Richter, Michael M.},
	month = feb,
	year = {2007},
	keywords = {Analogy-based effort estimation, Comparative analysis, Learning, Missing values, Non-quantitative attributes, Programming Languages, Compilers, Interpreters, software development effort, Software Engineering/Programming and Operating Systems},
	pages = {65--106},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DUDG84Q7\\Li et al. - 2007 - A flexible method for software effort estimation b.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\STAWE57C\\10.html:text/html}
}

@inproceedings{wahyudin_defect_2008,
	title = {Defect {Prediction} using {Combined} {Product} and {Project} {Metrics} - {A} {Case} {Study} from the {Open} {Source} "{Apache}" {MyFaces} {Project} {Family}},
	doi = {10.1109/SEAA.2008.36},
	abstract = {The quality evaluation of open source software (OSS) products, e.g., defect estimation and prediction approaches of individual releases, gains importance with increasing OSS adoption in industry applications. Most empirical studies on the accuracy of defect prediction and software maintenance focus on product metrics as predictors that are available only when the product is finished. Only few prediction models consider information on the development process (project metrics) that seems relevant to quality improvement of the software product. In this paper, we investigate defect prediction with data from a family of widely used OSS projects based both on product and project metrics as well as on combinations of these metrics. Main results of data analysis are (a) a set of project metrics prior to product release that had strong correlation to potential defect growth between releases and (b) a combination of product and project metrics enables a more accurate defect prediction than the application of one single type of measurement. Thus, the combined application of project and product metrics can (a) improve the accuracy of defect prediction, (b) enable a better guidance of the release process from project management point of view, and (c) help identifying areas for product and process improvement.},
	booktitle = {Software {Engineering} and {Advanced} {Applications}, 2008. {SEAA} '08. 34th {Euromicro} {Conference}},
	author = {Wahyudin, D. and Schatten, A. and Winkler, D. and Tjoa, A.M. and Biffl, S.},
	month = sep,
	year = {2008},
	keywords = {Accuracy, Application software, Context modeling, data analysis, defect prediction, Lead, open source myfaces project family, Open source software, Predictive models, product metrics, Project management, project metrics, public domain software, quality evaluation, Quality management, software maintenance, software metrics, software product quality improvement, Software quality},
	pages = {207--215},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QTQBV6MV\\articleDetails.html:text/html}
}

@inproceedings{liu_understanding_2005,
	address = {Genova, Italy},
	title = {Understanding the {Open}-{Source} {Software} {Development} {Process}: a {Case} {Study} with {CVSChecker}},
	shorttitle = {Understanding the {Open}-{Source} {Software} {Development} {Process}},
	publisher = {NRC 47453},
	author = {Liu, Ying and Stroulia, Eleni and Erdogmus, Hakan},
	year = {2005},
	keywords = {imported},
	pages = {154--161}
}

@article{linstead_sourcerer:_2009,
	title = {Sourcerer: mining and searching internet-scale software repositories},
	volume = {18},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Sourcerer},
	url = {http://link.springer.com/article/10.1007/s10618-008-0118-x},
	doi = {10.1007/s10618-008-0118-x},
	abstract = {Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632 Java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions. We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering an software development staffing. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92– roughly 10–30\% better than previous approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu.},
	language = {en},
	number = {2},
	urldate = {2014-06-23},
	journal = {Data Min Knowl Disc},
	author = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
	month = apr,
	year = {2009},
	keywords = {Artificial Intelligence (incl. Robotics), Author-topic probabilistic modeling, Code retrieval, Code search, Computing Methodologies, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Mining software, Program understanding, Software analysis, Statistics for Engineering, Physics, Computer Science, Chemistry \& Geosciences, Statistics, general},
	pages = {300--336},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5PZKWEPB\\Linstead et al. - 2009 - Sourcerer mining and searching internet-scale sof.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BPK8B7UM\\10.html:text/html}
}

@book{bailey_typologies_1994,
	address = {Thousand Oaks, Calif.},
	title = {Typologies and taxonomies: an introduction to classification techniques},
	isbn = {0803952597 9780803952591},
	shorttitle = {Typologies and taxonomies},
	language = {English},
	publisher = {Sage},
	author = {Bailey, Kenneth D},
	year = {1994}
}

@article{chidamber_metrics_1994,
	title = {A metrics suite for object oriented design},
	volume = {20},
	issn = {0098-5589},
	doi = {10.1109/32.295895},
	abstract = {Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Chidamber, S.R. and Kemerer, C.F.},
	month = jun,
	year = {1994},
	keywords = {Application software, automated data collection tool, Engineering management, Information management, Information technology, measurement principles, metrics suite, object oriented design, object-oriented methods, Object oriented programming, object-oriented programming, Ontologies, organization, process improvement, Programming, software development, software development management, Software engineering, Software measurement, software measures, software metrics, Technology management},
	pages = {476--493},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\SFS5PZCP\\defdeny.html:text/html}
}

@article{usc-cse_cocomo_1997,
	title = {{COCOMO} {II} {Model} {Definition} {Manual}},
	url = {http://sunset.usc. edu/COCOMOII/cocomo.html},
	journal = {Center for Software Engineering, Computer Science Department,University of Southern California, Los Angeles, CA},
	author = {USC-CSE},
	year = {1997}
}

@article{shepperd_estimating_1997,
	title = {Estimating software project effort using analogies},
	volume = {23},
	issn = {0098-5589},
	doi = {10.1109/32.637387},
	abstract = {Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used as the basis for the prediction. The process is automated using a PC-based tool known as ANGEL. The method is validated on nine different industrial datasets (a total of 275 projects) and in all cases analogy outperforms algorithmic models based upon stepwise regression. From this work we argue that estimation by analogy is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Shepperd, M. and Schofield, C.},
	month = nov,
	year = {1997},
	keywords = {Accuracy, algorithmic models, ANGEL, Centralized control, COCOMO, Costs, estimation by analogy, Euclidean distance, functional requirements document, industrial datasets, nearest neighbors, Nearest neighbor searches, Particle measurements, personal computer-based tool, Programming, project effort prediction, Project management, software cost estimation, software development management, software development method, Software engineering, software metrics, software project effort estimation, Software tools, stepwise regression},
	pages = {736--743},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CR4P28TS\\articleDetails.html:text/html}
}

@article{lieberherr_assuring_1989,
	title = {Assuring {Good} {Style} for {Object}-{Oriented} {Programs}},
	volume = {6},
	abstract = {We introduce a simple, programming language independent rule (known in-house as the Law of Demeter) which encodes the ideas of encapsulation and modularity in an easy to follow form for the object-oriented programmer. The rule achieves the following related benefits if code duplication, the number of method arguments and the number of methods per class are minimized: Easier software maintenance, less coupling between your methods, better information hiding, methods which are easier to reuse, and easier correctness proofs using structural induction. We show relationships between the Law and software engineering techniques, such as coupling control, information hiding, information restriction, localization of information, narrow interfaces and structural induction. We discuss two important interpretations of the Law (strong and weak) and we prove that any object-oriented program can be transformed to satisfy the Law. We express the Law in several languages which support object-oriented p...},
	journal = {IEEE Software},
	author = {Lieberherr, Karl J. and Holland, Ian},
	year = {1989},
	pages = {38--48},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\SDJ7FHX3\\Lieberherr and Holland - 1989 - Assuring Good Style for Object-Oriented Programs.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JKRGBGKQ\\summary.html:text/html}
}

@misc{_issue_2014,
	title = {Issue \& {Project} {Tracking} {Software} {\textbar} {Atlassian}},
	url = {https://www.atlassian.com/software/jira},
	urldate = {2014-03-11},
	year = {2014}
}

@article{sahin_code-smell_2014,
	title = {Code-{Smell} {Detection} {As} a {Bilevel} {Problem}},
	volume = {24},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/2675067},
	doi = {10.1145/2675067},
	abstract = {Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code-smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open-source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86\% in terms of precision and recall. The results confirm the outperformance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.},
	number = {1},
	urldate = {2015-05-18},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Sahin, Dilan and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy},
	month = oct,
	year = {2014},
	keywords = {code smells, search-based software engineering, Software quality},
	pages = {6:1--6:44},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IAG5XI9X\\Sahin et al. - 2014 - Code-Smell Detection As a Bilevel Problem.pdf:application/pdf}
}

@inproceedings{mendonca_framework_2008,
	title = {A {Framework} for {Software} {Engineering} {Experimental} {Replications}},
	doi = {10.1109/ICECCS.2008.38},
	abstract = {Experimental replications are very important to the advancement of empirical software engineering. Replications are one of the key mechanisms to confirm previous experimental findings. They are also used to transfer experimental knowledge, to train people, and to expand a base of experimental evidence. Unfortunately, experimental replications are difficult endeavors. It is not easy to transfer experimental know-how and experimental findings. Based on our experience, this paper discusses this problem and proposes a Framework for Improving the Replication of Experiments (FIRE). The FIRE addresses knowledge sharing issues both at the intra-group (internal replications) and inter-group (external replications) levels. It encourages coordination of replications in order to facilitate knowledge transfer for lower cost, higher quality replications and more generalizable results.},
	booktitle = {13th {IEEE} {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems}, 2008. {ICECCS} 2008},
	author = {Mendonca, M.G. and Maldonado, J.C. and de Oliveira, M.C.F. and Carver, J. and Fabbri, S.C.P.F. and Shull, F. and Travassos, Guilherme H. and Hohn, E.N. and Basili, V.R.},
	month = mar,
	year = {2008},
	keywords = {Costs, Cultural differences, Educational institutions, Empirical software engineering, experimental evidence, experimental knowledge transfer, Fires, Framework for Improving the Replication of Experiments, Humans, knowledge sharing, Knowledge transfer, Production, Programming, reading techniques, replications, robustness, Software engineering, software engineering experimental replication},
	pages = {203--212},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RHN7GRTW\\login.html:text/html}
}

@book{gamma_entwurfsmuster:_1996,
	address = {Bonn; Reading, Mass. [u.a.]},
	title = {Entwurfsmuster: {Elemente} wiederverwendbarer objektorientierter {Software}},
	isbn = {3893199500 9783893199501 0201633612 9780201633610},
	shorttitle = {Entwurfsmuster},
	language = {German},
	publisher = {Addison-Wesley},
	author = {Gamma, Erich},
	year = {1996}
}

@techreport{kitchenham_guidelines_2007,
	type = {Technical},
	title = {Guidelines for performing {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	institution = {Keele University and Durham University},
	author = {Kitchenham, Barbara and Charters, Stuart},
	year = {2007}
}

@inproceedings{jiang_personalized_2013,
	title = {Personalized defect prediction},
	doi = {10.1109/ASE.2013.6693087},
	abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20\% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.},
	booktitle = {2013 {IEEE}/{ACM} 28th {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
	month = nov,
	year = {2013},
	keywords = {Change classification, coding styles, commit frequencies, Computer bugs, C software projects, different defect patterns, Eclipse, experience levels, Feature extraction, Jackrabbit, Java, java software projects, Linux, Linux kernel, Lucene, Machine learning, Mars, personalized defect prediction, PostgreSQL, Predictive models, program compilers, separate prediction model, software defect prediction, software reliability, Syntactics, Training, Vectors, Xorg},
	pages = {279--289},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\6WSCBG83\\login.html:text/html}
}

@article{okeeffe_search-based_2008,
	title = {Search-based refactoring: an empirical study},
	volume = {20},
	copyright = {Copyright © 2008 John Wiley \& Sons, Ltd.},
	issn = {1532-0618},
	shorttitle = {Search-based refactoring},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.378/abstract},
	doi = {10.1002/smr.378},
	abstract = {Object-oriented systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying design. This problem must often be remedied in a costly refactoring phase before further maintenance programming can take place. Recently search-based approaches to automating the task of software refactoring, based on the concept of treating object-oriented design as a combinatorial optimization problem, have been proposed. However, because search-based refactoring is a novel approach it is yet to be established as to which search techniques are most suitable for the task. In this paper we report the results of an empirical comparison of simulated annealing (SA), genetic algorithms (GAs) and multiple ascent hill-climbing (HCM) in search-based refactoring. A prototype automated refactoring tool is employed, capable of making radical changes to the design of an existing program in order that it conforms more closely to a contemporary quality model. Results show HCM to outperform both SA and GA over a set of five input programs. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2015-05-18},
	journal = {J. Softw. Maint. Evol.: Res. Pract.},
	author = {O'Keeffe, Mark and Cinnéide, Mel Ó},
	month = sep,
	year = {2008},
	keywords = {automated design improvement, refactoring tools, search-based software engineering},
	pages = {345--364},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\T5UXKU7A\\O'Keeffe and Cinnéide - 2008 - Search-based refactoring an empirical study.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UFMIKTXG\\abstract.html:text/html}
}

@book{ghezzi_fundamentals_2003,
	address = {Upper Saddle River, N.J.},
	title = {Fundamentals of software engineering},
	isbn = {0133056996 9780133056990 013099183X 9780130991836 2900133056999},
	language = {English},
	publisher = {Prentice Hall},
	author = {Ghezzi, Carlo and Jazayeri, Mehdi and Mandrioli, Dino},
	year = {2003}
}

@inproceedings{nagappan_use_2005,
	address = {New York, NY, USA},
	series = {{ICSE} '05},
	title = {Use of {Relative} {Code} {Churn} {Measures} to {Predict} {System} {Defect} {Density}},
	isbn = {1-58113-963-2},
	url = {http://doi.acm.org/10.1145/1062455.1062514},
	doi = {10.1145/1062455.1062514},
	abstract = {Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn.Using statistical regression models, we show that while absolute measures of code churn are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.},
	urldate = {2014-08-27},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Nagappan, Nachiappan and Ball, Thomas},
	year = {2005},
	keywords = {defect density, fault-proneness, multiple regression, principal component analysis, relative code churn},
	pages = {284--292}
}

@misc{_tracelab_????,
	title = {{TraceLab}},
	url = {http://coest.org/coest-projects/projects/tracelab}
}

@inproceedings{zimmermann_predicting_2008,
	address = {New York, NY, USA},
	series = {{ICSE} '08},
	title = {Predicting {Defects} {Using} {Network} {Analysis} on {Dependency} {Graphs}},
	isbn = {978-1-60558-079-1},
	url = {http://doi.acm.org/10.1145/1368088.1368161},
	doi = {10.1145/1368088.1368161},
	abstract = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10\% points higher than for models built from complexity metrics. In addition, network measures could identify 60\% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.},
	urldate = {2014-08-25},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2008},
	keywords = {defect prediction, dependency graph, network analysis, windows server 2003},
	pages = {531--540}
}

@misc{finley_github_2011,
	title = {Github {Has} {Surpassed} {Sourceforge} and {Google} {Code} in {Popularity}},
	url = {http://readwrite.com/2011/06/02/github-has-passed-sourceforge},
	abstract = {Github is now the most popular open source forge, having surpassed Sourceforge, Google Code and Microsoft's CodePlex in total number of commits for the period of January to May 2011, according to data released today by Black Duck Software. This should probably come as no surprise, but it's good to have data to back assumptions.

During the\&hellip;},
	urldate = {2014-06-30},
	journal = {ReadWrite},
	author = {Finley, Klint},
	month = jun,
	year = {2011},
	keywords = {hack, News},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\M7ZGC3NC\\github-has-passed-sourceforge.html:text/html}
}

@book{runeson_case_2012,
	edition = {1 edition},
	title = {Case {Study} {Research} in {Software} {Engineering}: {Guidelines} and {Examples}},
	shorttitle = {Case {Study} {Research} in {Software} {Engineering}},
	abstract = {Based on their own experiences of in-depth case studies of software projects in international corporations, in this book the authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering.  This is the first software engineering specific book on the case study research method.},
	language = {English},
	publisher = {Wiley},
	author = {Runeson, Per and Host, Martin and Rainer, Austen and Regnell, Bjorn},
	month = mar,
	year = {2012}
}

@misc{_subversion_2014,
	title = {Subversion - {Tigris}.org},
	url = {http://subversion.tigris.org},
	urldate = {2014-03-11},
	year = {2014}
}

@book{riel_object-oriented_1996-1,
	address = {Reading, Mass.},
	title = {Object-oriented design heuristics},
	isbn = {020163385X  9780201633856  9780321774965  0321774965},
	language = {English},
	publisher = {Addison-Wesley Pub. Co.},
	author = {Riel, Arthur J},
	year = {1996}
}

@article{valverde_hierarchical_2003-1,
	title = {Hierarchical {Small} {Worlds} in {Software} {Architecture}},
	url = {http://arxiv.org/abs/cond-mat/0307278},
	abstract = {In this paper, we present a complex network approach to the study of software engineering. We have found universal network patterns in a large collection of object-oriented (OO) software systems written in C++ and Java. All the systems analyzed here display the small-world behavior, that is, the average distance between any pair of classes is very small even when coupling is low and cohesion is high. In addition, the structure of OO software is a very heterogeneous network characterized by a degree distribution following a power-law with similar exponents. We have investigated the origin of these universal patterns. Our study suggests that some features of OO programing languages, like encapsulation, seem to be largely responsible for the small-world behavior. On the other hand, software heterogeneity is largely independent of the purpose and objectives of the particular system under study and appears to be related to a pattern of constrained growth. A number of software engineering topics may benefit from the present approach, including empirical software measurement and program comprehension.},
	urldate = {2014-01-30},
	journal = {arXiv:cond-mat/0307278},
	author = {Valverde, Sergi and Sole, Ricard V.},
	month = jul,
	year = {2003},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: Accepted for publication in Special Issue on Software Engineering and Complex Networks Dynamics of Continuous, Discrete and Impulsive Systems Series B (2007)},
	file = {arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\C2Z6H2WS\\0307278.html:text/html;cond-mat/0307278 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NM64QAN7\\Valverde and Sole - 2003 - Hierarchical Small Worlds in Software Architecture.pdf:application/pdf}
}

@book{wohlin_experimentation_2012,
	address = {New York},
	title = {Experimentation in {Software} {Engineering}},
	isbn = {9783642290435 3642290434},
	abstract = {Annotation},
	language = {English},
	publisher = {Springer},
	author = {Wohlin and Regnell, Wesslén, Anders, Bjö},
	year = {2012}
}

@inproceedings{wettel_software_2011,
	address = {New York, NY, USA},
	series = {{ICSE} '11},
	title = {Software {Systems} {As} {Cities}: {A} {Controlled} {Experiment}},
	isbn = {978-1-4503-0445-0},
	shorttitle = {Software {Systems} {As} {Cities}},
	url = {http://doi.acm.org/10.1145/1985793.1985868},
	doi = {10.1145/1985793.1985868},
	abstract = {Software visualization is a popular program comprehension technique used in the context of software maintenance, reverse engineering, and software evolution analysis. While there is a broad range of software visualization approaches, only few have been empirically evaluated. This is detrimental to the acceptance of software visualization in both the academic and the industrial world. We present a controlled experiment for the empirical evaluation of a 3D software visualization approach based on a city metaphor and implemented in a tool called CodeCity. The goal is to provide experimental evidence of the viability of our approach in the context of program comprehension by having subjects perform tasks related to program comprehension. We designed our experiment based on lessons extracted from the current body of research. We conducted the experiment in four locations across three countries, involving 41 participants from both academia and industry. The experiment shows that CodeCity leads to a statistically significant increase in terms of task correctness and decrease in task completion time. We detail the experiment we performed, discuss its results and reflect on the many lessons learned.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Wettel, Richard and Lanza, Michele and Robbes, Romain},
	year = {2011},
	keywords = {empirical validation, software visualization},
	pages = {551--560}
}

@inproceedings{wohlin_empirical_2013,
	address = {Piscataway, NJ, USA},
	series = {{CESI} '14},
	title = {Empirical {Software} {Engineering} {Research} with {Industry}: {Top} 10 {Challenges}},
	isbn = {978-1-4673-6286-3},
	shorttitle = {Empirical {Software} {Engineering} {Research} with {Industry}},
	url = {http://dl.acm.org/citation.cfm?id=2662528.2662542},
	abstract = {Software engineering research can be done in many ways, in particular it can be done in different ways when it comes to working with industry. This paper presents a list of top 10 challenges to work with industry based on our experience from working with industry in a very close collaboration with continuous exchange of knowledge and information. The top 10 list is based on a large number of research projects and empirical studies conducted with industrial research partners since 1983. It is concluded that close collaboration is a long-term undertaking and a large investment. The importance of addressing the top 10 challenges is stressed, since they form the basis for a long-term sustainable and successful collaboration between industry and academia.},
	urldate = {2015-01-05},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Conducting} {Empirical} {Studies} in {Industry}},
	publisher = {IEEE Press},
	author = {Wohlin, Claes},
	year = {2013},
	keywords = {empirical studies, industry challenges, industry collaboration},
	pages = {43--46}
}

@misc{jdeodorant__2014,
	url = {http://www.jdeodorant.com/},
	journal = {JDeodorant - When quality matters the most!},
	author = {JDeodorant},
	month = dec,
	year = {2014}
}

@article{yamasaki_preferential_2006,
	title = {Preferential attachment and growth dynamics in complex systems},
	volume = {74},
	issn = {1539-3755},
	abstract = {Complex systems can be characterized by classes of equivalency of their elements defined according to system specific rules. We propose a generalized preferential attachment model to describe the class size distribution. The model postulates preferential growth of the existing classes and the steady influx of new classes. According to the model, the distribution changes from a pure exponential form for zero influx of new classes to a power law with an exponential cut-off form when the influx of new classes is substantial. Predictions of the model are tested through the analysis of a unique industrial database, which covers both elementary units (products) and classes (markets, firms) in a given industry (pharmaceuticals), covering the entire size distribution. The model's predictions are in good agreement with the data. The paper sheds light on the emergence of the exponent tau approximately 2 observed as a universal feature of many biological, social and economic problems.},
	language = {eng},
	number = {3 Pt 2},
	journal = {Phys Rev E Stat Nonlin Soft Matter Phys},
	author = {Yamasaki, Kazuko and Matia, Kaushik and Buldyrev, Sergey V. and Fu, Dongfeng and Pammolli, Fabio and Riccaboni, Massimo and Stanley, H. Eugene},
	month = sep,
	year = {2006},
	pmid = {17025688},
	pages = {035103}
}

@misc{_ohloh_????,
	title = {Ohloh},
	url = {https://www.ohloh.net/},
	urldate = {2014-06-30},
	journal = {Ohloh, the open source network}
}

@inproceedings{fagin_comparing_2003,
	address = {Philadelphia, PA, USA},
	series = {{SODA} '03},
	title = {Comparing {Top} {K} {Lists}},
	isbn = {0-89871-538-5},
	url = {http://dl.acm.org/citation.cfm?id=644108.644113},
	abstract = {Motivated by several applications, we introduce various distance measures between "top k lists." Some of these distance measures are metrics, while others are not. For each of these latter distance measures: we show that it is "almost" a metric in the following two seemingly unrelated aspects:step-(i) it satisfies a relaxed version of the polygonal (hence, triangle) inequality, andstep-(ii) there is a metric with positive constant multiples that bounds our measure above and below.This is not a coincidence---we show that these two notions of almost being a metric are the same. Based on the second notion, we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other. We thereby identify a large and robust equivalence class of distance measures.Besides the applications to the task of identifying good notions of (dis-)similarity between two top k lists, our results imply polynomial-time constant-factor approximation algorithms for the rank aggregation problem with respect to a large class of distance measures.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the {Fourteenth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Fagin, Ronald and Kumar, Ravi and Sivakumar, D.},
	year = {2003},
	pages = {28--36}
}

@misc{_nostra13/android-universal-image-loader_????,
	title = {nostra13/{Android}-{Universal}-{Image}-{Loader}},
	url = {https://github.com/nostra13/Android-Universal-Image-Loader},
	abstract = {Android-Universal-Image-Loader - Powerful and flexible library for loading, caching and displaying images on Android.},
	urldate = {2015-02-25},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QJBFGVJ8\\Android-Universal-Image-Loader.html:text/html}
}

@article{kocaguneli_kernel_2013,
	title = {Kernel methods for software effort estimation},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-011-9189-1},
	doi = {10.1007/s10664-011-9189-1},
	abstract = {Analogy based estimation (ABE) generates an effort estimate for a new software project through adaptation of similar past projects (a.k.a. analogies). Majority of ABE methods follow uniform weighting in adaptation procedure. In this research we investigated non-uniform weighting through kernel density estimation. After an extensive experimentation of 19 datasets, 3 evaluation criteria, 5 kernels, 5 bandwidth values and a total of 2090 ABE variants, we found that: (1) non-uniform weighting through kernel methods cannot outperform uniform weighting ABE and (2) kernel type and bandwidth parameters do not produce a definite effect on estimation performance. In summary simple ABE approaches are able to perform better than much more complex approaches. Hence,—provided that similar experimental settings are adopted—we discourage the use of kernel methods as a weighting strategy in ABE.},
	language = {en},
	number = {1},
	urldate = {2014-08-20},
	journal = {Empir Software Eng},
	author = {Kocaguneli, Ekrem and Menzies, Tim and Keung, Jacky W.},
	month = feb,
	year = {2013},
	keywords = {Bandwidth, data mining, Effort estimation, Kernel function, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems},
	pages = {1--24},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\SXFTICXX\\Kocaguneli et al. - 2013 - Kernel methods for software effort estimation.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\74J29ZRT\\s10664-011-9189-1.html:text/html}
}

@misc{_sourceforge_????,
	title = {{SourceForge} - {Download}, {Develop} and {Publish} {Free} {Open} {Source} {Software}},
	url = {http://sourceforge.net/}
}

@article{hales_formal_2008,
	title = {Formal {Proof}},
	volume = {55},
	number = {11},
	journal = {Notices of the AMS},
	author = {Hales, Thomas},
	month = dec,
	year = {2008},
	pages = {1370--1380}
}

@misc{_datahub:_????,
	title = {the {Datahub}: {Welcome}},
	url = {http://datahub.io/}
}

@article{chaikalis_forecasting_2015,
	title = {Forecasting {Java} {Software} {Evolution} {Trends} employing {Network} {Models}},
	volume = {PP},
	issn = {0098-5589},
	doi = {10.1109/TSE.2014.2381249},
	abstract = {The evolution of networks representing systems in various domains, including social networks, has been extensively studied enabling the development of growth models which govern their behavior over time. The architecture of software systems can also be naturally represented in the form of networks, whose properties change as software evolves. In this paper we attempt to model several aspects of graphs representing object-oriented software systems as they evolve over a number of versions. The goal is to develop a prediction model by considering global phenomena such as preferential attachment, past evolutionary trends such as the tendency of classes to create fewer relations as they age, as well as domain knowledge in terms of principles that have to be followed in object-oriented design. The derived models can provide insight into the future trends of software and potentially form the basis for eliciting improved or novel laws of software evolution. The forecasting power of the proposed model is evaluated against the actual evolution of ten open-source projects and the achieved accuracy in the prediction of several network and software properties, which reflect the underlying system design, appears to be promising.},
	number = {99},
	journal = {IEEE Transactions on Software Engineering},
	author = {Chaikalis, T. and Chatzigeorgiou, A.},
	year = {2015},
	note = {00000},
	keywords = {Analytical models, Forecasting, Graphs and networks, Market research, Objectoriented design methods, Object oriented modeling, Predictive models, reengineering, Restructuring, reverse engineering, Software Architectures, Software systems},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\I33PDP4Z\\articleDetails.html:text/html}
}

@article{watts_collective_1998,
	title = {Collective dynamics of ‘small-world’ networks},
	volume = {393},
	copyright = {© 1998 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v393/n6684/abs/393440a0.html},
	doi = {10.1038/30918},
	abstract = {Networks of coupled dynamical systems have been used to model biological oscillators, Josephson junction arrays,, excitable media, neural networks, spatial games, genetic control networks and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon, (popularly known as six degrees of separation). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
	language = {en},
	number = {6684},
	urldate = {2014-01-30},
	journal = {Nature},
	author = {Watts, Duncan J. and Strogatz, Steven H.},
	month = jun,
	year = {1998},
	pages = {440--442},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\AQK97R7T\\393440a0.html:text/html}
}

@article{rich_organizational_1992,
	title = {The {Organizational} {Taxonomy}: {Definition} and {Design}},
	volume = {17},
	issn = {03637425},
	shorttitle = {The {Organizational} {Taxonomy}},
	url = {http://www.jstor.org/discover/10.2307/258807?uid=3738128&uid=2134&uid=2&uid=70&uid=4&sid=21103300880791},
	doi = {10.2307/258807},
	number = {4},
	urldate = {2014-01-26},
	journal = {The Academy of Management Review},
	author = {Rich, Philip},
	month = oct,
	year = {1992},
	pages = {758},
	file = {JSTOR\: The Academy of Management Review, Vol. 17, No. 4 (Oct., 1992), pp. 758-781:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HBA34BFP\\258807.html:text/html}
}

@article{girba_modeling_2006,
	title = {Modeling {History} to {Analyze} {Software} {Evolution}: {Research} {Articles}},
	volume = {18},
	issn = {1532-060X},
	shorttitle = {Modeling {History} to {Analyze} {Software} {Evolution}},
	url = {http://dx.doi.org/10.1002/smr.v18:3},
	doi = {10.1002/smr.v18:3},
	abstract = {The histories of software systems hold useful information when reasoning about the systems at hand or when reasoning about general laws of software evolution. Over the past 30 years, research has been increasingly spent on understanding software evolution. However, the approaches developed so far do not rely on an explicit meta-model and, thus, they make it difficult to reuse or compare their results. We argue that there is a need for an explicit meta-model for software evolution analysis. We present a survey of the evolution analyses and deduce a set of requirements that an evolution meta-model should have. We define Hismo, a meta-model in which history is modeled as an explicit entity. Hismo adds a time layer on top of structural information, and provides a common infrastructure for expressing and combining evolution analyses and structural analyses. We validate the usefulness of our meta-model by presenting how different analyses are expressed on it. Copyright © 2006 John Wiley \& Sons, Ltd.},
	number = {3},
	urldate = {2014-01-30},
	journal = {J. Softw. Maint. Evol.},
	author = {Gîrba, Tudor and Ducasse, Stéphane},
	month = may,
	year = {2006},
	keywords = {evolution analysis, History, meta-modeling, reverse engineering, Software evolution},
	pages = {207--236}
}

@article{miyazaki_robust_1994-1,
	title = {Robust regression for developing software estimation models},
	volume = {27},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/0164121294901104},
	doi = {10.1016/0164-1212(94)90110-4},
	abstract = {To develop a good software estimation model fitted to actual data, the evaluation criteria of goodness of fit is necessary. The first major problem discussed here is that ordinary relative error used for this criterion is not suitable because it has a bound in the case of under-estimation and no bound in the case of overestimation. We propose use of a new relative error called balanced relative error as the basis for the criterion and introduce seven evaluation criteria for software estimation models. The second major problem is that the ordinary least-squares method used for calculation of parameter values of a software estimation model is neither consistent with the criteria nor robust enough, which means that the solution is easily distorted by outliers. We propose a new consistent and robust method called the least-squares of inverted balanced relative errors (LIRS) and demonstrates its superiority to the ordinary least-squares method by use of five actual data sets. Through the analysis of these five data sets with LIRS, we show the importance of consistent data collection and development standarization to develop a good software sizing model. We compare goodness of fit between the sizing model based on the number of screens, forms, and files, and the sizing model based on the number of data elements for each of them. Based on this comparison, the validity of the number of data elements as independent variables for a sizing model is examined. Moreover, the validity of increasing the number of independent variables is examined.},
	number = {1},
	urldate = {2014-08-21},
	journal = {Journal of Systems and Software},
	author = {Miyazaki, Y. and Terakado, M. and Ozaki, K. and Nozaki, H.},
	month = oct,
	year = {1994},
	pages = {3--16},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NZPBT69X\\Miyazaki et al. - 1994 - Robust regression for developing software estimati.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GIX3IZ9H\\0164121294901104.html:text/html}
}

@article{chatzigeorgiou_benchmarking_2011,
	title = {Benchmarking library and application software with {Data} {Envelopment} {Analysis}},
	volume = {19},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/article/10.1007/s11219-010-9113-8},
	doi = {10.1007/s11219-010-9113-8},
	abstract = {Library software is generally believed to be well-structured and follows certain design guidelines due to the need of continuous evolution and stability of the respective APIs. We perform an empirical study to investigate whether the design of open-source library software is actually superior to that of application software. By analyzing certain design principles and heuristics that are considered important for API design, we extract a set of software metrics that are expected to reflect the improved nature of libraries. An initial comparison by conventional statistical analysis confirms the overall belief that products of different software size scale should not be compared by simply examining metric values in isolation. In this paper, we propose the use of Data Envelopment Analysis (DEA), borrowed from production economics, as a means of measuring and benchmarking the quality of different object-oriented software designs captured by software metrics and apply this approach to the comparison of library and application software. The advantages offered by DEA and the differences between the application of DEA in an economic and a technological context are discussed. Results of the approach are presented for 44 open-source projects, equally divided between libraries and applications.},
	language = {en},
	number = {3},
	urldate = {2014-03-28},
	journal = {Software Qual J},
	author = {Chatzigeorgiou, Alexander and Stiakakis, Emmanouil},
	month = sep,
	year = {2011},
	keywords = {Benchmarking, Data Envelopment Analysis, Data Structures, Cryptology and Information Theory, Efficiency, object-oriented design, Operating Systems, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, software metrics},
	pages = {553--578},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\XH9G3D8K\\Chatzigeorgiou and Stiakakis - 2011 - Benchmarking library and application software with.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TITF3UT7\\10.html:text/html}
}

@book{robbes_change_2009,
	title = {Of {Change} and {Software}},
	isbn = {9781442153745},
	abstract = {Software changes. Any long-lived software system has maintenance costs dominating its initial development costs as it is adapted to new or changing requirements. Systems on which such continuous changes are performed inevitably decay, making maintenance harder. This   problem is not new: The software evolution research community has been tackling it for more than two decades. However, most approaches have been targeting specific maintenance activities using an ad-hoc model of software evolution.     Instead of only addressing individual maintenance activities, we propose to take a step back and address the software evolution problem at its root by treating change as a ?rst-class entity. We apply the strategy of rei?cation, used with success in other branches of software engineering, to the changes software systems experience. Our thesis is that a rei?ed change-based representation of software enables better evolution support for both reverse and forward engineering activities.},
	language = {English},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Robbes, Romain},
	month = aug,
	year = {2009}
}

@inproceedings{pinzger_can_2008,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '08/{FSE}-16},
	title = {Can {Developer}-module {Networks} {Predict} {Failures}?},
	isbn = {978-1-59593-995-1},
	url = {http://doi.acm.org/10.1145/1453101.1453105},
	doi = {10.1145/1453101.1453105},
	abstract = {Software teams should follow a well defined goal and keep their work focused. Work fragmentation is bad for efficiency and quality. In this paper we empirically investigate the relationship between the fragmentation of developer contributions and the number of post-release failures. Our approach is to represent developer contributions with a developer-module network that we call contribution network. We use network centrality measures to measure the degree of fragmentation of developer contributions. Fragmentation is determined by the centrality of software modules in the contribution network. Our claim is that central software modules are more likely to be failure-prone than modules located in surrounding areas of the network. We analyze this hypothesis by exploring the network centrality of Microsoft Windows Vista binaries using several network centrality measures as well as linear and logistic regression analysis. In particular, we investigate which centrality measures are significant to predict the probability and number of post-release failures. Results of our experiments show that central modules are more failure-prone than modules located in surrounding areas of the network. Results further confirm that number of authors and number of commits are significant predictors for the probability of post-release failures. For predicting the number of post-release failures the closeness centrality measure is most significant.},
	urldate = {2014-07-22},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
	year = {2008},
	keywords = {developer contribution network, failure prediction, network centrality measures, social network analysis},
	pages = {2--12},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BMI9NQ49\\Pinzger et al. - 2008 - Can Developer-module Networks Predict Failures.pdf:application/pdf}
}

@misc{_open_????,
	title = {Open {Science}},
	url = {http://openscience.us/}
}

@article{gray_comparison_1997,
	title = {A comparison of techniques for developing predictive models of software metrics},
	volume = {39},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584996000067},
	doi = {10.1016/S0950-5849(96)00006-7},
	abstract = {The use of regression analysis to derive predictive equations for software metrics has recently been complemented by increasing numbers of studies using non-traditional methods, such as neural networks, fuzzy logic models, case-based reasoning systems, and regression trees. There has also been an increasing level of sophistication in the regression-based techniques used, including robust regression methods, factor analysis, and more effective validation procedures. This paper examines the implications of using these methods and provides some recommendations as to when they may be appropriate. A comparison of the various techniques is also made in terms of their modelling capabilities with specific reference to software metrics.},
	number = {6},
	urldate = {2014-08-21},
	journal = {Information and Software Technology},
	author = {Gray, Andrew R. and MacDonell, Stephen G.},
	year = {1997},
	keywords = {Analysis techniques, Metrics, Predictive models},
	pages = {425--437},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TG8W74BK\\Gray and MacDonell - 1997 - A comparison of techniques for developing predicti.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\J2IUNZGI\\S0950584996000067.html:text/html}
}

@inproceedings{paymal_measuring_2011,
	title = {Measuring disruption from software evolution activities using graph-based metrics},
	doi = {10.1109/ICSM.2011.6080825},
	abstract = {In this paper, we investigate how class relationships are disrupted after large scale changes. We use graphs to represent different software versions and study changes to graph properties. We explore different combinatorial metrics to measure the extent of disruption after perfective maintenance activities. Our early results, on JHotDraw, demonstrate that combinatorial metrics can provide a good indicator to the degree to which relationships are disrupted or preserved across different versions.},
	booktitle = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {Paymal, P. and Patil, R. and Bhowmick, S. and Siy, H.},
	month = sep,
	year = {2011},
	keywords = {combinatorial metrics, Correlation, disruption measurement, dynamic network analysis, Electric breakdown, graph based metrics, graph properties, graph theory, JHotDraw, Maintenance engineering, Measurement, perfective maintenance, Social network services, software evolution activities, software metrics, Software systems},
	pages = {532--535},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RJFMZQZS\\login.html:text/html}
}

@misc{_international_????,
	title = {International {Software} {Engineering} {Research} {Network}},
	url = {http://isern.iese.de/Portal/}
}

@misc{_computer_2013,
	title = {Computer {Science} {Curricula} 2013},
	publisher = {The Joint Task Force on Computing Curricula Association for Computing Machinery (ACM) IEEE Computer Society},
	month = dec,
	year = {2013}
}

@article{alonso-alemany_further_2014,
	title = {Further {Steps} in {TANGO}: improved taxonomic assignment in metagenomics},
	volume = {30},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Further {Steps} in {TANGO}},
	url = {http://bioinformatics.oxfordjournals.org/content/30/1/17},
	doi = {10.1093/bioinformatics/btt256},
	abstract = {Motivation: TANGO is one of the most accurate tools for the taxonomic assignment of sequence reads. However, because of the differences in the taxonomy structures, performing a taxonomic assignment on different reference taxonomies will produce divergent results.
Results: We have improved the TANGO pipeline to be able to perform the taxonomic assignment of a metagenomic sample using alternative reference taxonomies, coming from different sources. We highlight the novel pre-processing step, necessary to accomplish this task, and describe the improvements in the assignment process. We present the new TANGO pipeline in details, and, finally, we show its performance on four real metagenomic datasets and also on synthetic datasets.
Availability: The new version of TANGO, including implementation improvements and novel developments to perform the assignment on different reference taxonomies, is freely available at http://sourceforge.net/projects/taxoassignment/.
Contact: valiente@lsi.upc.edu},
	language = {en},
	number = {1},
	urldate = {2014-01-20},
	journal = {Bioinformatics},
	author = {Alonso-Alemany, Daniel and Barré, Aurélien and Beretta, Stefano and Bonizzoni, Paola and Nikolski, Macha and Valiente, Gabriel},
	month = jan,
	year = {2014},
	pmid = {23645816},
	pages = {17--23},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7SWGWAG4\\17.html:text/html}
}

@inproceedings{kim_predicting_2007,
	address = {Washington, DC, USA},
	series = {{ICSE} '07},
	title = {Predicting {Faults} from {Cached} {History}},
	isbn = {0-7695-2828-7},
	url = {http://dx.doi.org/10.1109/ICSE.2007.66},
	doi = {10.1109/ICSE.2007.66},
	abstract = {We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10\% of the source code files; these files account for 73\%-95\% of faults-- a significant advance beyond the state of the art.},
	urldate = {2014-08-21},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Kim, Sunghun and Zimmermann, Thomas and Whitehead Jr., E. James and Zeller, Andreas},
	year = {2007},
	pages = {489--498},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\M8JQRTES\\Kim et al. - 2007 - Predicting Faults from Cached History.pdf:application/pdf}
}

@inproceedings{bevan_facilitating_2005-1,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {Facilitating {Software} {Evolution} {Research} with {Kenyon}},
	isbn = {1-59593-014-0},
	url = {http://doi.acm.org/10.1145/1081706.1081736},
	doi = {10.1145/1081706.1081736},
	abstract = {Software evolution research inherently has several resource-intensive logistical constraints. Archived project artifacts, such as those found in source code repositories and bug tracking systems, are the principal source of input data. Analysis-specific facts, such as commit metadata or the location of design patterns within the code, must be extracted for each change or configuration of interest. The results of this resource-intensive "fact extraction" phase must be stored efficiently, for later use by more experimental types of research tasks, such as algorithm or model refinement. In order to perform any type of software evolution research, each of these logistical issues must be addressed and an implementation to manage it created. In this paper, we introduce Kenyon, a system designed to facilitate software evolution research by providing a common set of solutions to these common logistical problems. We have used Kenyon for processing source code data from 12 systems of varying sizes and domains, archived in 3 different types of software configuration management systems. We present our experiences using Kenyon with these systems, and also describe Kenyon's usage by students in a graduate seminar class.},
	urldate = {2014-03-11},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Bevan, Jennifer and Whitehead,Jr., E. James and Kim, Sunghun and Godfrey, Michael},
	year = {2005},
	keywords = {software configuration management, Software evolution, software stratigraphy},
	pages = {177--186},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RE5IKDP6\\Bevan et al. - 2005 - Facilitating Software Evolution Research with Keny.pdf:application/pdf}
}

@inproceedings{chatzigeorgiou_trends_2012,
	title = {Trends in object-oriented software evolution: {Investigating} network properties},
	shorttitle = {Trends in object-oriented software evolution},
	doi = {10.1109/ICSE.2012.6227092},
	abstract = {The rise of social networks and the accompanying interest to study their evolution has stimulated a number of research efforts to analyze their growth patterns by means of network analysis. The inherent graph-like structure of object-oriented systems calls for the application of the corresponding methods and tools to analyze software evolution. In this paper we investigate network properties of two open-source systems and observe interesting phenomena regarding their growth. Relating the observed evolutionary trends to principles and laws of software design enables a highlevel assessment of tendencies in the underlying design quality.},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Chatzigeorgiou, A. and Melas, G.},
	month = jun,
	year = {2012},
	keywords = {Communities, design quality, graph-like structure, graph theory, growth patterns, Maintenance engineering, network analysis, network properties, object-oriented design, Object oriented modeling, object-oriented programming, object-oriented software evolution, object-oriented systems, open-source systems, public domain software, social networking (online), social networks, Social network services, Software design, Software evolution, software maintenance, Software quality, Software systems, Vegetation},
	pages = {1309--1312},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\A8F3H9X5\\login.html:text/html}
}

@misc{_ipa_????,
	title = {{IPA} {Information}-technology {Promotion} {Agency}, {Japan}},
	url = {http://www.ipa.go.jp/english/sec/}
}

@inproceedings{wheeldon_power_2003,
	title = {Power law distributions in class relationships},
	doi = {10.1109/SCAM.2003.1238030},
	abstract = {Power law distributions have been found in many natural and social phenomena, and more recently in the source code and run-time characteristics of Object-Oriented (OO) systems. A power law implies that small values are extremely common, whereas large values are extremely rare. We identify twelve new power laws relating to the static graph structures of Java programs. The graph structures analyzed represented different forms of OO coupling, namely, inheritance, aggregation, interface, parameter type and return type. Identification of these new laws provides the basis for predicting likely features of classes in future developments. The research ties together work in object-based coupling and World Wide Web structures.},
	booktitle = {Third {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}, 2003. {Proceedings}},
	author = {Wheeldon, R. and Counsell, S.},
	month = sep,
	year = {2003},
	keywords = {aggregation, Cities and towns, Computer science, data flow graphs, Earthquakes, Educational institutions, Frequency, Information systems, inheritance, Java, Java program, object-based coupling, object-oriented programming, object-oriented system, power law distribution, Runtime, run-time characteristics, source coding, static graph structure, Topology, Web sites, World Wide Web structure},
	pages = {45--54},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MMZNX8JT\\login.html:text/html}
}

@article{raja_defining_2012,
	title = {Defining and {Evaluating} a {Measure} of {Open} {Source} {Project} {Survivability}},
	volume = {38},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.39},
	abstract = {In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Raja, U. and Tretter, M.J.},
	month = jan,
	year = {2012},
	keywords = {Evaluation framework, external validity, Indexes, Maintenance engineering, multidimensional measure, open source project survivability, Open source software, open source software project survivability, organization, project evaluation, Project management, project viability, public domain software, resilience, Software measurement, software metrics, software survivability., viability index, vigor},
	pages = {163--174},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BPEKGQHC\\abs_all.html:text/html}
}

@article{willinger_mathematics_2009,
	title = {Mathematics and the {Internet}: {A} {Source} of {Enormous} {Confusion} and {Great} {Potential}},
	volume = {56},
	shorttitle = {Mathematics and the {Internet}},
	abstract = {For many mathematicians and physicists, the Internet has become a popular realworld domain for the application and/or development of new theories related to the organization and behavior of large-scale, complex, and dynamic systems. In some cases, the Internet has served both as inspiration and justification for the popularization of new models and mathematics within the scientific enterprise. For example, scale-free network models of the preferential attachment type [8] have been claimed to describe the Internet’s connectivity structure, resulting in surprisingly general and strong claims about the network’s resilience to random failures of its components and its vulnerability to targeted attacks against its infrastructure [2]. These models have, as their trademark, power-law type node degree distributions that drastically distinguish them from the classical Erdős-Rényi type random graph models [13]. These “scale-free ” network models have attracted significant attention within the scientific community and have been partly responsible for launching and fueling the new field of network science [42, 4]. To date, the main role that mathematics has played in network science has been to put the physicists’ largely empirical findings on solid grounds Walter Willinger is at AT\&T Labs-Research in Florham Park, NJ. His email address is walter@research.att. com.},
	number = {5},
	journal = {Notices of the American Mathematical Society},
	author = {Willinger, Walter and Alderson, David and Doyle, John C.},
	year = {2009},
	pages = {586--599},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\9IDW6RG6\\Willinger et al. - Mathematics and the Internet A Source of Enormous.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BRIGFPMM\\summary.html:text/html}
}

@misc{_buco_2014,
	title = {{BuCo} {Reporter}},
	url = {http://java.uom.gr/buco/},
	urldate = {2014-03-11},
	journal = {BuCo Reporter},
	year = {2014}
}

@article{chang_time-line_2008,
	title = {Time-line based model for software project scheduling with genetic algorithms},
	volume = {50},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584908000372},
	doi = {10.1016/j.infsof.2008.03.002},
	abstract = {Effective management of complex software projects depends on the ability to solve complex, subtle optimization problems. Most studies on software project management do not pay enough attention to difficult problems such as employee-to-task assignments, which require optimal schedules and careful use of resources. Commercial tools, such as Microsoft Project, assume that managers as users are capable of assigning tasks to employees to achieve the efficiency of resource utilization, while the project continually evolves. Our earlier work applied genetic algorithms (GAs) to these problems. This paper extends that work, introducing a new, richer model that is capable of more realistically simulating real-world situations. The new model is described along with a new GA that produces optimal or near-optimal schedules. Simulation results show that this new model enhances the ability of GA-based approaches, while providing decision support under more realistic conditions.},
	number = {11},
	urldate = {2014-08-27},
	journal = {Information and Software Technology},
	author = {Chang, Carl K. and Jiang, Hsin-yi and Di, Yu and Zhu, Dan and Ge, Yujia},
	month = oct,
	year = {2008},
	keywords = {Genetic algorithm, Optimization, Project management, Scheduling, task assignment},
	pages = {1142--1154},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DAH82SMH\\S0950584908000372.html:text/html}
}

@incollection{mens_introduction_2008,
	title = {Introduction and {Roadmap}: {History} and {Challenges} of {Software} {Evolution}},
	copyright = {©2008 Springer-Verlag},
	isbn = {978-3-540-76439-7, 978-3-540-76440-3},
	shorttitle = {Introduction and {Roadmap}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76440-3_1},
	abstract = {The ability to evolve software rapidly and reliably is a major challenge for software engineering. In this introductory chapter we start with a historic overview of the research domain of software evolution. Next, we briefly introduce the important research themes in software evolution, and identify research challenges for the years to come. Finally, we provide a roadmap of the topics treated in this book, and explain how the various chapters are related.},
	language = {en},
	urldate = {2014-01-30},
	booktitle = {Software {Evolution}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mens, Tom},
	month = jan,
	year = {2008},
	keywords = {Information Systems Applications (incl.Internet), Management of Computing and Information Systems, Software engineering},
	pages = {1--11},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\4WVHUE5N\\10.html:text/html}
}

@article{dijkstra_note_1959,
	title = {A note on two problems in connexion with graphs},
	volume = {1},
	issn = {0029-599X, 0945-3245},
	url = {http://link.springer.com/article/10.1007/BF01386390},
	doi = {10.1007/BF01386390},
	language = {en},
	number = {1},
	urldate = {2014-01-30},
	journal = {Numer. Math.},
	author = {Dijkstra, E. W.},
	month = dec,
	year = {1959},
	keywords = {Appl.Mathematics/Computational Methods of Engineering, Mathematical and Computational Physics, Mathematical Methods in Physics, Mathematics, general, Numerical Analysis, Numerical and Computational Methods},
	pages = {269--271},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QJMNCGM7\\10.html:text/html}
}

@inproceedings{sjoberg_conducting_2002,
	title = {Conducting realistic experiments in software engineering},
	doi = {10.1109/ISESE.2002.1166921},
	abstract = {An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.},
	booktitle = {Empirical {Software} {Engineering}, 2002. {Proceedings}. 2002 {International} {Symposium} n},
	author = {Sjoberg, Dag I.K. and Anda, B. and Arisholm, E. and Dyba, Tore and Jorgensen, M. and Karahasanovic, A. and Koren, E.F. and Vokac, M.},
	year = {2002},
	keywords = {Application software, case studies, Computer industry, consultancies, consultancy companies, empirical software engineering research, Glass, Hardware, industrial applications, Laboratories, Programming, realistic experiments, Software engineering, software maintenance, software professional, technology transfer, Telephony},
	pages = {17--26},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KSJF8VRM\\login.html:text/html}
}

@book{mens_software_2008,
	address = {Berlin, Heidelberg},
	title = {Software {Evolution}},
	isbn = {978-3-540-76439-7, 978-3-540-76440-3},
	url = {http://link.springer.com/10.1007/978-3-540-76440-3},
	language = {en},
	urldate = {2015-03-27},
	publisher = {Springer Berlin Heidelberg},
	author = {Mens, Tom and Demeyer, Serge},
	year = {2008}
}

@misc{_junit-team/junit_????,
	title = {junit-team/junit},
	url = {https://github.com/junit-team/junit},
	abstract = {junit - A programmer-oriented testing framework for Java.},
	urldate = {2015-02-25},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UWIJR6NE\\junit.html:text/html}
}

@incollection{olson_applications_2008,
	title = {Applications of {Methods}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_10},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {151--167},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\EBWUTFAN\\978-3-540-76917-0_10.html:text/html}
}

@misc{_machine_????,
	title = {Machine {Learning} {Repository}},
	url = {http://archive.ics.uci.edu/ml/}
}

@article{valverde_network_2005,
	title = {Network motifs in computational graphs: a case study in software architecture},
	volume = {72},
	issn = {1539-3755},
	shorttitle = {Network motifs in computational graphs},
	abstract = {Complex networks in both nature and technology have been shown to display characteristic, small subgraphs (so-called motifs) which appear to be related to their underlying functionality. All these networks share a common trait: they manipulate information at different scales in order to perform some kind of computation. Here we analyze a large set of software class diagrams and show that several highly frequent network motifs appear to be a consequence of network heterogeneity and size, thus suggesting a somewhat less relevant role of functionality. However, by using a simple model of network growth by duplication and rewiring, it is shown the rules of graph evolution seem to be largely responsible for the observed motif distribution.},
	language = {eng},
	number = {2 Pt 2},
	journal = {Phys Rev E Stat Nonlin Soft Matter Phys},
	author = {Valverde, Sergi and Solé, Ricard V},
	month = aug,
	year = {2005},
	pmid = {16196644},
	pages = {026107}
}

@article{wilkie_coupling_2000,
	title = {Coupling measures and change ripples in {C}++ application software},
	volume = {52},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121299001429},
	doi = {10.1016/S0164-1212(99)00142-9},
	abstract = {This paper describes an investigation into the effects of class couplings on changes made to a commercial C++ application over a period of {\textless}img height="35" border="0" style="vertical-align:bottom" width="22" alt="View the MathML source" title="View the MathML source" src="http://origin-ars.els-cdn.com/content/image/1-s2.0-S0164121299001429-si1.gif"{\textgreater} yr. The Chidamber and Kemerer CBO metric is used to measure class couplings within the application and its limitations are identified. Through an in-depth study of the ripple effects of changes to the source code, practical insight into the nature and extent of software couplings is provided.},
	number = {2–3},
	urldate = {2014-02-24},
	journal = {Journal of Systems and Software},
	author = {Wilkie, F. G and Kitchenham, B. A},
	month = jun,
	year = {2000},
	pages = {157--164},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\85WUPDD8\\Wilkie and Kitchenham - 2000 - Coupling measures and change ripples in C++ applic.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\75U2M6UG\\S0164121299001429.html:text/html}
}

@inproceedings{meneely_predicting_2008,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '08/{FSE}-16},
	title = {Predicting {Failures} with {Developer} {Networks} and {Social} {Network} {Analysis}},
	isbn = {978-1-59593-995-1},
	url = {http://doi.acm.org/10.1145/1453101.1453106},
	doi = {10.1145/1453101.1453106},
	abstract = {Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58\% of the failures in 20\% of the files compared with the optimal prioritization that would have found 61\% in 20\% of the files, indicating that a significant correlation exists between file-based developer network metrics and failures.},
	urldate = {2014-07-22},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
	year = {2008},
	keywords = {developer network, failure prediction, logistic regression, negative binomial regression, social network analysis},
	pages = {13--23},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PV3ICSAD\\Meneely et al. - 2008 - Predicting Failures with Developer Networks and So.pdf:application/pdf}
}

@book{mcdonald_handbook_2009,
	title = {Handbook of {Biological} {Statistics}},
	copyright = {John H. McDonald (Standard Copyright License)},
	abstract = {The third edition will be available by the end of July, 2014; you should wait and buy it instead. In the meantime, you can use the online third edition at www.biostathandbook.com , or get a free pdf of the second edition from http://www.lulu.com/product/18578349 .},
	language = {English},
	publisher = {Sparky House Publishing},
	author = {McDonald, John},
	month = sep,
	year = {2009}
}

@inproceedings{nickerson_taxonomy_2009,
	title = {Taxonomy development in information systems: developing a taxonomy of mobile applications},
	shorttitle = {{TAXONOMY} {DEVELOPMENT} {IN} {INFORMATION} {SYSTEMS}},
	url = {http://halshs.archives-ouvertes.fr/halshs-00375103},
	abstract = {The complexity of the information systems field often lends itself to classification schemes, or taxonomies, which provide ways to understand the similarities and differences among objects under study. Developing a taxonomy, however, is a complex process that is often done in an ad hoc way. This research-in-progress paper uses the design science paradigm to develop a systematic method for taxonomy development in information systems. The method we propose uses an indicator or operational level model that combines both empirical to deductive and deductive to empirical approaches. We evaluate this method by using it to develop a taxonomy of mobile applications, which we have chosen because of their ever-increasing number and variety. The resulting taxonomy contains seven dimensions with fifteen characteristics. We demonstrate the usefulness of this taxonomy by analyzing a range of current and proposed mobile applications. From the results of this analysis we identify combinations of characteristics where applications are missing and thus are candidates for new and potentially useful applications.},
	urldate = {2014-01-26},
	booktitle = {European {Conference} in {Information} {Systems}},
	author = {Nickerson, Robert and Muntermann, Jan and Varshney, Upkar and Isaac, Henri},
	year = {2009},
	keywords = {design science, mobile application, taxonomy},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\46884PJM\\Nickerson et al. - 2009 - TAXONOMY DEVELOPMENT IN INFORMATION SYSTEMS DEVEL.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\K3F86IK6\\halshs-00375103.html:text/html}
}

@inproceedings{zimmermann_predicting_2008-1,
	address = {New York, NY, USA},
	series = {{ICSE} '08},
	title = {Predicting {Defects} {Using} {Network} {Analysis} on {Dependency} {Graphs}},
	isbn = {978-1-60558-079-1},
	url = {http://doi.acm.org/10.1145/1368088.1368161},
	doi = {10.1145/1368088.1368161},
	abstract = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10\% points higher than for models built from complexity metrics. In addition, network measures could identify 60\% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.},
	urldate = {2014-08-25},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2008},
	keywords = {defect prediction, dependency graph, network analysis, windows server 2003},
	pages = {531--540}
}

@inproceedings{wang_linux_2009,
	title = {Linux kernels as complex networks: {A} novel method to study evolution},
	shorttitle = {Linux kernels as complex networks},
	doi = {10.1109/ICSM.2009.5306348},
	abstract = {In recent years, many graphs have turned out to be complex networks. This paper presents a novel method to study Linux kernel evolution - using complex networks to understand how Linux kernel modules evolve over time. After studying the node degree distribution and average path length of the call graphs corresponding to the kernel modules of 223 different versions (V1.1.0 to V2.4.35), we found that the call graphs of the file system and drivers module are scale-free small-world complex networks. In addition, both of the file system and drivers module exhibit very strong preferential attachment tendency. Finally, we proposed a generic method that could be used to find major structural changes that occur during the evolution of software systems.},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance}, 2009. {ICSM} 2009},
	author = {Wang, Lei and Wang, Zheng and Yang, Chen and Zhang, Li and Ye, Qiang},
	month = sep,
	year = {2009},
	keywords = {Complex networks, Computer science, Evolution (biology), File systems, IP networks, Kernel, Linux, Linux kernel evolution, Linux kernel modules, Open source software, scale-free small-world complex networks, Social network services, Software engineering, software system evolution, Software systems},
	pages = {41--50},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TWHMVZI9\\login.html:text/html}
}

@inproceedings{shibata_quantifying_2007,
	title = {Quantifying {Software} {Maintainability} {Based} on a {Fault}-{Detection}/{Correction} {Model}},
	doi = {10.1109/PRDC.2007.46},
	abstract = {The software fault correction profiles play significant roles to assess the quality of software testing as well as to keep the good software maintenance activity. In this paper we develop a quantitative method to evaluate the software maintainability based on a stochastic model. The model proposed here is a queueing model with an infinite number of servers, and is related to the software fault- detection/correction profiles. Based on the familiar maximum likelihood estimation, we estimate quantitatively both the software reliability and maintainability with real project data, and refer to their applicability to the software maintenance practice.},
	booktitle = {13th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing}, 2007. {PRDC} 2007},
	author = {Shibata, K. and Rinsaka, K. and Dohi, T. and Okamura, H.},
	month = dec,
	year = {2007},
	keywords = {Debugging, fault-correction model, Fault detection, fault-detection model, Maintenance engineering, Maximum likelihood estimation, Predictive models, program testing, queueing model, queueing theory, software fault correction profile, software fault tolerance, software maintainability, software maintenance, Software quality, software reliability, Software testing, stochastic model, stochastic processes},
	pages = {35--42},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CFTP4U74\\abs_all.html:text/html}
}

@book{bloch_effective_2008,
	address = {Upper Saddle River, NJ},
	title = {Effective {Java}},
	isbn = {9780321356680  0321356683  0201310058 9780201310054},
	abstract = {Designed to help Java programmers make the most effective use of the Java programming language and its fundamental libraries, this updated edition includes more than 50 essays, each of which conveys one rule. Helping programmers sidestep common misconceptions and errors, each rule captures best practices that have been tested in the real world. Just one of the key features in this book are the code examples that illustrate many useful design patterns and idoms. Another key feature is the advice on what not to do. Providing examples of what practices to avoid helps programmers side step common misconceptions and errors. While the second edition will cover all of the classic topics developers have come to rely on- objects, classes, libraries, methods, and serialization; new to this edition will be the coverage on generics, metadata, autoboxing, concurrency utilities, memory model, enumerations, and more. The book is based on the philosophy that clarity and simplicity are of paramount importance. The concise essays teach Java programmers of all levels how to write correct, clear, reusable, and effective code. Learning the art of Java programming, like most other disciples, consists of learning the rules and then learning when to violate them. With this book in hand, Java programmers will truly learn the rules and then learn when to violate them.},
	language = {English},
	publisher = {Addison-Wesley},
	author = {Bloch, Joshua},
	year = {2008}
}

@inproceedings{zimmermann_predicting_2007,
	address = {Washington, DC, USA},
	series = {{PROMISE} '07},
	title = {Predicting {Defects} for {Eclipse}},
	isbn = {0-7695-2954-2},
	url = {http://dx.doi.org/10.1109/PROMISE.2007.10},
	doi = {10.1109/PROMISE.2007.10},
	abstract = {We have mapped defects from the bug database of Eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the Eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
	urldate = {2014-08-27},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Predictor} {Models} in {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
	year = {2007},
	pages = {9}
}

@article{van_koten_application_2006,
	title = {An application of {Bayesian} network for predicting object-oriented software maintainability},
	volume = {48},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584905000339},
	doi = {10.1016/j.infsof.2005.03.002},
	abstract = {As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system.},
	number = {1},
	urldate = {2014-08-22},
	journal = {Information and Software Technology},
	author = {van Koten, C. and Gray, A. R.},
	month = jan,
	year = {2006},
	keywords = {Bayesian network, Maintainability, object-oriented systems, Regression, Regression tree},
	pages = {59--67},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ZTKUWZG2\\van Koten and Gray - 2006 - An application of Bayesian network for predicting .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\N5BK5GP3\\S0950584905000339.html:text/html}
}

@inproceedings{ghannem_detecting_2011,
	address = {Riverton, NJ, USA},
	series = {{CASCON} '11},
	title = {Detecting {Model} {Refactoring} {Opportunities} {Using} {Heuristic} {Search}},
	url = {http://dl.acm.org/citation.cfm?id=2093889.2093910},
	abstract = {Model-driven engineering (MDE) is an approach to software development where the primary focus is on models. To improve their quality, models continually evolve due, for example, to the detection of "bad design practices", called design defects. Presence of these defects in a model suggests refactoring opportunities. Most of the research work that tackle the problem of detecting and correcting defects, concentrate on source code. However, detecting defects at the model level and during the design process can be of great value to designers in particular within an MDE process. In this paper, we propose an automated approach to detect model refactoring opportunities related to various types of design defects. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. We evaluate our approach by finding three potential design defect types in two large class diagrams. For all these models, we succeed in detecting the majority of expected defects.},
	urldate = {2015-05-18},
	booktitle = {Proceedings of the 2011 {Conference} of the {Center} for {Advanced} {Studies} on {Collaborative} {Research}},
	publisher = {IBM Corp.},
	author = {Ghannem, Adnane and Kessentini, Marouane and El Boussaidi, Ghizlane},
	year = {2011},
	pages = {175--187},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WGW2XJA2\\Ghannem et al. - 2011 - Detecting Model Refactoring Opportunities Using He.pdf:application/pdf}
}

@book{bloch_effective_2008-1,
	address = {Upper Saddle River, NJ},
	title = {Effective {Java}},
	isbn = {9780321356680  0321356683  0201310058 9780201310054},
	abstract = {Designed to help Java programmers make the most effective use of the Java programming language and its fundamental libraries, this updated edition includes more than 50 essays, each of which conveys one rule. Helping programmers sidestep common misconceptions and errors, each rule captures best practices that have been tested in the real world. Just one of the key features in this book are the code examples that illustrate many useful design patterns and idoms. Another key feature is the advice on what not to do. Providing examples of what practices to avoid helps programmers side step common misconceptions and errors. While the second edition will cover all of the classic topics developers have come to rely on- objects, classes, libraries, methods, and serialization; new to this edition will be the coverage on generics, metadata, autoboxing, concurrency utilities, memory model, enumerations, and more. The book is based on the philosophy that clarity and simplicity are of paramount importance. The concise essays teach Java programmers of all levels how to write correct, clear, reusable, and effective code. Learning the art of Java programming, like most other disciples, consists of learning the rules and then learning when to violate them. With this book in hand, Java programmers will truly learn the rules and then learn when to violate them.},
	language = {English},
	publisher = {Addison-Wesley},
	author = {Bloch, Joshua},
	year = {2008}
}

@article{dyba_empirical_2008,
	title = {Empirical studies of agile software development: {A} systematic review},
	volume = {50},
	issn = {0950-5849},
	shorttitle = {Empirical studies of agile software development},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584908000256},
	doi = {10.1016/j.infsof.2008.01.006},
	abstract = {Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.},
	number = {9–10},
	urldate = {2014-03-26},
	journal = {Information and Software Technology},
	author = {Dybå, Tore and Dingsøyr, Torgeir},
	month = aug,
	year = {2008},
	keywords = {Agile software development, Empirical software engineering, Evidence-based software engineering, Extreme programming, Research synthesis, Scrum, Systematic review, XP},
	pages = {833--859},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MZ647B67\\Dybå and Dingsøyr - 2008 - Empirical studies of agile software development A.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7SCHG867\\S0950584908000256.html:text/html}
}

@article{glass_analysis_2004-2,
	title = {An {Analysis} of {Research} in {Computing} {Disciplines}},
	volume = {47},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/990680.990686},
	doi = {10.1145/990680.990686},
	abstract = {Comparing the topics and methods of the three major subdivisions of the computing realm.},
	number = {6},
	urldate = {2015-01-05},
	journal = {Commun. ACM},
	author = {Glass, Robert L. and Ramesh, V. and Vessey, Iris},
	month = jun,
	year = {2004},
	pages = {89--94}
}

@book{devroye_non-uniform_1986,
	address = {New York},
	title = {Non-{Uniform} {Random} {Variate} {Generation}},
	publisher = {Springer-Verlag},
	author = {Devroye, Luc},
	year = {1986}
}

@misc{finley_github_2011-1,
	title = {Github {Has} {Surpassed} {Sourceforge} and {Google} {Code} in {Popularity}},
	url = {http://readwrite.com/2011/06/02/github-has-passed-sourceforge},
	abstract = {Github is now the most popular open source forge, having surpassed Sourceforge, Google Code and Microsoft's CodePlex in total number of commits for the period of January to May 2011, according to data released today by Black Duck Software. This should probably come as no surprise, but it's good to have data to back assumptions.

During the\&hellip;},
	urldate = {2014-06-30},
	journal = {ReadWrite},
	author = {Finley, Klint},
	month = jun,
	year = {2011},
	keywords = {hack, News},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8BCMTR6E\\github-has-passed-sourceforge.html:text/html}
}

@inproceedings{russo_proposed_2014,
	address = {New York, NY, USA},
	series = {{PROMISE} '14},
	title = {A {Proposed} {Method} to {Evaluate} and {Compare} {Fault} {Predictions} {Across} {Studies}},
	isbn = {978-1-4503-2898-2},
	url = {http://doi.acm.org/10.1145/2639490.2639504},
	doi = {10.1145/2639490.2639504},
	abstract = {Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework.},
	urldate = {2015-01-05},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Predictive} {Models} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Russo, Barbara},
	year = {2014},
	keywords = {confusion matrix, fault, Machine learning, model comparison},
	pages = {2--11}
}

@incollection{olson_data_2008,
	title = {Data {Mining} {Process}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_2},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {9--35},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\79V8FPDH\\978-3-540-76917-0_2.html:text/html}
}

@inproceedings{girba_yesterdays_2004,
	title = {Yesterday"s {Weather}: {Guiding} {Early} {Reverse} {Engineering} {Efforts} by {Summarizing} the {Evolution} of {Changes}.},
	isbn = {0-7695-2213-0},
	url = {http://dblp.uni-trier.de/db/conf/icsm/icsm2004.html#GirbaDL04},
	booktitle = {{ICSM}},
	publisher = {IEEE Computer Society},
	author = {Gîrba, Tudor and Ducasse, Stéphane and Lanza, Michele},
	month = oct,
	year = {2004},
	keywords = {dblp},
	pages = {40--49}
}

@incollection{dambros_analysing_2008,
	title = {Analysing {Software} {Repositories} to {Understand} {Software} {Evolution}},
	copyright = {©2008 Springer-Verlag},
	isbn = {978-3-540-76439-7, 978-3-540-76440-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76440-3_3},
	abstract = {Software repositories such as versioning systems, defect tracking systems, and archived communication between project personnel are used to help manage the progress of software projects. Software practitioners and researchers increasingly recognize the potential benefit of mining this information to support the maintenance of software systems, improve software design or reuse, and empirically validate novel ideas and techniques. Research is now proceeding to uncover ways in which mining these repositories can help to understand software development, to support predictions about software development, and to plan various evolutionary aspects of software projects. This chapter presents several analysis and visualization techniques to understand software evolution by exploiting the rich sources of artifacts that are available. Based on the data models that need to be developed to cover sources such as modification and bug reports we describe how to use a Release History Database for evolution analysis. For that we present approaches to analyse developer effort for particular software entities. Further we present change coupling analyses that can reveal hidden change dependencies among software entities. Finally, we show how to investigate architectural shortcomings over many releases and to identify trends in the evolution. Kiviat graphs can be effectively used to visualize such analysis results.},
	language = {en},
	urldate = {2014-03-12},
	booktitle = {Software {Evolution}},
	publisher = {Springer Berlin Heidelberg},
	author = {D’Ambros, Marco and Gall, Harald and Lanza, Michele and Pinzger, Martin},
	month = jan,
	year = {2008},
	keywords = {Information Systems Applications (incl.Internet), Management of Computing and Information Systems, Software engineering},
	pages = {37--67},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JD2W6WJ8\\D’Ambros et al. - 2008 - Analysing Software Repositories to Understand Soft.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BKMRHT8J\\10.html:text/html}
}

@article{chaikalis_investigating_????-2,
	title = {Investigating the effect of evolution and refactorings on feature scattering},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/article/10.1007/s11219-013-9204-4},
	doi = {10.1007/s11219-013-9204-4},
	abstract = {The implementation of a functional requirement is often distributed across several modules posing difficulties to software maintenance. In this paper, we attempt to quantify the extent of feature scattering and study its evolution with the passage of software versions. To this end, we trace the classes and methods involved in the implementation of a feature, apply formal approaches for studying variations across versions, measure whether feature implementation is uniformly distributed and visualize the reuse among features. Moreover, we investigate the impact of refactoring application on feature scattering in order to assess the circumstances under which a refactoring might improve the distribution of methods implementing a feature. The proposed techniques are exemplified for various features on several versions of four open-source projects.},
	language = {en},
	urldate = {2014-04-18},
	journal = {Software Qual J},
	author = {Chaikalis, Theodore and Chatzigeorgiou, Alexander and Examiliotou, Georgina},
	keywords = {Data Structures, Cryptology and Information Theory, Feature identification, Feature scattering, Operating Systems, Programming Languages, Compilers, Interpreters, Program understanding, Refactorings, Requirements traceability, Software Engineering/Programming and Operating Systems, Software evolution},
	pages = {1--27},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\4CQ62GAU\\Chaikalis et al. - Investigating the effect of evolution and refactor.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\988I2VEH\\Chaikalis et al. - Investigating the effect of evolution and refactor.html:text/html}
}

@article{ampatzoglou_research_2013,
	title = {Research {State} of the {Art} on {GoF} {Design} {Patterns}: {A} {Mapping} {Study}},
	volume = {86},
	issn = {0164-1212},
	shorttitle = {Research {State} of the {Art} on {GoF} {Design} {Patterns}},
	url = {http://dx.doi.org/10.1016/j.jss.2013.03.063},
	doi = {10.1016/j.jss.2013.03.063},
	abstract = {Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful.},
	number = {7},
	urldate = {2014-03-26},
	journal = {J. Syst. Softw.},
	author = {Ampatzoglou, Apostolos and Charalampidou, Sofia and Stamelos, Ioannis},
	month = jul,
	year = {2013},
	keywords = {Design patterns, Mapping study, Software quality attributes},
	pages = {1945--1964}
}

@article{turnu_fractal_2013-1,
	series = {Statistics with {Imperfect} {Data}},
	title = {The fractal dimension of software networks as a global quality metric},
	volume = {245},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025513003885},
	doi = {10.1016/j.ins.2013.05.014},
	abstract = {We analyzed the source code of various releases of two large Object Oriented Open Source Java software systems, Eclipse and Netbeans, investigating the complexity of the whole release and of its subprojects. We show that when the classes in the source code and the dependencies between them are considered, such systems can be viewed as complex software networks, and emerging structures, characteristic of fractals, appear at different length scales – on the entire systems and on subprojects of any size.

We were able to find in all examined cases a scaling region where it is possible to compute a self-similar coefficient, the fractal dimension, using “the box counting method”. Such a coefficient is a single metric related to the system’s complexity.

More importantly, we were able to show that this measure looks fairly related to software quality, acting as a global quality software metric. In particular, we computed the defects of each software system, and we found a clear correlation among the number of defects in the system, or in a subproject, and its fractal dimension. This correlation exists across all the subprojects and also along the time evolution of the software systems, as new releases are delivered.},
	urldate = {2014-08-22},
	journal = {Information Sciences},
	author = {Turnu, I. and Concas, G. and Marchesi, M. and Tonelli, R.},
	month = oct,
	year = {2013},
	keywords = {Fractal dimension, Object-oriented languages, Software engineering, software metrics, Software quality},
	pages = {290--303},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\63KXAF4G\\S0020025513003885.html:text/html}
}

@inproceedings{zibran_useful_2011,
	title = {Useful, {But} {Usable}? {Factors} {Affecting} the {Usability} of {APIs}},
	shorttitle = {Useful, {But} {Usable}?},
	doi = {10.1109/WCRE.2011.26},
	abstract = {Software development today has been largely dependent on the use of API libraries, frameworks, and reusable components. However, the API usability issues often increase the development cost (e.g., time, effort) and lower code quality. In this regard, we study 1,513 bug-posts across five different bug repositories, using both qualitative and quantitative analysis. We identify the API usability issues that are reflected in the bug-posts from the API users, and distinguish relative significance of the usability factors. Moreover, from the lessons learned by manual investigation of the bug-posts, we provide further insight into the most frequent API usability issues.},
	booktitle = {2011 18th {Working} {Conference} on {Reverse} {Engineering} ({WCRE})},
	author = {Zibran, M.F. and Eishita, F.Z. and Roy, C.K.},
	month = oct,
	year = {2011},
	keywords = {API, API library, API usability, application program interfaces, Application Programming Interface, bug post, bug repository, Complexity theory, Documentation, Libraries, lower code quality, Memory management, Open source software, Production facilities, program debugging, qualitative analysis, quantitative analysis, reusable components, software cost estimation, software development, software libraries, software reusability, Usability},
	pages = {151--155}
}

@misc{__????,
	title = {Έλεως {Tο} {Official} {Fan} {Club} της Ελεωνόρας Ζουγανέλη},
	url = {http://eleonora-zouganeli.blogspot.gr/},
	abstract = {Έλεως Tο Official Fan Club της Ελεωνόρας Ζουγανέλη},
	urldate = {2014-03-29},
	keywords = {eleonora, elews, epimonh, fan club zouganeli, metakomisi tora, monaxa ego, official fan club, official fan club elews, zouganeli, zoyganeli, ελεωνορα, ελεως, επισημο φαν κλαμπ, ζουγανελη},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\HT23ZWXT\\eleonora-zouganeli.blogspot.gr.html:text/html}
}

@article{cai_analysis_2008,
	title = {An analysis of research topics in software engineering - 2006},
	volume = {81},
	doi = {10.1016/j.jss.2007.08.028},
	abstract = {This paper is the first in a new annual series whose goal is to answer the following question: what are the active research focuses within the field of software engineering? We considered 7 top journals and 7 top international conferences in software engineering and examined all the 691 papers published in these journals or presented at these conferences in 2006. Consequently, we have a number of findings.(1)Seventy-three percent of journal papers focus on 20\% of subject indexes in software engineering, including Testing and Debugging (D.2.5), Management (D.2.9), and Software/Program Verification (D.2.4).(2)Eighty-nine percent of conference papers focus on 20\% of subject indexes in software engineering, including Software/Program Verification (D.2.4), Testing and Debugging (D.2.5), and Design Tools and Techniques (D.2.2).(3)Seventy-seven percent of journal/conference papers focus on 20\% of subject indexes in software engineering, including Testing and Debugging (D.2.5), Software/Program Verification (D.2.4), and Management (D.2.9).(4)The average number of references cited by a journal paper is about 33, whereas this number becomes around 24 for a conference paper.},
	number = {6},
	journal = {Journal of Systems and Software},
	author = {Cai, Kai-yuan and Card, David},
	year = {2008},
	keywords = {Design Tool, Indexation, program verification, Software engineering, Testing and Debugging},
	pages = {1051--1058}
}

@article{ma_network_2007,
	series = {B},
	title = {Network {Motifs} in {Object}-{Oriented} {Software} {Systems}},
	volume = {14},
	url = {http://arxiv.org/abs/0808.3292},
	abstract = {Nowadays, software has become a complex piece of work that may be beyond our control. Understanding how software evolves over time plays an important role in controlling software development processes. Recently, a few researchers found the quantitative evidence of structural duplication in software systems or web applications, which is similar to the evolutionary trend found in biological systems. To investigate the principles or rules of software evolution, we introduce the relevant theories and methods of complex networks into structural evolution and change of software systems. According to the results of our experiment on network motifs, we find that the stability of a motif shows positive correlation with its abundance and a motif with high Z score tends to have stable structure. These findings imply that the evolution of software systems is based on functional cloning as well as structural duplication and tends to be structurally stable. So, the work presented in this paper will be useful for the analysis of structural changes of software systems in reverse engineering.},
	urldate = {2015-01-10},
	journal = {Dynamics of Continuous, Discrete and Impulsive Systems},
	author = {Ma, Yutao and He, Keqing and Liu, Jing},
	year = {2007},
	note = {arXiv: 0808.3292},
	keywords = {Computer Science - Software Engineering, D.2.8, K.6.3},
	pages = {166--172},
	annote = {Comment: 7 pages, 4 figures, 1 table, the revised version has been published by DCDIS-B special issue on software engineering and complex networks},
	file = {arXiv\:0808.3292 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\VHK3IJIX\\Ma et al. - 2008 - Network Motifs in Object-Oriented Software Systems.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\V2CZI85S\\0808.html:text/html}
}

@article{simons_comparison_2013,
	title = {A comparison of meta-heuristic search for interactive software design},
	volume = {17},
	issn = {1432-7643, 1433-7479},
	url = {http://link.springer.com/article/10.1007/s00500-013-1039-1},
	doi = {10.1007/s00500-013-1039-1},
	abstract = {Advances in processing capacity, coupled with the desire to tackle problems where a human subjective judgment plays an important role in determining the value of a proposed solution, has led to a dramatic rise in the number of applications of Interactive Artificial Intelligence. Of particular note is the coupling of meta-heuristic search engines with user-provided evaluation and rating of solutions, usually in the form of Interactive Evolutionary Algorithms (IEAs). These have a well-documented history of successes, but arguably the preponderance of IEAs stems from this history, rather than as a conscious design choice of meta-heuristic based on the characteristics of the problem at hand. This paper sets out to examine the basis for that assumption, taking as a case study the domain of interactive software design. We consider a range of factors that should affect the design choice including ease of use, scalability, and of course, performance, i.e. that ability to generate good solutions within the limited number of evaluations available in interactive work before humans lose focus. We then evaluate three methods, namely greedy local search, an evolutionary algorithm and ant colony optimization (ACO), with a variety of representations for candidate solutions. Results show that after suitable parameter tuning, ACO is highly effective within interactive search and out-performs evolutionary algorithms with respect to increasing numbers of attributes and methods in the software design problem. However, when larger numbers of classes are present in the software design, an evolutionary algorithm using a naïve grouping integer-based representation appears more scalable.},
	language = {en},
	number = {11},
	urldate = {2015-05-18},
	journal = {Soft Comput},
	author = {Simons, C. L. and Smith, J. E.},
	month = apr,
	year = {2013},
	keywords = {Artificial Intelligence (incl. Robotics), Computational Intelligence, Control, Robotics, Mechatronics, Interactive search, Mathematical Logic and Foundations, Meta-heuristics, search-based software engineering, Software design},
	pages = {2147--2162},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\D5XR5CNG\\Simons and Smith - 2013 - A comparison of meta-heuristic search for interact.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8TFBWXPN\\s00500-013-1039-1.html:text/html}
}

@incollection{hochstein_chapter_2008,
	series = {Software {Development}},
	title = {Chapter 5 {An} {Environment} for {Conducting} {Families} of {Software} {Engineering} {Experiments}},
	volume = {74},
	url = {http://www.sciencedirect.com/science/article/pii/S0065245808006050},
	abstract = {The classroom is a valuable resource for conducting software engineering experiments. However, coordinating a family of experiments in classroom environments presents a number of challenges to researchers. Understanding how to run such experiments, developing procedures to collect accurate data, and collecting data that is consistent across multiple studies are major problems. This paper describes an environment, the Experiment Manager that simplifies the process of collecting, managing, and sanitizing data from classroom experiments, while minimizing disruption to natural subject behavior. We have successfully used this environment to study the impact of parallel programming languages in the high‐performance computing domain on programmer productivity at multiple universities across the United States.},
	urldate = {2015-01-05},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Hochstein, Lorin and Nakamura, Taiga and Shull, Forrest and Zazworka, Nico and Basili, Victor R. and Zelkowitz, Marvin V.},
	editor = {Zelkowitz, Marvin V.},
	year = {2008},
	pages = {175--200},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\3IT9A59U\\S0065245808006050.html:text/html}
}

@incollection{basili_goal_2002,
	title = {Goal {Question} {Metric} ({GQM}) {Approach}},
	copyright = {Copyright © 2002 by John Wiley \& Sons, Inc. All rights reserved.},
	isbn = {9780471028956},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/0471028959.sof142/abstract},
	abstract = {As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement supports creating a corporate memory and is an aid in answering a variety of questions associated with the enactment of any software process. Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action.According to many studies made on the application of metrics and models in industrial environments, measurement in order to be effective must be. * Focused on specific goals * Applied to all life-cycle products, processes, and resources * Interpreted on the basis of characterization and understanding of the organizational context, environment, and goals This means that measurement must be defined in a top-down fashion. It must be focused, based on goals and models. A metric-driven, bottom-up approach, will not work because there are many observable characteristics in software (e.g., time, number of defects, complexity, lines of code, severity of failures, effort, productivity, defect density). A context specific selection of metrics and guidelines on how to use and interpret them should be made, based on the appropriate models and goals of that environment.The most common and popular mechanism for goal-oriented software measurement is the Goal Question Metric approach which is presented in this article in combination with examples from GQM application in industry},
	language = {en},
	urldate = {2014-01-20},
	booktitle = {Encyclopedia of {Software} {Engineering}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Basili, Vic and Caldiera, Gianluigi and Rombach, H. Dieter},
	year = {2002},
	keywords = {industrial application, measurable software, phases, quality improvement},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\46T2PZCB\\abstract\;jsessionid=4786A2F347623F88B83D631CAA27FD0A.html:text/html}
}

@article{fortuna_evolution_2011,
	title = {Evolution of a modular software network},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2011/11/14/1115960108},
	doi = {10.1073/pnas.1115960108},
	abstract = {“Evolution behaves like a tinkerer” (François Jacob, Science, 1977). Software systems provide a singular opportunity to understand biological processes using concepts from network theory. The Debian GNU/Linux operating system allows us to explore the evolution of a complex network in a unique way. The modular design detected during its growth is based on the reuse of existing code in order to minimize costs during programming. The increase of modularity experienced by the system over time has not counterbalanced the increase in incompatibilities between software packages within modules. This negative effect is far from being a failure of design. A random process of package installation shows that the higher the modularity, the larger the fraction of packages working properly in a local computer. The decrease in the relative number of conflicts between packages from different modules avoids a failure in the functionality of one package spreading throughout the entire system. Some potential analogies with the evolutionary and ecological processes determining the structure of ecological networks of interacting species are discussed.},
	language = {en},
	urldate = {2014-01-30},
	journal = {PNAS},
	author = {Fortuna, Miguel A. and Bonachela, Juan A. and Levin, Simon A.},
	month = nov,
	year = {2011},
	pmid = {22106260},
	keywords = {community assembly, evolvability, food webs, network evolution, robustness},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NHT7ZS9X\\Fortuna et al. - 2011 - Evolution of a modular software network.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KUBUPMHP\\1115960108.html:text/html}
}

@article{li_modular_2013,
	title = {A modular attachment mechanism for software network evolution},
	volume = {392},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437113000708},
	doi = {10.1016/j.physa.2013.01.035},
	abstract = {A modular attachment mechanism of software network evolution is presented in this paper. Compared with the previous models, our treatment of object-oriented software system as a network of modularity is inherently more realistic. To acquire incoming and outgoing links in directed networks when new nodes attach to the existing network, a new definition of asymmetric probabilities is given. Based on this, modular attachment instead of single node attachment in the previous models is then adopted. The proposed mechanism is demonstrated to be able to generate networks with features of power-law, small-world, and modularity, which represents more realistic properties of actual software networks. This work therefore contributes to a more accurate understanding of the evolutionary mechanism of software systems. What is more, explorations of the effects of various software development principles on the structure of software systems have been carried out, which are expected to be beneficial to the software engineering practices.},
	number = {9},
	urldate = {2014-01-30},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Li, Hui and Zhao, Hai and Cai, Wei and Xu, Jiu-Qiang and Ai, Jun},
	month = may,
	year = {2013},
	keywords = {Asymmetric probabilities, Evolutionary mechanisms, Modular attachment, Power-law, Software networks},
	pages = {2025--2037},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WQCIX8UZ\\S0378437113000708.html:text/html}
}

@article{jun_quasi-optimal_2001,
	title = {Quasi-optimal case-selective neural network model for software effort estimation},
	volume = {21},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417401000215},
	doi = {10.1016/S0957-4174(01)00021-5},
	abstract = {A number of software effort estimations have attempted using statistical models, case based reasoning, and neural networks. The research results showed that the neural network models perform at least as well as the other approaches, so we selected the neural network model as the estimator. However, since the computing environment changes so rapidly in terms of programming languages, development tools, and methodologies, it is very difficult to maintain the performance of estimation models for the new breed of projects. Therefore, we propose a search method that finds the right level of relevant cases for the neural network model. For the selected case set, the scale of the neural network model can be reduced by eliminating the qualitative input factors with the same values. Since there exist a multitude of combinations of case sets, we need to search for the optimal reduced neural network model and corresponding case set. To find the quasi-optimal model from the hierarchy of reduced neural network models, we adopted the beam search technique and devised the case-set selection algorithm. We have shown that the resulting model significantly outperforms the original full model for the software effort estimation. This approach can be also used for building any case-selective neural network.},
	number = {1},
	urldate = {2014-08-21},
	journal = {Expert Systems with Applications},
	author = {Jun, Eung Sup and Lee, Jae Kyu},
	month = jul,
	year = {2001},
	keywords = {Beam search, Case set hierarchy, Case-set selection algorithm, Reduced neural network model, Sensitivity of beam width, software effort estimation},
	pages = {1--14},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QR2DE4R8\\Jun and Lee - 2001 - Quasi-optimal case-selective neural network model .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\E7V388ZF\\S0957417401000215.html:text/html}
}

@article{kitchenham_systematic_2009,
	title = {Systematic literature reviews in software engineering – {A} systematic literature review},
	volume = {51},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584908001390},
	doi = {10.1016/j.infsof.2008.09.009},
	abstract = {Background
In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.
Aims
This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.
Method
We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results
Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.
Conclusions
Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.},
	number = {1},
	urldate = {2014-01-20},
	journal = {Information and Software Technology},
	author = {Kitchenham, Barbara and Pearl Brereton, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	month = jan,
	year = {2009},
	keywords = {Cost estimation, Evidence-based software engineering, Systematic literature review, Systematic review quality, Tertiary study},
	pages = {7--15},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5SFSPQCI\\S0950584908001390.html:text/html}
}

@article{tsantalis_predicting_2005,
	title = {Predicting the probability of change in object-oriented systems},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.83},
	abstract = {Of all merits of the object-oriented paradigm, flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches. However, it is rather difficult to quantify this aspect of quality: this paper describes a probabilistic approach to estimate the change proneness of an object-oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. The extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible "saturation" level beyond which any attempt to improve the design without major refactoring is impossible. The proposed model has been evaluated on two multiversion open source projects. The process has been fully automated by a Java program, while statistical analysis has proved improved correlation between the extracted probabilities and actual changes in each of the classes in comparison to a prediction model that relies simply on past data.},
	number = {7},
	journal = {IEEE Transactions on Software Engineering},
	author = {Tsantalis, N. and Chatzigeorgiou, A and Stephanides, G.},
	month = jul,
	year = {2005},
	keywords = {Computer Society, configuration management, data mining, Design methodology, Index Terms- Object-oriented programming, Java, Java program, multiversion open source project, object-oriented design method, object-oriented design methods, object-oriented methods, Object oriented modeling, Object oriented programming, object-oriented programming, object-oriented system, Predictive models, Probability, probability prediction model, product metrics, public domain software, quality analysis and evaluation., software flexibility, software maintenance, software metrics, Software quality, Stability, Statistical analysis},
	pages = {601--614},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\G69UHWZ4\\abs_all.html:text/html}
}

@article{fagin_comparing_2003-1,
	title = {Comparing {Top} k {Lists}.},
	volume = {17},
	url = {http://dblp.uni-trier.de/db/journals/siamdm/siamdm17.html#FaginKS03},
	number = {1},
	journal = {SIAM J. Discrete Math.},
	author = {Fagin, Ronald and Kumar, Ravi and Sivakumar, D.},
	month = nov,
	year = {2003},
	keywords = {dblp},
	pages = {134--160}
}

@misc{_sonarqube_????-1,
	title = {{SonarQube}™ » {Features}},
	url = {http://www.sonarqube.org/features/},
	urldate = {2014-06-24},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IGS94HUI\\features.html:text/html}
}

@article{gomez-perez_evaluation_2001,
	title = {Evaluation of {Taxonomic} {Knowledge} in {Ontologies} and {Knowledge} {Bases}},
	author = {Gómez-Pérez, A.},
	year = {2001},
	file = {:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CV4QMZVM\\50426323_Evaluation_of_Taxonomic_Knowledge_in_Ontologies_and_Knowledge_Bases.html:text/html}
}

@incollection{olson_genetic_2008,
	title = {Genetic {Algorithm} {Support} to {Data} {Mining}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_8},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {125--135},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8I33AV88\\978-3-540-76917-0_8.html:text/html}
}

@article{basili_paradigms_1991,
	title = {Paradigms for experimentation and empirical studies in software engineering},
	volume = {32},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/095183209190053A},
	doi = {10.1016/0951-8320(91)90053-A},
	abstract = {The software engineering field requires major advances in order to attain the high standards of quality and productivity that are needed by the complex systems of the future. The immaturity of the field is reflected by the fact that most of its technologies have not yet been analyzed to determine their effects on quality and productivity. Moreover, when these analyses have occured the resulting guidance is not quantitative but only ethereal. One fundamental area of software engineering that is just beginning to blossom is the use of measurement techniques and empirical methods. These techniques need to be adopted by software researchers and practitioners in order to help the field respond to the demands being placed upon it. This paper outlines four paradigms for experimentation and empirical study in software engineering and describes their interrelationships: (1) Improvement paradigm (2) Goal-question-metric paradigm, (3) Experimentation framework paradigm, and (4) Classification paradigm. These paradigms are intended to catalyze the use of measurement techniques and empirical methods in software engineering.},
	number = {1–2},
	urldate = {2014-12-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Basili, Victor R. and Selby, Richard W.},
	year = {1991},
	pages = {171--191},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\F5DTDRU2\\Basili and Selby - 1991 - Paradigms for experimentation and empirical studie.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DPGAWG4T\\095183209190053A.html:text/html}
}

@article{dig_how_2006,
	title = {How do {APIs} evolve? {A} story of refactoring},
	volume = {18},
	copyright = {Copyright © 2006 John Wiley \& Sons, Ltd.},
	issn = {1532-0618},
	shorttitle = {How do {APIs} evolve?},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.328/abstract},
	doi = {10.1002/smr.328},
	abstract = {Frameworks and libraries change their APIs. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and ideas have been proposed to solve the evolution of APIs, most updates are done manually. To better understand the requirements for migration tools, we studied the API changes of four frameworks and one library. We discovered that the changes that break existing applications are not random, but tend to fall into particular categories. Over 80\% of these changes are refactorings. This suggests that refactoring-based migration tools should be used to update applications. Copyright © 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {2},
	urldate = {2015-03-30},
	journal = {J. Softw. Maint. Evol.: Res. Pract.},
	author = {Dig, Danny and Johnson, Ralph},
	month = mar,
	year = {2006},
	keywords = {API evolution, backwards compatibility, component reuse, frameworks, Libraries, refactoring},
	pages = {83--107},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8IEP4TD8\\Dig και Johnson - 2006 - How do APIs evolve A story of refactoring.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TJZTH6MN\\abstract.html:text/html}
}

@book{martin_agile_2003-2,
	address = {Upper Saddle River, N.J.},
	title = {Agile software development: principles, patterns, and practices},
	isbn = {0135974445  9780135974445  9780132760584  0132760584},
	shorttitle = {Agile software development},
	abstract = {This comprehensive, pragmatic tutorial on Agile Development and eXtreme programming, written by one of the founding fathers of Agile Development: Teaches software developers and project managers how to get projects done on time, and on budget using the power of Agile Development; Uses real-world case studies to show how to of plan, test, refactor, and pair program using eXtreme programming; Contains a wealth of reusable C++ and Java code; Focuses on solving customer oriented systems problems using UML and Design Patterns.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Martin, Robert C},
	year = {2003}
}

@article{gousios_conducting_2014,
	title = {Conducting quantitative software engineering studies with {Alitheia} {Core}},
	volume = {19},
	issn = {1382-3256},
	url = {/pub/conducting-quantitative-softeng-studies-alitheia-core.pdf},
	doi = {10.1007/s10664-013-9242-3},
	number = {4},
	journal = {Empirical Software Engineering},
	author = {Gousios, Georgios and Spinellis, Diomidis},
	year = {2014},
	keywords = {Quantitative software engineering, Software repository mining},
	pages = {885--925}
}

@article{coleman_using_1994,
	title = {Using metrics to evaluate software system maintainability},
	volume = {27},
	issn = {0018-9162},
	doi = {10.1109/2.303623},
	abstract = {Software metrics have been much criticized in the last few years, sometimes justly but more often unjustly, because critics misunderstand the intent behind the technology. Software complexity metrics, for example, rarely measure the "inherent complexity" embedded in software systems, but they do a very good job of comparing the relative complexity of one portion of a system with another. In essence, they are good modeling tools. Whether they are also good measuring tools depends on how consistently and appropriately they are applied.{\textless}{\textgreater}},
	number = {8},
	journal = {Computer},
	author = {Coleman, D. and Ash, D. and Lowther, B. and Oman, P.},
	month = aug,
	year = {1994},
	keywords = {complexity metrics, computational complexity, Computer industry, consistent application, Costs, Embedded software, inherent complexity, Large-scale systems, measuring tools, modeling tools, relative complexity, Software engineering, software maintenance, Software measurement, software metrics, Software quality, software system maintainability evaluation, Software systems},
	pages = {44--49},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\EPXX8GMM\\freeabs_all.html:text/html}
}

@book{weis_predicting_????,
	title = {Predicting {Effort} to {Fix} {Software} {Bugs}},
	abstract = {Predicting the time and effort for a software problem has long been a difficult task. We present an approach that predicts the fixing effort for an issue. Our technique leverages existing issue tracking systems: given a new issue report, we search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach on the JBoss project data and found that we can estimate within ±7 hours of the actual effort. 1.},
	author = {Weiß, Cathrin and Premraj, Rahul and Zimmermann, Thomas and Zeller, Andreas},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\WSC2EVHU\\Weiß et al. - Predicting Effort to Fix Software Bugs.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\XBJDXJT6\\summary.html:text/html}
}

@article{willinger_mathematics_2009-1,
	title = {Mathematics and the {Internet}: {A} {Source} of {Enormous} {Confusion} and {Great} {Potential}},
	volume = {56},
	number = {5},
	journal = {Notices of the American Mathematical Society},
	author = {Willinger, Walter and Alderson, David and Doyle, John C.},
	year = {2009},
	pages = {586--599}
}

@inproceedings{chatzigeorgiou_application_2006,
	address = {New York, NY, USA},
	series = {{WISER} '06},
	title = {Application of {Graph} {Theory} to {OO} {Software} {Engineering}},
	isbn = {1-59593-409-X},
	url = {http://doi.acm.org/10.1145/1137661.1137669},
	doi = {10.1145/1137661.1137669},
	abstract = {Graph Theory, which studies the properties of graphs, has been widely accepted as a core subject in the knowledge of computer scientists. So is Object-Oriented (OO) software engineering, which deals with the analysis, design and implementation of systems employing classes as modules. The latter field can greatly benefit from the application of Graph Theory, since the main mode of representation, namely the class diagram, is essentially a directed graph. The study of graph properties can be valuable in many ways for understanding the characteristics of the underlying software systems. Representative examples for the usefulness of graph theory on OO systems based on recent research results are presented in this paper.},
	urldate = {2014-01-30},
	booktitle = {Proceedings of the 2006 {International} {Workshop} on {Workshop} on {Interdisciplinary} {Software} {Engineering} {Research}},
	publisher = {ACM},
	author = {Chatzigeorgiou, Alexander and Tsantalis, Nikolaos and Stephanides, George},
	year = {2006},
	keywords = {clustering, design pattern detection, "God" classes, graph theory, scale-free},
	pages = {29--36}
}

@article{ivarsson_method_2011,
	title = {A method for evaluating rigor and industrial relevance of technology evaluations},
	volume = {16},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-010-9146-4},
	doi = {10.1007/s10664-010-9146-4},
	abstract = {One of the main goals of an applied research field such as software engineering is the transfer and widespread use of research results in industry. To impact industry, researchers developing technologies in academia need to provide tangible evidence of the advantages of using them. This can be done trough step-wise validation, enabling researchers to gradually test and evaluate technologies to finally try them in real settings with real users and applications. The evidence obtained, together with detailed information on how the validation was conducted, offers rich decision support material for industry practitioners seeking to adopt new technologies and researchers looking for an empirical basis on which to build new or refined technologies. This paper presents model for evaluating the rigor and industrial relevance of technology evaluations in software engineering. The model is applied and validated in a comprehensive systematic literature review of evaluations of requirements engineering technologies published in software engineering journals. The aim is to show the applicability of the model and to characterize how evaluations are carried out and reported to evaluate the state-of-research. The review shows that the model can be applied to characterize evaluations in requirements engineering. The findings from applying the model also show that the majority of technology evaluations in requirements engineering lack both industrial relevance and rigor. In addition, the research field does not show any improvements in terms of industrial relevance over time.},
	language = {en},
	number = {3},
	urldate = {2015-01-05},
	journal = {Empir Software Eng},
	author = {Ivarsson, Martin and Gorschek, Tony},
	month = jun,
	year = {2011},
	keywords = {Programming Languages, Compilers, Interpreters, Requirements engineering, Software Engineering/Programming and Operating Systems, Systematic review, Technology evaluation},
	pages = {365--395},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MEKPT22I\\s10664-010-9146-4.html:text/html}
}

@inproceedings{chaikalis_seagle:_2014,
	address = {Victoria, British Columbia, Canada},
	series = {Tool {Demonstration} {Track}},
	title = {Seagle: {Effortless} {Software} {Evolution} {Analysis}},
	author = {Chaikalis, Theodore and Melas, George and Ligu, Elivs and Chatzigeorgiou, Alexander},
	month = oct,
	year = {2014},
	pages = {581--584}
}

@article{kitchenham_case_1995,
	title = {Case {Studies} for {Method} and {Tool} {Evaluation}},
	volume = {12},
	issn = {0740-7459},
	url = {http://dx.doi.org/10.1109/52.391832},
	doi = {10.1109/52.391832},
	abstract = {The last decade has seen explosive growth in the number of software-engineering methods and tools, each one offering to improve some characteristic of software, its development, or its maintenance. With an increasing awareness of the competitive advantage to be gained from continuing process improvement, we all seek methods and tools that will make us more productive and improve the quality of our software. But how do we ensure that our changes lead to positive improvement? Suppose you have decided to evaluate a technology. How do you proceed? Do you do a survey? An experiment? A case study? In this article, we discuss the conditions under which each type of investigation is appropriate. Then, because good case studies are as rare as they are powerful and informative, we focus on how to do a proper and effective case study. Although they cannot achieve the scientific rigor of formal experiments, case studies can provide sufficient information to help you judge if specific technologies will benefit your own organization or project. Even when you cannot do a case study of your own, the principles of good case-study analysis will help you determine if the case-study results you read about are applicable to your situation. Good case studies involve specifying the hypothesis under test; using state variables for project selection and data analysis; establishing a basis for comparisons; planning case studies properly; and using appropriate presentation and analysis techniques to assess the results.},
	number = {4},
	urldate = {2014-12-17},
	journal = {IEEE Softw.},
	author = {Kitchenham, Barbara and Pickard, Lesley and Pfleeger, Shari Lawrence},
	month = jul,
	year = {1995},
	pages = {52--62}
}

@article{jorgensen_forecasting_2007-1,
	title = {Forecasting of software development work effort: {Evidence} on expert judgement and formal models},
	volume = {23},
	issn = {0169-2070},
	shorttitle = {Forecasting of software development work effort},
	url = {http://www.sciencedirect.com/science/article/pii/S016920700700074X},
	doi = {10.1016/j.ijforecast.2007.05.008},
	abstract = {The review presented in this paper examines the evidence on the use of expert judgement, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgement-based effort estimates was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgement-based effort estimates were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated effort estimates based on a combination of expert judgement and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other estimation methods.},
	number = {3},
	urldate = {2014-08-27},
	journal = {International Journal of Forecasting},
	author = {Jørgensen, Magne},
	month = jul,
	year = {2007},
	keywords = {Combining forecasts, Comparative studies, Evaluating forecasts, Forecasting practice, Judgemental forecasting},
	pages = {449--462},
	file = {ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\MXPPS7W6\\S016920700700074X.html:text/html}
}

@article{subelj_community_2011,
	title = {Community structure of complex software systems: {Analysis} and applications},
	volume = {390},
	issn = {0378-4371},
	shorttitle = {Community structure of complex software systems},
	url = {http://www.sciencedirect.com/science/article/pii/S037843711100269X},
	doi = {10.1016/j.physa.2011.03.036},
	abstract = {Due to notable discoveries in the fast evolving field of complex networks, recent research in software engineering has also focused on representing software systems with networks. Previous work has observed that these networks follow scale-free degree distributions and reveal small-world phenomena, while we here explore another property commonly found in different complex networks, i.e. community structure. We adopt class dependency networks, where nodes represent software classes and edges represent dependencies among them, and show that these networks reveal a significant community structure, characterized by similar properties as observed in other complex networks. However, although intuitive and anticipated by different phenomena, identified communities do not exactly correspond to software packages. We empirically confirm our observations on several networks constructed from Java and various third party libraries, and propose different applications of community detection to software engineering.},
	number = {16},
	urldate = {2014-07-10},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Šubelj, Lovro and Bajec, Marko},
	month = aug,
	year = {2011},
	keywords = {Community structure, Complex networks, Software systems},
	pages = {2968--2975},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\PJC588FF\\Šubelj and Bajec - 2011 - Community structure of complex software systems A.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\7P2JWQ7K\\S037843711100269X.html:text/html}
}

@misc{_geant_????,
	title = {The {Geant} {Project}},
	url = {http://www.geant.net/About/Pages/home.aspx}
}

@article{basili_validation_1996,
	title = {A validation of object-oriented design metrics as quality indicators},
	volume = {22},
	issn = {0098-5589},
	doi = {10.1109/32.544352},
	abstract = {This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than “traditional” code metrics, which can only be collected at a later phase of the software development processes},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Basili, V.R. and Briand, L.C. and Melo, W.L.},
	month = oct,
	year = {1996},
	keywords = {C++ programming language, C language, class maintenance changes, Computer languages, Costs, data set, Design methodology, fault-prone classes, information management systems, Information systems, metric validation, object oriented analysis, object-oriented design metrics, Object-oriented languages, object-oriented methods, Object oriented modeling, Predictive models, Programming, Resource management, sequential life cycle model, software development, software maintenance, software metrics, Software quality, software quality indicators, Software systems, System testing},
	pages = {751--761},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\STVUGRSZ\\login.html:text/html}
}

@book{martin_agile_2003-3,
	address = {Upper Saddle River, N.J.},
	title = {Agile software development: principles, patterns, and practices},
	isbn = {0135974445  9780135974445  9780132760584  0132760584},
	shorttitle = {Agile software development},
	abstract = {This comprehensive, pragmatic tutorial on Agile Development and eXtreme programming, written by one of the founding fathers of Agile Development: Teaches software developers and project managers how to get projects done on time, and on budget using the power of Agile Development; Uses real-world case studies to show how to of plan, test, refactor, and pair program using eXtreme programming; Contains a wealth of reusable C++ and Java code; Focuses on solving customer oriented systems problems using UML and Design Patterns.},
	language = {English},
	publisher = {Prentice Hall},
	author = {Martin, Robert C},
	year = {2003}
}

@misc{seanets__2013,
	url = {www.seanets.org},
	journal = {Software Evolution Analysis with Networks},
	author = {SEANets},
	month = oct,
	year = {2013}
}

@misc{freecol__2013,
	url = {http://www.freecol.org/},
	journal = {FreeCol - The Colonization of America},
	author = {FreeCol},
	month = oct,
	year = {2013}
}

@book{easley_networks_2010,
	address = {New York},
	title = {Networks, crowds, and markets reasoning about a highly connected world},
	isbn = {9780511776755  0511776756  0521195330  9780521195331  9780511761942  0511761945},
	abstract = {"Over the past decade there has been a growing public fascination with the complex connectedness of modern society. This connectedness is found in many incarnations: in the rapid growth of the Internet, in the ease with which global communication takes place, and in the ability of news and information as well as epidemics and financial crises to spread with surprising speed and intensity. These are phenomena that involve networks, incentives, and the aggregate behavior of groups of people; they are based on the links that connect us and the ways in which our decisions can have subtle consequences for others. This introductory undergraduate textbook takes an interdisciplinary look at economics, sociology, computing and information science, and applied mathematics to understand networks and behavior. It describes the emerging field of study that is growing at the interface of these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected"--Provided by publisher.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Easley, David and Kleinberg, Jon},
	year = {2010}
}

@article{lehman_programs_1980,
	title = {Programs, life cycles, and laws of software evolution},
	volume = {68},
	issn = {0018-9219},
	doi = {10.1109/PROC.1980.11805},
	abstract = {By classifying programs according to their relationship to the environment in which they are executed, the paper identifies the sources of evolutionary pressure on computer applications and programs and shows why this results in a process of never ending maintenance activity. The resultant life cycle processes are then briefly discussed. The paper then introduces laws of Program Evolution that have been formulated following quantitative studies of the evolution of a number of different systems. Finally an example is provided of the application of Evolution Dynamics models to program release planning.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Lehman, M.M.},
	month = sep,
	year = {1980},
	keywords = {Application software, Automatic programming, Computer applications, Economic indicators, Environmental economics, Fabrics, Helium, Microprocessors, Productivity, software maintenance},
	pages = {1060--1076},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ZAIMIEQP\\defdeny.html:text/html}
}

@inproceedings{zimmermann_predicting_2008-2,
	address = {New York, NY, USA},
	series = {{ICSE} '08},
	title = {Predicting {Defects} {Using} {Network} {Analysis} on {Dependency} {Graphs}},
	isbn = {978-1-60558-079-1},
	url = {http://doi.acm.org/10.1145/1368088.1368161},
	doi = {10.1145/1368088.1368161},
	abstract = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10\% points higher than for models built from complexity metrics. In addition, network measures could identify 60\% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.},
	urldate = {2014-02-24},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2008},
	keywords = {defect prediction, dependency graph, network analysis, windows server 2003},
	pages = {531--540},
	file = {ACM Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\D9K9S222\\Zimmermann and Nagappan - 2008 - Predicting Defects Using Network Analysis on Depen.pdf:application/pdf}
}

@incollection{olson_introduction_2008,
	title = {Introduction},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_1},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {3--8},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QIN4C6VB\\978-3-540-76917-0_1.html:text/html}
}

@inproceedings{palomba_detecting_2013,
	title = {Detecting bad smells in source code using change history information},
	doi = {10.1109/ASE.2013.6693086},
	abstract = {Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61\% and 80\%, and its recall ranges between 61\% and 100\%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.},
	booktitle = {2013 {IEEE}/{ACM} 28th {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Palomba, F. and Bavota, G. and Di Penta, M. and Oliveto, R. and De Lucia, A. and Poshyvanyk, D.},
	month = nov,
	year = {2013},
	keywords = {association rules, bad smells detection, blob, change history information, code elements, code smells, Detectors, divergent change, fault proneness, fault tolerant computing, feature envy, Feature extraction, HIST, Historical Information for Smell deTection, History, Java, Measurement, parallel inheritance, shotgun surgery, smell detectors, software maintenance, software management, software projects, Source code analysis, source code (software), structural information, Surgery, versioning systems},
	pages = {268--278},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\Q2WUVSFJ\\freeabs_all.html:text/html}
}

@inproceedings{keivanloo_linked_2012,
	title = {A {Linked} {Data} platform for mining software repositories},
	doi = {10.1109/MSR.2012.6224296},
	abstract = {The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.},
	booktitle = {2012 9th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Keivanloo, I. and Forbes, C. and Hmood, A. and Erfani, M. and Neal, C. and Peristerakis, G. and Rilling, J.},
	month = jun,
	year = {2012},
	keywords = {Cloning, code clones, collaborative platform, Communities, data mining, Encyclopedias, fact sharing, Licenses, Linked Data, linked data platform, mining software repositories, online software ecosystem linked data platform, on-the-fly inter-dataset integration, Ontologies, Software, software datasets, software licenses, software mining, software packages, software repositories, source code statements, value added information},
	pages = {32--35},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\79URSHEX\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\H5B5NRCV\\Keivanloo et al. - 2012 - A Linked Data platform for mining software reposit.pdf:application/pdf}
}

@article{sjoberg_quantifying_2013,
	title = {Quantifying the {Effect} of {Code} {Smells} on {Maintenance} {Effort}},
	volume = {39},
	issn = {0098-5589},
	doi = {10.1109/TSE.2012.89},
	abstract = {Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Sjoberg, D.I.K. and Yamashita, A. and Anda, B.C.D. and Mockus, A. and Dyba, T.},
	month = aug,
	year = {2013},
	keywords = {code churn, code size reduction, code smell effect quantification, code smell refactoring, Context, Eclipse IDE plug-in, Electronic mail, file properties, file size, Java, Java files, Java systems, Maintainability, maintainable code, maintenance effort, Maintenance engineering, maintenance tasks, object-oriented design, product metrics, refused bequest, regression analysis, Software, software maintenance, Surgery, Time measurement},
	pages = {1144--1156},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UXB38227\\freeabs_all.html:text/html}
}

@inproceedings{dambros_extensive_2010,
	title = {An extensive comparison of bug prediction approaches},
	doi = {10.1109/MSR.2010.5463279},
	abstract = {Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.},
	booktitle = {2010 7th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {D'Ambros, M. and Lanza, M. and Robbes, R.},
	month = may,
	year = {2010},
	keywords = {bug prediction approaches, Computer bugs, Computer science, Entropy, Informatics, Open source software, Power system modeling, Predictive models, program debugging, resource allocation, resource allocation problem, software defects, Software engineering, Software systems, Stability},
	pages = {31--41},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5PEIATJT\\login.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\BFSEXDQS\\D'Ambros et al. - 2010 - An extensive comparison of bug prediction approach.pdf:application/pdf}
}

@incollection{arcuri_parameter_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Parameter} {Tuning} in {Search} {Based} {Software} {Engineering}},
	copyright = {©2011 Springer-Verlag GmbH Berlin Heidelberg},
	isbn = {978-3-642-23715-7, 978-3-642-23716-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23716-4_6},
	abstract = {When applying search-based software engineering (SBSE) techniques one is confronted with a multitude of different parameters that need to be chosen: Which population size for a genetic algorithm? Which selection mechanism to use? What settings to use for dozens of other parameters? This problem not only troubles users who want to apply SBSE tools in practice, but also researchers performing experimentation – how to compare algorithms that can have different parameter settings? To shed light on the problem of parameters, we performed the largest empirical analysis on parameter tuning in SBSE to date, collecting and statistically analysing data from more than a million experiments. As case study, we chose test data generation, one of the most popular problems in SBSE. Our data confirm that tuning does have a critical impact on algorithmic performance, and over-fitting of parameter tuning is a dire threat to external validity of empirical analyses in SBSE. Based on this large empirical evidence, we give guidelines on how to handle parameter tuning.},
	language = {en},
	number = {6956},
	urldate = {2015-05-18},
	booktitle = {Search {Based} {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Arcuri, Andrea and Fraser, Gordon},
	editor = {Cohen, Myra B. and Cinnéide, Mel Ó},
	year = {2011},
	keywords = {Computation by Abstract Devices, Object-oriented, Operating Systems, Programming Techniques, Search based software engineering, Software engineering, test data generation, unit testing},
	pages = {33--47},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\QHQEP667\\Arcuri and Fraser - 2011 - On Parameter Tuning in Search Based Software Engin.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\NE8KFDMU\\978-3-642-23716-4_6.html:text/html}
}

@misc{maestrosnew_._2007-1,
	title = {Γ.Νταλάρας - Όταν σφίγγουν το χέρι (Θεοδωράκης - Ρίτσος)},
	url = {http://www.youtube.com/watch?v=Ml9vXQecaPE&feature=youtube_gdata_player},
	abstract = {Dalaras sings Otan sfiggoun to heri, a song from Romiosini at Megaron Mousikis Athinon. George Kimoulis is reading a text of Giannis Ritsos at the beggining. Romiosini by Mikis Theodorakis, Giannis Ritsos.},
	urldate = {2014-02-22},
	collaborator = {{maestrosnew}},
	month = may,
	year = {2007}
}

@incollection{olson_association_2008,
	title = {Association {Rules} in {Knowledge} {Discovery}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76916-3, 978-3-540-76917-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76917-0_4},
	urldate = {2014-01-20},
	booktitle = {Advanced {Data} {Mining} {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Olson, David L. and Delen, Dursun},
	month = jan,
	year = {2008},
	keywords = {Business Information Systems, Data Mining and Knowledge Discovery, Operations Research/Decision Theory},
	pages = {53--68},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\T5EE75FW\\978-3-540-76917-0_4.html:text/html}
}

@inproceedings{mcdonnell_empirical_2013-1,
	title = {An {Empirical} {Study} of {API} {Stability} and {Adoption} in the {Android} {Ecosystem}},
	doi = {10.1109/ICSM.2013.18},
	abstract = {When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28\% of API references in client applications are outdated with a median lagging time of 16 months. 22\% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.},
	booktitle = {2013 29th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {McDonnell, T. and Ray, B. and Kim, Miryung},
	month = sep,
	year = {2013},
	keywords = {Android API coevolution behavior, Android ecosystem, Androids, API evolution, API Stability, API usage adaptation code, application program interfaces, github, Google, History, Humanoid robots, Mobile communication, mobile computing, operating systems (computers), Smart phones, Software, software ecosystems, software maintenance, version history data},
	pages = {70--79},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\T55JFB4K\\freeabs_all.html:text/html}
}

@article{barabasi_emergence_1999,
	title = {Emergence of {Scaling} in {Random} {Networks}},
	volume = {286},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/286/5439/509},
	doi = {10.1126/science.286.5439.509},
	abstract = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
	language = {en},
	number = {5439},
	urldate = {2014-01-30},
	journal = {Science},
	author = {Barabási, Albert-László and Albert, Réka},
	month = oct,
	year = {1999},
	pmid = {10521342},
	pages = {509--512},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\KJ3TNB5T\\509.html:text/html}
}

@misc{_aws/aws-sdk-java_????,
	title = {aws/aws-sdk-java},
	url = {https://github.com/aws/aws-sdk-java},
	abstract = {aws-sdk-java - Official mirror of the AWS SDK for Java. For more information on the AWS SDK for Java, see our web site:},
	urldate = {2015-02-25},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\THNWA3PM\\aws-sdk-java.html:text/html}
}

@misc{_iso/iec/ieee_????,
	title = {{ISO}/{IEC}/{IEEE} 42010 {Homepage}},
	url = {http://www.iso-architecture.org/42010/},
	abstract = {ISO/IEC/IEEE 42010, Systems and software engineering
— Architecture description, is the international standard for
best practices in the description of the architectures of systems,
where systems range from software applications to enterprises to
systems of systems.},
	urldate = {2015-01-05},
	journal = {ISO/IEC/IEEE 42010 Homepage}
}

@article{lehman_programs_1980-1,
	title = {Programs, life cycles, and laws of software evolution},
	volume = {68},
	issn = {0018-9219},
	doi = {10.1109/PROC.1980.11805},
	abstract = {By classifying programs according to their relationship to the environment in which they are executed, the paper identifies the sources of evolutionary pressure on computer applications and programs and shows why this results in a process of never ending maintenance activity. The resultant life cycle processes are then briefly discussed. The paper then introduces laws of Program Evolution that have been formulated following quantitative studies of the evolution of a number of different systems. Finally an example is provided of the application of Evolution Dynamics models to program release planning.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Lehman, M.M.},
	month = sep,
	year = {1980},
	keywords = {Application software, Automatic programming, Computer applications, Economic indicators, Environmental economics, Fabrics, Helium, Microprocessors, Productivity, software maintenance},
	pages = {1060--1076},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\AWN8HHSB\\login.html:text/html}
}

@incollection{yazdi_analysis_2014-1,
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Analysis and {Prediction} of {Design} {Model} {Evolution} {Using} {Time} {Series}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-07868-7, 978-3-319-07869-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-07869-4_1},
	abstract = {Tools which support Model-Driven Engineering have to be evaluated and tested. In the domain of model differencing and model versioning, sequences of software models (model histories), in which a model is obtained from its immediate predecessor by some modification, are of special interest. Unfortunately, in this application domain adequate real test models are scarcely available and must be artificially created. To this end, model generators were proposed in recent years. Generally, such model generators should be configured in a way that the generated sequences of models are as realistic as possible, i.e. they should mimic the changes that happen in real software models. Hence, it is a necessary prerequisite to analyze and to stochastically model the evolution (changes) of real software systems at the abstraction level of models. In this paper, we present a new approach to statistically analyze the evolution of models. Our approach uses time series as a statistical method to capture the dynamics of the evolution. We applied this approach to several typical projects and we successfully modeled their evolutions. The time series models could predict the future changes of the next revisions of the systems with good accuracies. The obtained time series models are used to create more realistic model histories for model versioning and model differencing tools.},
	language = {en},
	number = {178},
	urldate = {2014-08-21},
	booktitle = {Advanced {Information} {Systems} {Engineering} {Workshops}},
	publisher = {Springer International Publishing},
	author = {Yazdi, Hamed Shariat and Mirbolouki, Mahnaz and Pietsch, Pit and Kehrer, Timo and Kelter, Udo},
	editor = {Iliadis, Lazaros and Papazoglou, Michael and Pohl, Klaus},
	month = jan,
	year = {2014},
	keywords = {Business Information Systems, Computer Appl. in Administrative Data Processing, Information Systems Applications (incl. Internet), Software engineering, Systems and Data Security},
	pages = {1--15},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RHPS4BUD\\978-3-319-07869-4_1.html:text/html}
}

@misc{presto__2014,
	url = {http://prestodb.io/},
	journal = {Presto {\textbar} Distributed SQL Query Engine for Big Data},
	author = {Presto},
	month = nov,
	year = {2014}
}

@article{concas_suitability_2006,
	title = {On the suitability of {Yule} process to stochastically model some properties of object-oriented systems},
	volume = {370},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437106002378},
	doi = {10.1016/j.physa.2006.02.024},
	abstract = {We present a study of three large object-oriented software systems—VisualWorks Smalltalk, Java JDK and Eclipse—searching for scaling laws in some of their properties. We study four system properties related to code production, namely the inheritance hierarchies, the naming of variables and methods, and the calls to methods. We systematically found power-law distributions in these properties, most of which have never been reported before. We were also able to statistically model the programming activities leading to the studied properties as Yule processes, with very good correspondence between empirical data and the prediction of Yule model. The fact that a design and optimization process like software development can be modeled on the large with the laws of statistical physics poses intriguing issues to software engineers, and could be exploited for finding new metrics and quality measures.},
	number = {2},
	urldate = {2014-07-10},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Concas, Giulio and Marchesi, Michele and Pinna, Sandro and Serra, Nicola},
	month = oct,
	year = {2006},
	keywords = {Java, Object-oriented languages, Power-laws, Smalltalk, Software engineering, Yule process},
	pages = {817--831},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GEWADWE7\\Concas et al. - 2006 - On the suitability of Yule process to stochastical.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\GJZFEZD8\\S0378437106002378.html:text/html}
}

@article{hosseini_influence_2013,
	title = {Influence of {Choice} of {Null} {Network} on {Small}-{World} {Parameters} of {Structural} {Correlation} {Networks}},
	volume = {8},
	url = {http://dx.doi.org/10.1371/journal.pone.0067354},
	doi = {10.1371/journal.pone.0067354},
	abstract = {In recent years, coordinated variations in brain morphology (e.g., volume, thickness) have been employed as a measure of structural association between brain regions to infer large-scale structural correlation networks. Recent evidence suggests that brain networks constructed in this manner are inherently more clustered than random networks of the same size and degree. Thus, null networks constructed by randomizing topology are not a good choice for benchmarking small-world parameters of these networks. In the present report, we investigated the influence of choice of null networks on small-world parameters of gray matter correlation networks in healthy individuals and survivors of acute lymphoblastic leukemia. Three types of null networks were studied: 1) networks constructed by topology randomization (TOP), 2) networks matched to the distributional properties of the observed covariance matrix (HQS), and 3) networks generated from correlation of randomized input data (COR). The results revealed that the choice of null network not only influences the estimated small-world parameters, it also influences the results of between-group differences in small-world parameters. In addition, at higher network densities, the choice of null network influences the direction of group differences in network measures. Our data suggest that the choice of null network is quite crucial for interpretation of group differences in small-world parameters of structural correlation networks. We argue that none of the available null models is perfect for estimation of small-world parameters for correlation networks and the relative strengths and weaknesses of the selected model should be carefully considered with respect to obtained network measures.},
	number = {6},
	urldate = {2014-09-02},
	journal = {PLoS ONE},
	author = {Hosseini, S. M. Hadi and Kesler, Shelli R.},
	month = jun,
	year = {2013},
	pages = {e67354},
	file = {PLoS Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\A44Q5GXW\\infodoi10.1371journal.pone.html:text/html}
}

@inproceedings{tsantalis_jdeodorant:_2008,
	title = {{JDeodorant}: {Identification} and {Removal} of {Type}-{Checking} {Bad} {Smells}},
	shorttitle = {{JDeodorant}},
	doi = {10.1109/CSMR.2008.4493342},
	abstract = {In this demonstration, we present an Eclipse plug-in that automatically identifies type-checking bad smells in Java source code, and resolves them by applying the "replace conditional with polymorphism" or "replace type code with state/strategy " refactorings. To the best of our knowledge there is a lack of tools that identify type-checking bad smells. Moreover, none of the state-of-the-art IDEs support the refactorings that resolve such kind of bad smells.},
	booktitle = {12th {European} {Conference} on {Software} {Maintenance} and {Reengineering}, 2008. {CSMR} 2008},
	author = {Tsantalis, N. and Chaikalis, T. and Chatzigeorgiou, A.},
	year = {2008},
	keywords = {Eclipse plug-in, Employment, Gettering, Informatics, Java, Java source code, JDeodorant, Object oriented programming, polymorphism, Programming profession, Refactorings, Runtime, software maintenance, Software quality, source coding, Switches, type-checking bad smells},
	pages = {329--331},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IZJ4BDRS\\login.html:text/html}
}

@misc{jmol__2013,
	url = {http://www.jmol.org/},
	journal = {Jmol: an open-source Java viewer for chemical structures in 3D},
	author = {Jmol},
	month = oct,
	year = {2013}
}

@article{jorgensen_forecasting_2007-2,
	title = {Forecasting of software development work effort: {Evidence} on expert judgement and formal models},
	volume = {23},
	issn = {0169-2070},
	shorttitle = {Forecasting of software development work effort},
	url = {http://www.sciencedirect.com/science/article/pii/S016920700700074X},
	doi = {10.1016/j.ijforecast.2007.05.008},
	abstract = {The review presented in this paper examines the evidence on the use of expert judgement, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgement-based effort estimates was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgement-based effort estimates were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated effort estimates based on a combination of expert judgement and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other estimation methods.},
	number = {3},
	urldate = {2014-08-21},
	journal = {International Journal of Forecasting},
	author = {Jørgensen, Magne},
	month = jul,
	year = {2007},
	keywords = {Combining forecasts, Comparative studies, Evaluating forecasts, Forecasting practice, Judgemental forecasting},
	pages = {449--462},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\AURSSPHM\\Jørgensen - 2007 - Forecasting of software development work effort E.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\FD7MBKUC\\S016920700700074X.html:text/html}
}

@misc{_sbse_????,
	title = {{SBSE} {REpository}},
	url = {http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/}
}

@article{savin_test_1977,
	title = {A {Test} of the {Monte} {Carlo} {Hypothesis}: {Comment}*},
	volume = {15},
	issn = {1465-7295},
	shorttitle = {A {Test} of the {Monte} {Carlo} {Hypothesis}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1465-7295.1977.tb01124.x/abstract},
	doi = {10.1111/j.1465-7295.1977.tb01124.x},
	language = {en},
	number = {4},
	urldate = {2014-01-20},
	journal = {Economic Inquiry},
	author = {Savin, N. E.},
	year = {1977},
	pages = {613--617},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\TE2TVN5P\\abstract.html:text/html}
}

@misc{jedit__2013,
	url = {http://www.jedit.org/},
	journal = {jEdit - Programmer's Text Editor},
	author = {JEdit},
	month = oct,
	year = {2013}
}

@article{li_systematic_2015,
	title = {A systematic mapping study on technical debt and its management},
	volume = {101},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121214002854},
	doi = {10.1016/j.jss.2014.12.027},
	abstract = {AbstractContext
Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system.
Objective
This work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM.
Method
A systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013.
Results
Ninety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected.
Conclusions
The term “debt” has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need for more empirical studies with high-quality evidence on the whole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process.},
	urldate = {2015-03-27},
	journal = {Journal of Systems and Software},
	author = {Li, Zengyang and Avgeriou, Paris and Liang, Peng},
	month = mar,
	year = {2015},
	keywords = {Systematic mapping study, Technical debt, Technical debt management},
	pages = {193--220},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DRMBG67U\\Li et al. - 2015 - A systematic mapping study on technical debt and i.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\U3DB8GRA\\S0164121214002854.html:text/html}
}

@article{leskovec_graph_2006,
	title = {Graph {Evolution}: {Densification} and {Shrinking} {Diameters}},
	shorttitle = {Graph {Evolution}},
	url = {http://arxiv.org/abs/physics/0603229},
	abstract = {How do real graphs evolve over time? What are ``normal'' growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time. Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)). Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a ``forest fire'' spreading process, that has a simple, intuitive justification, requires very few parameters (like the ``flammability'' of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study. We also notice that the ``forest fire'' model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point.},
	urldate = {2014-11-18},
	journal = {arXiv:physics/0603229},
	author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
	month = mar,
	year = {2006},
	note = {arXiv: physics/0603229},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability, Physics - Physics and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\74RUVN67\\0603229.html:text/html;arXiv\:physics/0603229 PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\8XCJDB67\\Leskovec et al. - 2006 - Graph Evolution Densification and Shrinking Diame.pdf:application/pdf}
}

@article{boehm_software_2000-1,
	title = {Software development cost estimation approaches — {A} survey},
	volume = {10},
	issn = {1022-7091, 1573-7489},
	url = {http://link.springer.com/article/10.1023/A%3A1018991717352},
	doi = {10.1023/A:1018991717352},
	abstract = {This paper summarizes several classes of software cost estimation models and techniques: parametric models, expertise‐based techniques, learning‐oriented techniques, dynamics‐based models, regression‐based models, and composite‐Bayesian techniques for integrating expertise‐based and regression‐based models. Experience to date indicates that neural‐net and dynamics‐based techniques are less mature than the other classes of techniques, but that all classes of techniques are challenged by the rapid pace of change in software technology. The primary conclusion is that no single technique is best for all situations, and that a careful comparison of the results of several approaches is most likely to produce realistic estimates.},
	language = {en},
	number = {1-4},
	urldate = {2014-08-21},
	journal = {Annals of Software Engineering},
	author = {Boehm, Barry and Abts, Chris and Chulani, Sunita},
	month = nov,
	year = {2000},
	keywords = {Software Engineering/Programming and Operating Systems},
	pages = {177--205},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\46FDWVDI\\A1018991717352.html:text/html}
}

@inproceedings{kleinberg_small-world_2000,
	title = {The {Small}-{World} {Phenomenon}: {An} {Algorithmic} {Perspective}},
	shorttitle = {The {Small}-{World} {Phenomenon}},
	abstract = {Long a matter of folklore, the “small-world phenomenon ”  — the principle that we are all linked by short chains of acquaintances — was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960’s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the “six degrees of separation ” between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insufficient to explain the striking algorithmic component of Milgram’s original findings: that individuals using local information are collectively very effective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are effective.},
	booktitle = {in {Proceedings} of the 32nd {ACM} {Symposium} on {Theory} of {Computing}},
	author = {Kleinberg, Jon},
	year = {2000},
	pages = {163--170},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\89ZDTJU7\\Kleinberg - 2000 - The Small-World Phenomenon An Algorithmic Perspec.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\9RK86BZ4\\summary.html:text/html}
}

@article{briand_property-based_1996,
	title = {Property-based software engineering measurement},
	volume = {22},
	issn = {0098-5589},
	doi = {10.1109/32.481535},
	abstract = {Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Briand, L.C. and Morasca, Sandro and Basili, V.R.},
	year = {1996},
	keywords = {cohesion, commercial static analyzers, complexity, computational complexity, Computer science, Costs, coupling, evaluation methods, Job shop scheduling, mathematical properties, property-based software engineering measurement, Proposals, Size measurement, Software design, Software engineering, Software measurement, software metrics, software products, Software quality, software system measurement, system monitoring, Virtual reality},
	pages = {68--86},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\N4CKNNC5\\login.html:text/html}
}

@book{joint_task_force_on_computing_curricula_computer_2013,
	address = {New York, NY, USA},
	title = {Computer {Science} {Curricula} 2013: {Curriculum} {Guidelines} for {Undergraduate} {Degree} {Programs} in {Computer} {Science}},
	isbn = {978-1-4503-2309-3},
	shorttitle = {Computer {Science} {Curricula} 2013},
	publisher = {ACM},
	author = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and Society, IEEE Computer},
	year = {2013},
	note = {999133}
}

@article{raja_defining_2012-1,
	title = {Defining and {Evaluating} a {Measure} of {Open} {Source} {Project} {Survivability}},
	volume = {38},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.39},
	abstract = {In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Raja, U. and Tretter, M.J.},
	month = jan,
	year = {2012},
	keywords = {Evaluation framework, external validity, Indexes, Maintenance engineering, multidimensional measure, open source project survivability, Open source software, open source software project survivability, organization, project evaluation, Project management, project viability, public domain software, resilience, Software measurement, software metrics, software survivability., viability index, vigor},
	pages = {163--174},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\P8UUG7KI\\abs_all.html:text/html}
}

@article{arcuri_parameter_2013,
	title = {Parameter tuning or default values? {An} empirical investigation in search-based software engineering},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Parameter tuning or default values?},
	url = {http://link.springer.com/article/10.1007/s10664-013-9249-9},
	doi = {10.1007/s10664-013-9249-9},
	abstract = {Many software engineering problems have been addressed with search algorithms. Search algorithms usually depend on several parameters (e.g., population size and crossover rate in genetic algorithms), and the choice of these parameters can have an impact on the performance of the algorithm. It has been formally proven in the No Free Lunch theorem that it is impossible to tune a search algorithm such that it will have optimal settings for all possible problems. So, how to properly set the parameters of a search algorithm for a given software engineering problem? In this paper, we carry out the largest empirical analysis so far on parameter tuning in search-based software engineering. More than one million experiments were carried out and statistically analyzed in the context of test data generation for object-oriented software using the EvoSuite tool. Results show that tuning does indeed have impact on the performance of a search algorithm. But, at least in the context of test data generation, it does not seem easy to find good settings that significantly outperform the “default” values suggested in the literature. This has very practical value for both researchers (e.g., when different techniques are compared) and practitioners. Using “default” values is a reasonable and justified choice, whereas parameter tuning is a long and expensive process that might or might not pay off in the end.},
	language = {en},
	number = {3},
	urldate = {2015-05-18},
	journal = {Empir Software Eng},
	author = {Arcuri, Andrea and Fraser, Gordon},
	month = feb,
	year = {2013},
	keywords = {Design of experiments, EvoSuite, Java, Object-oriented, Programming Languages, Compilers, Interpreters, Response surface, search-based software engineering, Software Engineering/Programming and Operating Systems, test data generation, Tuning, unit testing},
	pages = {594--623},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IERGBAB9\\Arcuri and Fraser - 2013 - Parameter tuning or default values An empirical i.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CNABRIVM\\s10664-013-9249-9.html:text/html}
}

@article{chaikalis_investigating_????-3,
	title = {Investigating the effect of evolution and refactorings on feature scattering},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/article/10.1007/s11219-013-9204-4},
	doi = {10.1007/s11219-013-9204-4},
	abstract = {The implementation of a functional requirement is often distributed across several modules posing difficulties to software maintenance. In this paper, we attempt to quantify the extent of feature scattering and study its evolution with the passage of software versions. To this end, we trace the classes and methods involved in the implementation of a feature, apply formal approaches for studying variations across versions, measure whether feature implementation is uniformly distributed and visualize the reuse among features. Moreover, we investigate the impact of refactoring application on feature scattering in order to assess the circumstances under which a refactoring might improve the distribution of methods implementing a feature. The proposed techniques are exemplified for various features on several versions of four open-source projects.},
	language = {en},
	urldate = {2014-04-18},
	journal = {Software Qual J},
	author = {Chaikalis, Theodore and Chatzigeorgiou, Alexander and Examiliotou, Georgina},
	keywords = {Data Structures, Cryptology and Information Theory, Feature identification, Feature scattering, Operating Systems, Programming Languages, Compilers, Interpreters, Program understanding, Refactorings, Requirements traceability, Software Engineering/Programming and Operating Systems, Software evolution},
	pages = {1--27},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DR76SZGR\\Chaikalis et al. - Investigating the effect of evolution and refactor.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ZHK4MZU4\\10.html:text/html}
}

@article{turnu_modified_2011,
	title = {A modified {Yule} process to model the evolution of some object-oriented system properties},
	volume = {181},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025510005219},
	doi = {10.1016/j.ins.2010.10.022},
	abstract = {We present a model based on the Yule process, able to explain the evolution of some properties of large object-oriented software systems. We study four system properties related to code production of four large object-oriented software systems – Eclipse, Netbeans, JDK and Ant. The properties analysed, namely the naming of variables and methods, the call to methods and the inheritance hierarchies, show a power-law distribution as reported in previous papers for different systems. We use the simulation approach to verify the goodness of our model, finding a very good correspondence between empirical data of subsequent software versions, and the prediction of the model presented.},
	number = {4},
	urldate = {2014-07-10},
	journal = {Information Sciences},
	author = {Turnu, I. and Concas, G. and Marchesi, M. and Pinna, S. and Tonelli, R.},
	month = feb,
	year = {2011},
	keywords = {Java, Object-oriented languages, power laws, Software engineering, Yule process},
	pages = {883--902},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\FXVAK9A2\\Turnu et al. - 2011 - A modified Yule process to model the evolution of .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5EIJVCPN\\S0020025510005219.html:text/html}
}

@book{yin_case_2013,
	address = {Thousand Oaks, Calif.},
	title = {Case study research: design and methods.},
	isbn = {9781452242569  1452242569},
	shorttitle = {Case study research},
	language = {English},
	publisher = {Sage},
	author = {Yin, Robert K},
	year = {2013}
}

@article{siau_evaluation_2011,
	title = {Evaluation techniques for systems analysis and design modelling methods – a review and comparative analysis},
	volume = {21},
	copyright = {© 2007 The Authors. Information Systems Journal © 2007 Blackwell Publishing Ltd},
	issn = {1365-2575},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2575.2007.00255.x/abstract},
	doi = {10.1111/j.1365-2575.2007.00255.x},
	abstract = {Even though hundreds of modelling methods are in existence today, practitioners and researchers are zealously ‘producing’ new modelling methods. The ‘blooming’ production of modelling methods is not the problem; the lack of standardized techniques for evaluating them is. To complicate the matter even further, most of the modelling methods have been introduced based on common sense and intuition of the methods' developers. Many of these methods lack theoretical foundations and empirical evidence to demonstrate their worthiness. With the current state of affairs, studies on evaluation of modelling methods have become necessary and critical. Comparing modelling methods provides us with the necessary knowledge and understanding of the strengths and weaknesses of each method. This knowledge can also guide us in our quest for better modelling methods. This paper reviews various evaluation techniques used by both researchers and practitioners. The evaluation techniques are categorized into three classifications: feature comparison, theoretical and conceptual evaluation, and empirical evaluation. This research also analyses the underlying philosophies and assumptions of these evaluation techniques.},
	language = {en},
	number = {3},
	urldate = {2014-01-20},
	journal = {Information Systems Journal},
	author = {Siau, Keng and Rossi, Matti},
	year = {2011},
	keywords = {action research, cognitive modelling, feature comparison, metamodelling, modelling methods, ontology},
	pages = {249--268},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\CGC6C4MM\\abstract.html:text/html}
}

@book{wohlin_experimentation_2000-1,
	address = {Norwell, MA, USA},
	title = {Experimentation in {Software} {Engineering}: {An} {Introduction}},
	isbn = {0-7923-8682-5},
	shorttitle = {Experimentation in {Software} {Engineering}},
	publisher = {Kluwer Academic Publishers},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Bjöorn and Wesslén, Anders},
	year = {2000}
}

@misc{jfreechart__2013,
	url = {http://www.jfree.org/jfreechart},
	journal = {JFreeChart},
	author = {JFreeChart},
	month = oct,
	year = {2013}
}

@misc{_fix_????,
	title = {Fix {Factory} of {Sound} » {HOME}},
	url = {http://www.fixfactoryofsound.gr/home/},
	urldate = {2014-03-29},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\VVTC65B3\\home.html:text/html}
}

@inproceedings{sharafat_probabilistic_2007,
	title = {A {Probabilistic} {Approach} to {Predict} {Changes} in {Object}-{Oriented} {Software} {Systems}},
	doi = {10.1109/CSMR.2007.9},
	abstract = {Predicting the changes in the next release of a software system has become a quest during its maintenance phase. Such a prediction can help managers to allocate resources more appropriately which results in reducing costs associated with software maintenance activities. A measure of change-proneness of a software system also provides a good understanding of its architectural stability. This research work proposes a novel approach to predict changes in an object oriented software system. The rationale behind this approach is that in a well-designed software system, feature enhancement or corrective maintenance should affect a limited amount of existing code. The goal is to quantify this aspect of quality by assessing the probability that each class will change in a future generation. Our proposed probabilistic approach uses the dependencies obtained from the UML diagrams, as well as other data extracted from source code of several releases of a software system using reverse engineering techniques. The proposed systematic approach has been evaluated on a multi-version medium size open source project namely JFlex, the fast scanner generator for Java. The obtained results indicate the simplicity and accuracy of our approach in the comparison with existing methods in the literature},
	booktitle = {11th {European} {Conference} on {Software} {Maintenance} and {Reengineering}, 2007. {CSMR} '07},
	author = {Sharafat, AR. and Tahvildari, L.},
	month = mar,
	year = {2007},
	keywords = {architectural stability, changes prediction, configuration management, cost reduction, Costs, data mining, Java, JFlex, object-oriented programming, object-oriented software systems, open source project, probabilistic approach, Probability, resource allocation, Resource management, reverse engineering, scanner generator, software architecture, software maintenance, Software measurement, Software systems, Stability, UML diagrams, Unified Modeling Language},
	pages = {27--38},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\F5G63WCI\\login.html:text/html}
}

@incollection{basili_goal_1994,
	title = {The {Goal} {Question} {Metric} {Approach}},
	booktitle = {Encyclopedia of {Software} {Engineering}},
	publisher = {Wiley},
	author = {Basili, Victor R. and Caldiera, Gianluigi and Rombach, H. Dieter},
	year = {1994}
}

@article{ouni_maintainability_2012,
	title = {Maintainability defects detection and correction: a multi-objective approach},
	volume = {20},
	issn = {0928-8910, 1573-7535},
	shorttitle = {Maintainability defects detection and correction},
	url = {http://link.springer.com/article/10.1007/s10515-011-0098-8},
	doi = {10.1007/s10515-011-0098-8},
	abstract = {Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort.},
	language = {en},
	number = {1},
	urldate = {2015-05-18},
	journal = {Autom Softw Eng},
	author = {Ouni, Ali and Kessentini, Marouane and Sahraoui, Houari and Boukadoum, Mounir},
	month = jan,
	year = {2012},
	keywords = {Artificial Intelligence (incl. Robotics), by example, Effort, Maintainability defects, Multi-objective optimization, search-based software engineering, Software Engineering/Programming and Operating Systems, software maintenance},
	pages = {47--79},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\UVD8VBUA\\Ouni et al. - 2012 - Maintainability defects detection and correction .pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\SRWMDD3R\\s10515-011-0098-8.html:text/html}
}

@book{chung_complex_2006,
	address = {Providence, RI},
	title = {Complex graphs and networks},
	isbn = {9780821836576  0821836579},
	language = {English},
	publisher = {American Mathematical Society},
	author = {Chung, Fan R. K and Lu, Linyuan},
	year = {2006}
}

@book{runeson_case_2012-1,
	address = {Hoboken, N.J},
	edition = {1 edition},
	title = {Case {Study} {Research} in {Software} {Engineering}: {Guidelines} and {Examples}},
	isbn = {9781118104354},
	shorttitle = {Case {Study} {Research} in {Software} {Engineering}},
	abstract = {Based on their own experiences of in-depth case studies of software projects in international corporations, in this book the authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering.  This is the first software engineering specific book on the case study research method.},
	language = {English},
	publisher = {Wiley},
	author = {Runeson, Per and Host, Martin and Rainer, Austen and Regnell, Bjorn},
	month = apr,
	year = {2012}
}

@article{kitchenham_systematic_2009-1,
	series = {Special {Section} - {Most} {Cited} {Articles} in 2002 and {Regular} {Research} {Papers}},
	title = {Systematic literature reviews in software engineering – {A} systematic literature review},
	volume = {51},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584908001390},
	doi = {10.1016/j.infsof.2008.09.009},
	abstract = {Background
In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.
Aims
This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.
Method
We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results
Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.
Conclusions
Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.},
	number = {1},
	urldate = {2014-12-17},
	journal = {Information and Software Technology},
	author = {Kitchenham, Barbara and Pearl Brereton, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	month = jan,
	year = {2009},
	keywords = {Cost estimation, Evidence-based software engineering, Systematic literature review, Systematic review quality, Tertiary study},
	pages = {7--15},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\DWH6JC2K\\Kitchenham et al. - 2009 - Systematic literature reviews in software engineer.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\EQW7HM4P\\S0950584908001390.html:text/html}
}

@inproceedings{mcdonnell_empirical_2013-2,
	address = {Washington, DC, USA},
	series = {{ICSM} '13},
	title = {An {Empirical} {Study} of {API} {Stability} and {Adoption} in the {Android} {Ecosystem}},
	isbn = {978-0-7695-4981-1},
	url = {http://dx.doi.org/10.1109/ICSM.2013.18},
	doi = {10.1109/ICSM.2013.18},
	abstract = {When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28\% of API references in client applications are outdated with a median lagging time of 16 months. 22\% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.},
	urldate = {2014-03-26},
	booktitle = {Proceedings of the 2013 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE Computer Society},
	author = {McDonnell, Tyler and Ray, Baishakhi and Kim, Miryung},
	year = {2013},
	pages = {70--79}
}

@misc{_isbsg_????,
	title = {{ISBSG} {\textbar} {Estimation} {Techniques} {\textbar} {Software} {Benchmarking} {\textbar} {Software} {Estimation} {\textbar} {Software} {Standards}},
	url = {http://www.isbsg.org/}
}

@inproceedings{nagappan_mining_2006,
	address = {New York, NY, USA},
	series = {{ICSE} '06},
	title = {Mining {Metrics} to {Predict} {Component} {Failures}},
	isbn = {1-59593-375-1},
	url = {http://doi.acm.org/10.1145/1134285.1134349},
	doi = {10.1145/1134285.1134349},
	abstract = {What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.},
	urldate = {2014-08-21},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
	year = {2006},
	keywords = {bug database, complexity metrics, empirical study, principal component analysis, regression model},
	pages = {452--461}
}

@incollection{jedlitschka_reporting_2008,
	title = {Reporting {Experiments} in {Software} {Engineering}},
	copyright = {©2008 Springer-Verlag London},
	isbn = {978-1-84800-043-8, 978-1-84800-044-5},
	url = {http://link.springer.com/chapter/10.1007/978-1-84800-044-5_8},
	abstract = {Background: One major problem for integrating study results into a common body of knowledge is the heterogeneity of reporting styles: (1) It is difficult to locate relevant information and (2) important information is often missing. Objective: A guideline for reporting results from controlled experiments is expected to support a systematic, standardized presentation of empirical research, thus improving reporting in order to support readers in (1) finding the information they are looking for, (2) understanding how an experiment is conducted, and (3) assessing the validity of its results. Method: The guideline for reporting is based on (1) a survey of the most prominent published proposals for reporting guidelines in software engineering and (2) an iterative development incorporating feedback from members of the research community. Result: This chapter presents the unification of a set of guidelines for reporting experiments in software engineering. Limitation: The guideline has not been evaluated broadly yet. Conclusion: The resulting guideline provides detailed guidance on the expected content of the sections and subsections for reporting a specific type of empirical study, i.e., experiments (controlled experiments and quasi-experiments).},
	language = {en},
	urldate = {2015-01-05},
	booktitle = {Guide to {Advanced} {Empirical} {Software} {Engineering}},
	publisher = {Springer London},
	author = {Jedlitschka, Andreas and Ciolkowski, Marcus and Pfahl, Dietmar},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	month = jan,
	year = {2008},
	keywords = {Management of Computing and Information Systems, Software engineering, Software Engineering/Programming and Operating Systems, System Performance and Evaluation, User Interfaces and Human Computer Interaction},
	pages = {201--228},
	file = {Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\JKQHDI44\\978-1-84800-044-5_8.html:text/html}
}

@article{raja_defining_2012-2,
	title = {Defining and {Evaluating} a {Measure} of {Open} {Source} {Project} {Survivability}},
	volume = {38},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.39},
	abstract = {In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Raja, U. and Tretter, M.J.},
	month = jan,
	year = {2012},
	keywords = {Evaluation framework, external validity, Indexes, Maintenance engineering, multidimensional measure, open source project survivability, Open source software, open source software project survivability, organization, project evaluation, Project management, project viability, public domain software, resilience, Software measurement, software metrics, software survivability., viability index, vigor},
	pages = {163--174},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\ZB6NQWJ8\\abs_all.html:text/html}
}

@article{moore_utilization_2013-1,
	title = {Utilization of {Relationship}-{Oriented} {Social} {Media} in the {Selling} {Process}: {A} {Comparison} of {Consumer} ({B}2C) and {Industrial} ({B}2B) {Salespeople}},
	volume = {12},
	issn = {1533-2861},
	shorttitle = {Utilization of {Relationship}-{Oriented} {Social} {Media} in the {Selling} {Process}},
	url = {http://dx.doi.org/10.1080/15332861.2013.763694},
	doi = {10.1080/15332861.2013.763694},
	abstract = {The rapid pace of technological advancement and adoption of social media among consumers/organizations has been unprecedented in recent years. This study provides insights into understanding social media utilization among professional salespeople. Specifically, social media applications are separated into 15 categories, with multiple applications falling within each category. From a sample of 395 salespeople in B2B and B2C markets, utilization of relationship-oriented social media applications are presented and examined. Overall, findings show that B2B practitioners tend to use media targeted at professionals whereas their B2C counterparts tend to utilize more sites targeted to the general public for engaging in one-on-one dialogue with their customers. Moreover, B2B professionals tend to use relationship-oriented social media technologies more than B2C professionals for the purpose of prospecting, handling objections, and after sale follow-up.},
	number = {1},
	urldate = {2015-02-05},
	journal = {Journal of Internet Commerce},
	author = {Moore, Jesse N. and Hopkins, Christopher D. and Raymond, Mary Anne},
	month = feb,
	year = {2013},
	note = {00003},
	pages = {48--75},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\9RQWD8AN\\Moore et al. - 2013 - Utilization of Relationship-Oriented Social Media .pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\5DEW4PCJ\\15332861.2013.html:text/html}
}

@misc{_acm_2012,
	title = {{ACM} {Computing} {Classification} {System}},
	url = {http://www.acm.org/about/class/2012},
	abstract = {The 2012 ACM Computing Classification System has been developed as a poly-hierarchical ontology that can be utilized in semantic web applications. It replaces the traditional 1998 version of the ACM Computing Classification System (CCS), which has served as the de facto standard classification system for the computing field. It is being integrated into the search capabilities and visual topic displays of the ACM Digital Library. It relies on a semantic vocabulary as the single source of categories and concepts that reflect the state of the art of the computing discipline and is receptive to structural change as it evolves in the future. ACM will a provide tools to facilitate the application of 2012 CCS categories to forthcoming papers and a process to ensure that the CCS stays current and relevant. The new classification system will play a key role in the development of a people search interface in the ACM Digital Library to supplement its current traditional bibliographic search.},
	language = {English},
	publisher = {Association for Computing Machinery},
	year = {2012}
}

@article{turnu_fractal_2013-2,
	series = {Statistics with {Imperfect} {Data}},
	title = {The fractal dimension of software networks as a global quality metric},
	volume = {245},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025513003885},
	doi = {10.1016/j.ins.2013.05.014},
	abstract = {We analyzed the source code of various releases of two large Object Oriented Open Source Java software systems, Eclipse and Netbeans, investigating the complexity of the whole release and of its subprojects. We show that when the classes in the source code and the dependencies between them are considered, such systems can be viewed as complex software networks, and emerging structures, characteristic of fractals, appear at different length scales – on the entire systems and on subprojects of any size.

We were able to find in all examined cases a scaling region where it is possible to compute a self-similar coefficient, the fractal dimension, using “the box counting method”. Such a coefficient is a single metric related to the system’s complexity.

More importantly, we were able to show that this measure looks fairly related to software quality, acting as a global quality software metric. In particular, we computed the defects of each software system, and we found a clear correlation among the number of defects in the system, or in a subproject, and its fractal dimension. This correlation exists across all the subprojects and also along the time evolution of the software systems, as new releases are delivered.},
	urldate = {2014-07-10},
	journal = {Information Sciences},
	author = {Turnu, I. and Concas, G. and Marchesi, M. and Tonelli, R.},
	month = oct,
	year = {2013},
	keywords = {Fractal dimension, Object-oriented languages, Software engineering, software metrics, Software quality},
	pages = {290--303},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\IKQZ5HDA\\Turnu et al. - 2013 - The fractal dimension of software networks as a gl.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\2GSISMMI\\S0020025513003885.html:text/html}
}

@article{newman_structure_2003,
	title = {The {Structure} and {Function} of {Complex} {Networks}},
	volume = {45},
	issn = {0036-1445},
	url = {http://epubs.siam.org/doi/abs/10.1137/S003614450342480},
	doi = {10.1137/S003614450342480},
	abstract = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.,  Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
	number = {2},
	urldate = {2014-08-28},
	journal = {SIAM Rev.},
	author = {Newman, M.},
	month = jan,
	year = {2003},
	pages = {167--256},
	file = {Full Text PDF:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\4U4MKKXI\\Newman - 2003 - The Structure and Function of Complex Networks.pdf:application/pdf;Snapshot:C\:\\Users\\Thodoris\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5x3gm89e.default\\zotero\\storage\\RZGDCSQ7\\S003614450342480.html:text/html}
}